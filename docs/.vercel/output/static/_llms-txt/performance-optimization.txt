<SYSTEM>Performance Optimization: Best practices for GPU memory, pipeline efficiency, and profiling</SYSTEM>

# Performance Optimization

## Overview [Section titled “Overview”](#overview) WebGPU performance optimization requires understanding GPU architecture fundamentals: massive parallelism, memory bandwidth limitations, and CPU-GPU synchronization overhead. ## GPU Architecture [Section titled “GPU Architecture”](#gpu-architecture) ### Memory Hierarchy [Section titled “Memory Hierarchy”](#memory-hierarchy) | Level | Speed | Scope | | ----------------------- | ------- | ------------- | | Registers | Fastest | Per-thread | | Shared/Workgroup Memory | Fast | Per-workgroup | | L1/L2 Cache | Medium | Automatic | | Global Memory | Slowest | All threads | ### Key Performance Factors [Section titled “Key Performance Factors”](#key-performance-factors) * **Parallelism**: GPUs run thousands of threads simultaneously * **Latency hiding**: GPUs switch thread groups when one stalls on memory * **Divergence**: Threads in same group taking different paths causes serialization * **Memory bandwidth**: Often the primary bottleneck ## Buffer Management [Section titled “Buffer Management”](#buffer-management) ### Buffer Pooling [Section titled “Buffer Pooling”](#buffer-pooling) ### Interleaved Vertex Attributes [Section titled “Interleaved Vertex Attributes”](#interleaved-vertex-attributes) ### Minimizing Buffer Updates [Section titled “Minimizing Buffer Updates”](#minimizing-buffer-updates) Batch uniform updates ```javascript // Group uniforms by update frequency const frameUniforms = device.createBuffer({ size: 256, ... }); // Per-frame const materialUniforms = device.createBuffer({ size: 1024, ... }); // Per-material const objectUniforms = device.createBuffer({ size: 65536, ... }); // Per-object with offsets // Single write per group instead of many small writes device.queue.writeBuffer(frameUniforms, 0, frameData); ``` Avoid Per-Frame Allocations Creating new buffers every frame triggers garbage collection and wastes GPU resources. Update existing buffers instead. ## Pipeline Optimization [Section titled “Pipeline Optimization”](#pipeline-optimization) ### Pipeline Caching [Section titled “Pipeline Caching”](#pipeline-caching) Cache pipeline objects ```javascript const pipelineCache = new Map(); function getPipeline(config) { const key = JSON.stringify(config); if (!pipelineCache.has(key)) { pipelineCache.set(key, device.createRenderPipeline(config)); } return pipelineCache.get(key); } ``` ### Async Pipeline Creation [Section titled “Async Pipeline Creation”](#async-pipeline-creation) Background pipeline compilation ```javascript // During initialization (prevents frame drops) const pipelines = await Promise.all([ device.createRenderPipelineAsync(desc1), device.createRenderPipelineAsync(desc2), device.createComputePipelineAsync(desc3), ]); ``` ### Reducing State Changes [Section titled “Reducing State Changes”](#reducing-state-changes) ## Memory Optimization [Section titled “Memory Optimization”](#memory-optimization) ### Texture Compression [Section titled “Texture Compression”](#texture-compression) | Format | Platform | Compression | | -------- | ------------------- | ------------- | | BC (DXT) | Desktop | \~75% smaller | | ETC2/EAC | Mobile | \~75% smaller | | ASTC | Mobile/Some Desktop | Variable | Check compression support ```javascript const hasBCTextures = adapter.features.has("texture-compression-bc"); const hasETC2 = adapter.features.has("texture-compression-etc2"); ``` ### Mipmaps [Section titled “Mipmaps”](#mipmaps) ### Staging Buffers [Section titled “Staging Buffers”](#staging-buffers) Ring buffer for streaming data ```javascript const RING_SIZE = 3; const stagingBuffers = Array.from({ length: RING_SIZE }, () => device.createBuffer({ size: bufferSize, usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC, }) ); let frameIndex = 0; function getNextStagingBuffer() { return stagingBuffers[frameIndex++ % RING_SIZE]; } ``` ## Compute Optimization [Section titled “Compute Optimization”](#compute-optimization) ### Workgroup Size [Section titled “Workgroup Size”](#workgroup-size) ### Memory Coalescing [Section titled “Memory Coalescing”](#memory-coalescing) Coalesced vs scattered access ```wgsl // Good: Adjacent threads access adjacent memory let value = input[global_id.x]; // Bad: Scattered access let value = input[global_id.x * stride]; ``` ### Shared Memory [Section titled “Shared Memory”](#shared-memory) Use workgroup memory for data reuse ```wgsl var<workgroup> cache: array<f32, 256>; @compute @workgroup_size(256) fn compute(@builtin(local_invocation_id) lid: vec3u) { // Load from global to shared cache[lid.x] = globalData[gid.x]; workgroupBarrier(); // Multiple reads from fast shared memory var sum = 0.0; for (var i = 0u; i < 256u; i++) { sum += cache[i]; } } ``` ## Instancing [Section titled “Instancing”](#instancing) Render thousands with one draw call ```javascript // Per-instance buffer const instanceBuffer = device.createBuffer({ size: instanceCount * 64, // 4x4 matrix per instance usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }); // Vertex layout with instance step mode buffers: [ { arrayStride: 12, stepMode: "vertex", ... }, { arrayStride: 64, stepMode: "instance", ... }, ] // Single draw call for all instances passEncoder.draw(vertexCount, instanceCount); ``` ## Shader Optimization [Section titled “Shader Optimization”](#shader-optimization) ### Avoid Branching [Section titled “Avoid Branching”](#avoid-branching) Replace branches with math ```wgsl // Bad: Causes divergence if (value > threshold) { result = a; } else { result = b; } // Good: No divergence result = mix(b, a, step(threshold, value)); ``` ### Use Built-in Functions [Section titled “Use Built-in Functions”](#use-built-in-functions) Built-ins like `dot()`, `normalize()`, `mix()`, `smoothstep()` map directly to hardware instructions and are significantly faster than manual implementations. ## Profiling [Section titled “Profiling”](#profiling) ### Timestamp Queries [Section titled “Timestamp Queries”](#timestamp-queries) Measure GPU execution time ```javascript const querySet = device.createQuerySet({ type: "timestamp", count: 2, }); const resolveBuffer = device.createBuffer({ size: 16, usage: GPUBufferUsage.QUERY_RESOLVE | GPUBufferUsage.COPY_SRC, }); // In render pass passEncoder.writeTimestamp(querySet, 0); // ... render commands ... passEncoder.writeTimestamp(querySet, 1); // Resolve and read encoder.resolveQuerySet(querySet, 0, 2, resolveBuffer, 0); ``` ### Browser DevTools [Section titled “Browser DevTools”](#browser-devtools) * **Chrome**: Performance panel, about://gpu * **Firefox**: about:support, Performance profiler * **Platform tools**: PIX (Windows), Instruments (macOS) ## Common Bottlenecks [Section titled “Common Bottlenecks”](#common-bottlenecks) ## Quick Checklist [Section titled “Quick Checklist”](#quick-checklist) | Area | Optimization | | -------------- | -------------------------------------------------------------- | | **Buffers** | Pool and reuse; interleave vertices; batch updates | | **Pipelines** | Cache objects; use async creation; sort by pipeline | | **Memory** | Compress textures; generate mipmaps; use staging buffers | | **Draw Calls** | Use instancing; batch by material; minimize state changes | | **Shaders** | Avoid branching; use built-ins; move work to vertex stage | | **Compute** | Use workgroup size 64; coalesce access; leverage shared memory | | **Profiling** | Measure before optimizing; use timestamp queries | ## Resources [Section titled “Resources”](#resources)