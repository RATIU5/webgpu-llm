<SYSTEM>Compute Pipelines: General-purpose GPU computing with compute shaders and workgroups</SYSTEM>

# Compute Pipelines

## Overview [Section titled “Overview”](#overview) Compute pipelines enable general-purpose GPU computation (GPGPU) in WebGPU, executing arbitrary parallel algorithms without rendering operations. Unlike render pipelines that produce visual output, compute pipelines process data through storage buffers and textures. ## GPU Architecture [Section titled “GPU Architecture”](#gpu-architecture) ### Parallelism Model [Section titled “Parallelism Model”](#parallelism-model) | CPU | GPU | | ------------------------------------ | --------------------------------- | | 4-16 high-performance cores | 2,000-10,000+ simpler cores | | Optimized for sequential performance | Optimized for throughput | | Low latency per operation | High throughput across operations | GPUs execute the same instruction on different data simultaneously—the **SIMT (Single Instruction, Multiple Threads)** model. All threads in a group run the same instruction in lockstep. Thread Divergence When threads in a group take different branches, the GPU must execute both paths sequentially: ```wgsl if (data[id] > threshold) { result[id] = expensiveCalculation(data[id]); } else { result[id] = 0.0; } ``` If half the threads take each branch, execution time equals the sum of both paths, not the faster one. ### When GPUs Excel [Section titled “When GPUs Excel”](#when-gpus-excel) GPUs become advantageous when: * Workload is highly parallel (thousands+ independent operations) * Data transfers to GPU memory are efficient * Algorithm doesn’t require frequent CPU-GPU synchronization * Memory access patterns align with GPU architecture ## Workgroups and Invocations [Section titled “Workgroups and Invocations”](#workgroups-and-invocations) ### Workgroup Structure [Section titled “Workgroup Structure”](#workgroup-structure) A **workgroup** is a collection of shader invocations that execute together and can cooperate through shared memory. Workgroup size declaration ```wgsl @compute @workgroup_size(8, 8, 1) fn computeMain( @builtin(global_invocation_id) global_id: vec3<u32>, @builtin(local_invocation_id) local_id: vec3<u32>, @builtin(workgroup_id) workgroup_id: vec3<u32> ) { // 8×8×1 = 64 invocations per workgroup } ``` ### Built-in Identifiers [Section titled “Built-in Identifiers”](#built-in-identifiers) | Built-in | Type | Description | | ------------------------ | ----------- | ------------------------------------------ | | `local_invocation_id` | `vec3<u32>` | Position within workgroup (0 to size-1) | | `workgroup_id` | `vec3<u32>` | Which workgroup this invocation belongs to | | `global_invocation_id` | `vec3<u32>` | Unique position across all workgroups | | `local_invocation_index` | `u32` | Linearized index within workgroup | | `num_workgroups` | `vec3<u32>` | Total workgroups dispatched | The `global_invocation_id` is computed as: `workgroup_id × workgroup_size + local_invocation_id` ### Choosing Workgroup Size [Section titled “Choosing Workgroup Size”](#choosing-workgroup-size) Query device limits: Check workgroup limits ```javascript const limits = device.limits; console.log("Max workgroup size X:", limits.maxComputeWorkgroupSizeX); console.log("Max invocations per workgroup:", limits.maxComputeInvocationsPerWorkgroup); ``` ## Dispatching Work [Section titled “Dispatching Work”](#dispatching-work) ### dispatchWorkgroups() [Section titled “dispatchWorkgroups()”](#dispatchworkgroups) Dispatch compute work ```javascript const commandEncoder = device.createCommandEncoder(); const passEncoder = commandEncoder.beginComputePass(); passEncoder.setPipeline(computePipeline); passEncoder.setBindGroup(0, bindGroup); passEncoder.dispatchWorkgroups(workgroupsX, workgroupsY, workgroupsZ); passEncoder.end(); device.queue.submit([commandEncoder.finish()]); ``` ### Calculating Dispatch Dimensions [Section titled “Calculating Dispatch Dimensions”](#calculating-dispatch-dimensions) **1D Data** (10,000 elements, workgroup size 64): 1D dispatch calculation ```javascript const dataSize = 10000; const workgroupSize = 64; const workgroupsNeeded = Math.ceil(dataSize / workgroupSize); passEncoder.dispatchWorkgroups(workgroupsNeeded, 1, 1); // Dispatches 157 workgroups (157 × 64 = 10,048 invocations) ``` **2D Data** (1920×1080 image, 8×8 workgroups): 2D dispatch calculation ```javascript const workgroupsX = Math.ceil(1920 / 8); const workgroupsY = Math.ceil(1080 / 8); passEncoder.dispatchWorkgroups(workgroupsX, workgroupsY, 1); ``` Always Add Bounds Checks Dispatching more invocations than data elements causes out-of-bounds access: ```wgsl @compute @workgroup_size(64) fn process(@builtin(global_invocation_id) id: vec3<u32>) { if (id.x >= arrayLength(&data)) { return; // Guard against overflow } // Process data[id.x] } ``` ### Dispatch Strategies [Section titled “Dispatch Strategies”](#dispatch-strategies) **Multi-pass for cross-workgroup dependencies:** When results from one workgroup affect another, use separate dispatches: Multi-pass reduction ```javascript // Pass 1: Partial sums within workgroups pass1.dispatchWorkgroups(Math.ceil(dataSize / 256)); pass1.end(); // Pass 2: Final reduction of partial results const pass2 = encoder.beginComputePass(); pass2.setPipeline(reductionPipeline); pass2.setBindGroup(0, partialResultsBindGroup); pass2.dispatchWorkgroups(1); // Single workgroup for final sum pass2.end(); ``` **Indirect dispatch for GPU-driven workloads:** Let the GPU determine dispatch size: GPU-driven dispatch ```javascript // Buffer holds dispatch dimensions (3 × u32) const indirectBuffer = device.createBuffer({ size: 12, usage: GPUBufferUsage.INDIRECT | GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }); // Compute shader writes workgroup counts // Then dispatch using those counts pass.dispatchWorkgroupsIndirect(indirectBuffer, 0); ``` Write dispatch args in shader ```wgsl struct DispatchArgs { x: u32, y: u32, z: u32, } @group(0) @binding(0) var<storage, read_write> dispatch: DispatchArgs; @group(0) @binding(1) var<storage, read> activeCount: u32; @compute @workgroup_size(1) fn prepareDispatch() { dispatch.x = (activeCount + 63u) / 64u; // Workgroups needed dispatch.y = 1u; dispatch.z = 1u; } ``` ## Creating Compute Pipelines [Section titled “Creating Compute Pipelines”](#creating-compute-pipelines) ### GPUComputePipeline [Section titled “GPUComputePipeline”](#gpucomputepipeline) Create compute pipeline ```javascript const computePipeline = device.createComputePipeline({ label: "data-processing-pipeline", layout: "auto", compute: { module: shaderModule, entryPoint: "main", }, }); ``` Use async creation to avoid blocking: Async pipeline creation ```javascript const computePipeline = await device.createComputePipelineAsync({ layout: "auto", compute: { module: shaderModule, entryPoint: "main" }, }); ``` ### Storage Buffers [Section titled “Storage Buffers”](#storage-buffers) Storage buffers are the primary data mechanism for compute shaders: Create storage buffers ```javascript const inputBuffer = device.createBuffer({ size: dataArray.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); new Float32Array(inputBuffer.getMappedRange()).set(dataArray); inputBuffer.unmap(); const outputBuffer = device.createBuffer({ size: dataArray.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC, }); ``` In WGSL: Storage buffer declarations ```wgsl @group(0) @binding(0) var<storage, read> input: array<f32>; @group(0) @binding(1) var<storage, read_write> output: array<f32>; ``` | Access Mode | WGSL | Use Case | | ----------- | -------------------------- | -------------------------- | | Read-only | `var<storage, read>` | Input data | | Read-write | `var<storage, read_write>` | Output or in-place updates | ### Complete Example [Section titled “Complete Example”](#complete-example) Full compute pipeline setup ```javascript const shaderCode = ` @group(0) @binding(0) var<storage, read> input: array<f32>; @group(0) @binding(1) var<storage, read_write> output: array<f32>; @compute @workgroup_size(64) fn main(@builtin(global_invocation_id) id: vec3<u32>) { let i = id.x; if (i < arrayLength(&input)) { output[i] = sqrt(input[i]); } } `; const shaderModule = device.createShaderModule({ code: shaderCode }); const pipeline = device.createComputePipeline({ layout: "auto", compute: { module: shaderModule, entryPoint: "main" }, }); // Create buffers and bind group const inputData = new Float32Array(1000).map((_, i) => i); const inputBuffer = device.createBuffer({ size: inputData.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer(inputBuffer, 0, inputData); const outputBuffer = device.createBuffer({ size: inputData.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC, }); const bindGroup = device.createBindGroup({ layout: pipeline.getBindGroupLayout(0), entries: [ { binding: 0, resource: { buffer: inputBuffer } }, { binding: 1, resource: { buffer: outputBuffer } }, ], }); // Execute const encoder = device.createCommandEncoder(); const pass = encoder.beginComputePass(); pass.setPipeline(pipeline); pass.setBindGroup(0, bindGroup); pass.dispatchWorkgroups(Math.ceil(inputData.length / 64)); pass.end(); device.queue.submit([encoder.finish()]); ``` ## TypeGPU Compute Pipelines [Section titled “TypeGPU Compute Pipelines”](#typegpu-compute-pipelines) TypeGPU provides type-safe compute pipeline abstractions: TypeGPU compute pipeline ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; const root = await tgpu.init(); const doubleValues = tgpu.computeFn([], () => { "use gpu"; const id = builtin.globalInvocationId.x; if (id < inputBuffer.value.length) { outputBuffer.value[id] = inputBuffer.value[id] * 2.0; } }).$workgroupSize(64); const pipeline = root["~unstable"].withCompute(doubleValues).createPipeline(); pipeline.with(bindGroup).dispatchWorkgroups(Math.ceil(1000 / 64)); ``` ## Memory Access Patterns [Section titled “Memory Access Patterns”](#memory-access-patterns) ### Coalesced Access [Section titled “Coalesced Access”](#coalesced-access) ### Shared Memory [Section titled “Shared Memory”](#shared-memory) Workgroup shared memory (`var<workgroup>`) is fast on-chip memory shared by all invocations in a workgroup: Using shared memory ```wgsl var<workgroup> sharedCache: array<f32, 64>; @compute @workgroup_size(64) fn process( @builtin(local_invocation_id) local_id: vec3<u32>, @builtin(global_invocation_id) global_id: vec3<u32> ) { // Load into shared memory sharedCache[local_id.x] = input[global_id.x]; // Wait for all threads to finish loading workgroupBarrier(); // Perform operations using cached data var sum = 0.0; for (var i = 0u; i < 64u; i++) { sum += sharedCache[i]; } output[global_id.x] = sum; } ``` ## Synchronization [Section titled “Synchronization”](#synchronization) ### workgroupBarrier() [Section titled “workgroupBarrier()”](#workgroupbarrier) Synchronizes all invocations in a workgroup—all threads must reach the barrier before any can proceed: Barrier synchronization ```wgsl var<workgroup> data: array<f32, 64>; @compute @workgroup_size(64) fn compute(@builtin(local_invocation_id) local_id: vec3<u32>) { // Phase 1: Each thread writes data[local_id.x] = computeValue(); // Wait for all writes to complete workgroupBarrier(); // Phase 2: Safely read other threads' data let sum = data[0] + data[local_id.x]; } ``` ### storageBarrier() [Section titled “storageBarrier()”](#storagebarrier) Ensures all storage buffer writes within a workgroup are visible before proceeding: Storage barrier ```wgsl @group(0) @binding(0) var<storage, read_write> buffer: array<f32>; @compute @workgroup_size(64) fn compute(@builtin(global_invocation_id) id: vec3<u32>) { // Write to storage buffer buffer[id.x] = computeValue(id.x); // Ensure writes are visible within workgroup storageBarrier(); // Now reads see updated values (within same workgroup) let neighbor = buffer[id.x + 1u]; } ``` Workgroup Scope Only Both `workgroupBarrier()` and `storageBarrier()` only synchronize invocations **within the same workgroup**. They cannot synchronize across different workgroups. For cross-workgroup synchronization: * Use multiple dispatch calls (separate passes) * Use atomics for coordination ### Barrier Comparison [Section titled “Barrier Comparison”](#barrier-comparison) | Barrier | Scope | Memory Affected | | -------------------- | --------- | ---------------- | | `workgroupBarrier()` | Workgroup | `var<workgroup>` | | `storageBarrier()` | Workgroup | `var<storage>` | | `textureBarrier()` | Workgroup | Storage textures | Race Conditions Without barriers, some threads may read before others have written: ```wgsl // BAD: Race condition! counter[0] = counter[0] + 1; // Multiple threads write simultaneously // GOOD: Use atomics atomicAdd(&counter[0], 1u); ``` ## Common Use Cases [Section titled “Common Use Cases”](#common-use-cases) ## Resources [Section titled “Resources”](#resources)