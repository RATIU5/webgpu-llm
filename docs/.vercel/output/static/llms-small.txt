<SYSTEM>This is the abridged developer documentation for WebGPU and TypeGPU Guide</SYSTEM>

# RATIU5' TypeGPU Docs

> Learn about TypeGPU and WebGPU from these docs and AI

## Next steps [Section titled “Next steps”](#next-steps) Read the docs Begin reading the [documentation](/getting-started/) and learning about TypeGPU and WebGPU Install the CLI Using AI to learn? [Install the CLI](/installation/) to make the learning process easier.

# Advanced TypeGPU Patterns

## Overview [Section titled “Overview”](#overview) TypeGPU provides advanced features for building production-ready GPU applications: externals for dependency management, the Resolve API for debugging, external textures for video processing, WebGPU interoperability, and React Native support. ## Externals and $uses [Section titled “Externals and $uses”](#externals-and-uses) ### Automatic Dependency Management [Section titled “Automatic Dependency Management”](#automatic-dependency-management) Externals allow TGSL functions to reference resources and other functions without manual binding management: Externals in template literals ```typescript const timeUniform = root.createBuffer(d.f32).$usage("uniform"); const particleBuffer = root.createBuffer(d.arrayOf(d.vec3f, 1000)).$usage("storage"); const updateParticles = tgpu.fn([d.u32], d.void).does`(idx: u32) { let dt = ${timeUniform}; particles[idx] = particles[idx] + velocities[idx] * dt; }`.$uses({ particles: particleBuffer, velocities: velocityBuffer, }); ``` ### Nested Dependencies [Section titled “Nested Dependencies”](#nested-dependencies) TypeGPU walks the dependency tree automatically: Dependency chain ```typescript const computeGravity = tgpu.fn([d.vec3f, d.vec3f], d.vec3f) .does`(pos1: vec3f, pos2: vec3f) -> vec3f { let diff = pos2 - pos1; return normalize(diff) * ${d.f32(9.81)} / dot(diff, diff); }`; const physicsUpdate = tgpu.fn([d.u32], d.void).does`(idx: u32) { totalForce += ${computeGravity}(positions[idx], otherPos); }`.$uses({ positions: positionBuffer }); // computeGravity is automatically included ``` ## Resolve API [Section titled “Resolve API”](#resolve-api) ### Debugging Generated WGSL [Section titled “Debugging Generated WGSL”](#debugging-generated-wgsl) Use `tgpu.resolve()` to inspect generated shader code: Inspect WGSL output ```typescript const wgsl = tgpu.resolve(myShaderFunction); console.log(wgsl); // With context for bind group information const resolved = tgpu.resolveWithContext({ main: shaderFunction, uniforms: uniformBuffer, }); console.log("WGSL:", resolved.code); console.log("Bind Groups:", resolved.bindGroupLayouts); ``` ## External Textures [Section titled “External Textures”](#external-textures) ### Video Frame Processing [Section titled “Video Frame Processing”](#video-frame-processing) External textures enable zero-copy GPU access to video frames: Process video frames ```typescript const video = document.createElement("video"); video.src = "video.mp4"; video.play(); function frame() { const externalTexture = device.importExternalTexture({ source: video }); // Use in shader (must complete in same frame) processFrame(externalTexture); requestAnimationFrame(frame); } ``` Frame Lifetime External textures are destroyed automatically when JavaScript returns to the browser. Create and consume in the same callback. ### Camera Input [Section titled “Camera Input”](#camera-input) Access camera stream ```typescript const stream = await navigator.mediaDevices.getUserMedia({ video: { width: 1920, height: 1080 }, }); const video = document.createElement("video"); video.srcObject = stream; video.play(); // Use same pattern as video files ``` ## WebGPU Interoperability [Section titled “WebGPU Interoperability”](#webgpu-interoperability) ### Access Raw Resources [Section titled “Access Raw Resources”](#access-raw-resources) Unwrap TypeGPU to WebGPU ```typescript const root = await tgpu.init(); // Get raw GPUDevice const device = root.unwrap().device; // Get raw GPUBuffer from typed buffer const typedBuffer = root.createBuffer(schema, data).$usage("storage"); const rawBuffer = root.unwrap(typedBuffer); // Use in vanilla WebGPU const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [{ binding: 0, resource: { buffer: rawBuffer } }], }); ``` ### Gradual Migration [Section titled “Gradual Migration”](#gradual-migration) TypeGPU is non-contagious—use it incrementally: Mix TypeGPU with vanilla WebGPU ```typescript // TypeGPU for type-safe buffer management const typedBuffer = root.createBuffer(schema, data).$usage("storage"); // Vanilla WebGPU for custom pipeline const rawBuffer = root.unwrap(typedBuffer); const vanillaPipeline = device.createComputePipeline({ ... }); // TypeGPU for type-safe writes await typedBuffer.write(newData); ``` ## React Native Support [Section titled “React Native Support”](#react-native-support) ### Setup [Section titled “Setup”](#setup) ```bash npm install react-native-wgpu typegpu npm install --save-dev @webgpu/types unplugin-typegpu ``` Configure Babel: babel.config.js ```javascript module.exports = (api) => { api.cache(true); return { presets: ["babel-preset-expo"], plugins: ["unplugin-typegpu/babel"], }; }; ``` ### Mobile Rendering [Section titled “Mobile Rendering”](#mobile-rendering) React Native canvas ```typescript import { Canvas, useCanvasEffect } from 'react-native-wgpu'; export default function App() { const ref = useCanvasEffect(async () => { const context = ref.current!.getContext('webgpu')!; const device = await navigator.gpu.requestAdapter() .then(a => a!.requestDevice()); context.configure({ device, format: navigator.gpu.getPreferredCanvasFormat() }); const frame = () => { // ... render commands ... device.queue.submit([commandEncoder.finish()]); context.present(); // Required on React Native! requestAnimationFrame(frame); }; requestAnimationFrame(frame); }); return <Canvas ref={ref} style={{ flex: 1 }} />; } ``` Mobile Differences * Requires `context.present()` after submit * iOS: Disable Metal validation in Xcode * Expo: Must use `expo prebuild`, not Expo Go ## Generator CLI (tgpu-gen) [Section titled “Generator CLI (tgpu-gen)”](#generator-cli-tgpu-gen) Convert WGSL to TypeGPU: ```bash # Single file npx tgpu-gen src/shaders/compute.wgsl # Glob pattern npx tgpu-gen "src/shaders/**/*.wgsl" # Watch mode npx tgpu-gen "src/shaders/**/*.wgsl" --watch ``` ## CPU Simulation [Section titled “CPU Simulation”](#cpu-simulation) Test shaders on CPU without GPU: Unit test GPU functions ```typescript import { test, expect } from "vitest"; const normalize2D = (x: number, y: number) => { "use gpu"; const length = Math.sqrt(x * x + y * y); return { x: x / length, y: y / length }; }; test("normalize2D produces unit vectors", () => { const result = tgpu["~unstable"].simulate(normalize2D, [3, 4]); expect(result.x).toBeCloseTo(0.6); expect(result.y).toBeCloseTo(0.8); }); ``` Limitations * No buffer/texture access * CPU/GPU floating-point may differ slightly * For correctness testing, not performance ## Custom Extensions [Section titled “Custom Extensions”](#custom-extensions) ### Building Domain Libraries [Section titled “Building Domain Libraries”](#building-domain-libraries) Particle system library ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; export const ParticleSchema = d.struct({ position: d.vec3f, velocity: d.vec3f, lifetime: d.f32, }); export class ParticleSystem { private particleBuffer: TgpuBuffer; constructor(root: TgpuRoot, maxParticles: number) { this.particleBuffer = root .createBuffer(d.arrayOf(ParticleSchema, maxParticles)) .$usage("storage"); } getBuffer() { return this.particleBuffer; } } ``` ### Custom Data Types [Section titled “Custom Data Types”](#custom-data-types) Physics type library ```typescript import * as d from "typegpu/data"; export const Quaternion = d.vec4f; export const RigidBody = d.struct({ position: d.vec3f, rotation: Quaternion, linearVelocity: d.vec3f, angularVelocity: d.vec3f, mass: d.f32, }); ``` ## Advanced Slots [Section titled “Advanced Slots”](#advanced-slots) ### Slot Hierarchies [Section titled “Slot Hierarchies”](#slot-hierarchies) Nested configuration ```typescript const simulationConfig = tgpu.slot({ physics: tgpu.slot({ gravity: d.f32, damping: d.f32, }), rendering: tgpu.slot({ particleSize: d.f32, }), }); const pipeline = root .makeComputePipeline(shader) .$with(simulationConfig, { physics: { gravity: -9.81, damping: 0.99 }, rendering: { particleSize: 5.0 }, }); ``` ### Runtime Configuration [Section titled “Runtime Configuration”](#runtime-configuration) Debug visualization modes ```typescript const shaderMode = tgpu.slot(d.u32); const shader = tgpu.fn([d.u32], d.vec4f).does`(idx: u32) -> vec4f { switch (${shaderMode}) { case 0u: { return normalVisualization(idx); } case 1u: { return depthVisualization(idx); } default: { return fullRendering(idx); } } }`; // Switch at runtime pipeline.$with(shaderMode, 1); ``` ## Common Pitfalls [Section titled “Common Pitfalls”](#common-pitfalls) Unwrapped Resources Lose Type Safety ```typescript const rawBuffer = root.unwrap(typedBuffer); await rawBuffer.write([1, 2, 3]); // TypeError! // Use typed buffer for write operations await typedBuffer.write([1, 2, 3]); // Correct ``` External Texture Lifetime ```typescript // Wrong: texture destroyed before use const tex = device.importExternalTexture({ source: video }); requestAnimationFrame(() => renderWithTexture(tex)); // Error! // Correct: create and use in same frame requestAnimationFrame(() => { const tex = device.importExternalTexture({ source: video }); renderWithTexture(tex); }); ``` Editing Generated Files Never edit files generated by tgpu-gen. Import and extend instead: ```typescript // compute.ts (generated - don't edit!) export const updateParticles = /* ... */; // particle-system.ts (your code) import { updateParticles } from './compute'; export class ParticleSystem { /* custom logic */ } ``` ## Framework Integration [Section titled “Framework Integration”](#framework-integration) ### Three.js with TypeGPU [Section titled “Three.js with TypeGPU”](#threejs-with-typegpu) Use TypeGPU for type-safe compute shaders alongside Three.js rendering: Three.js + TypeGPU compute ```typescript import * as THREE from "three/webgpu"; import tgpu from "typegpu"; import * as d from "typegpu/data"; // Share device between Three.js and TypeGPU const renderer = new THREE.WebGPURenderer(); await renderer.init(); const root = await tgpu.init({ device: renderer.backend.device, // Reuse Three.js device }); // TypeGPU compute for physics const positions = root.createBuffer(d.arrayOf(d.vec3f, 1000)).$usage("storage"); // Three.js reads TypeGPU buffer const geometry = new THREE.BufferGeometry(); geometry.setAttribute("position", new THREE.BufferAttribute( root.unwrap(positions), 3 )); function animate() { // TypeGPU: Update physics physicsComputePipeline.dispatchWorkgroups(64); // Three.js: Render scene renderer.render(scene, camera); requestAnimationFrame(animate); } ``` ### Babylon.js with TypeGPU [Section titled “Babylon.js with TypeGPU”](#babylonjs-with-typegpu) Babylon.js + TypeGPU ```typescript import { Engine, Scene } from "@babylonjs/core"; import tgpu from "typegpu"; // Initialize Babylon with WebGPU const engine = new Engine(canvas, true, { preserveDrawingBuffer: true, stencil: true, }); await engine.initAsync(); // Initialize TypeGPU with Babylon's device const root = await tgpu.init({ device: engine._device, // Access internal device }); // Use TypeGPU for compute workloads const computeBuffer = root .createBuffer(d.arrayOf(d.f32, 1024)) .$usage("storage"); // Babylon handles rendering, TypeGPU handles compute engine.runRenderLoop(() => { computePipeline.execute(); // TypeGPU compute scene.render(); // Babylon render }); ``` ### Pattern: Hybrid Rendering [Section titled “Pattern: Hybrid Rendering”](#pattern-hybrid-rendering) Framework renders, TypeGPU computes ```typescript // 1. TypeGPU owns compute resources const particleData = root.createBuffer(particleSchema, particles); const computePipeline = root .withCompute(updateParticlesShader) .createPipeline(); // 2. Framework reads results const rawBuffer = root.unwrap(particleData); framework.setInstanceBuffer(rawBuffer); // 3. Render loop function frame() { computePipeline.execute(); // TypeGPU: physics framework.render(scene, camera); // Framework: graphics requestAnimationFrame(frame); } ``` WebGPU Renderer Required Framework integration requires WebGPU backends: * Three.js: `import * as THREE from "three/webgpu"` * Babylon.js: Enable WebGPU engine mode WebGL renderers cannot share resources with TypeGPU. ## Resources [Section titled “Resources”](#resources)

# Atomic Operations

## Overview [Section titled “Overview”](#overview) Atomic operations provide thread-safe access to shared memory in GPU compute shaders. When thousands of invocations execute concurrently, atomics guarantee that read-modify-write sequences complete without interference. ## Atomic Types [Section titled “Atomic Types”](#atomic-types) ### Declaration [Section titled “Declaration”](#declaration) Atomic variable declarations ```wgsl // Storage buffer atomics @group(0) @binding(0) var<storage, read_write> counter: atomic<u32>; @group(0) @binding(1) var<storage, read_write> histogram: array<atomic<u32>, 256>; // Workgroup shared atomics var<workgroup> localCounter: atomic<i32>; ``` ### Supported Types [Section titled “Supported Types”](#supported-types) | Type | Description | | ------------- | ----------------------- | | `atomic<u32>` | Unsigned 32-bit integer | | `atomic<i32>` | Signed 32-bit integer | No Float Atomics WGSL does not support `atomic<f32>`. Use fixed-point representation or compare-exchange loops for floating-point atomics. ### Address Space Restrictions [Section titled “Address Space Restrictions”](#address-space-restrictions) Atomics only work in: * `var<storage, read_write>` — Storage buffers * `var<workgroup>` — Workgroup shared memory ## Atomic Operations [Section titled “Atomic Operations”](#atomic-operations) ### Load and Store [Section titled “Load and Store”](#load-and-store) Basic atomic access ```wgsl let value = atomicLoad(&counter); atomicStore(&counter, 42u); ``` ### Arithmetic Operations [Section titled “Arithmetic Operations”](#arithmetic-operations) All return the **previous value**: Atomic arithmetic ```wgsl let old = atomicAdd(&counter, 1u); // Add, return old let old = atomicSub(&counter, 1u); // Subtract, return old let old = atomicMax(&counter, value); // Store max, return old let old = atomicMin(&counter, value); // Store min, return old ``` ### Bitwise Operations [Section titled “Bitwise Operations”](#bitwise-operations) Atomic bitwise ```wgsl let old = atomicAnd(&flags, mask); // AND, return old let old = atomicOr(&flags, mask); // OR, return old let old = atomicXor(&flags, mask); // XOR, return old ``` ### Exchange and Compare-Exchange [Section titled “Exchange and Compare-Exchange”](#exchange-and-compare-exchange) Atomic exchange operations ```wgsl // Simple exchange let old = atomicExchange(&lock, 1u); // Compare and exchange (weak) let result = atomicCompareExchangeWeak(&value, expected, newValue); if (result.exchanged) { // Successfully updated } else { // Failed, result.old_value contains current value } ``` Spurious Failure `atomicCompareExchangeWeak` can fail even when values match. Always check `result.exchanged` and retry in a loop. ## Common Patterns [Section titled “Common Patterns”](#common-patterns) ## Memory Ordering [Section titled “Memory Ordering”](#memory-ordering) ### Acquire-Release Semantics [Section titled “Acquire-Release Semantics”](#acquire-release-semantics) * **Atomic write** (release): All prior memory ops complete before write visible * **Atomic read** (acquire): Read completes before subsequent memory ops begin ### Synchronization with Barriers [Section titled “Synchronization with Barriers”](#synchronization-with-barriers) Combining atomics with barriers ```wgsl var<workgroup> ready: atomic<u32>; var<workgroup> data: f32; @compute @workgroup_size(64) fn process(@builtin(local_invocation_index) idx: u32) { if (idx == 0u) { data = 42.0; workgroupBarrier(); // Ensure write completes atomicStore(&ready, 1u); } // Consumer threads if (idx > 0u) { loop { if (atomicLoad(&ready) == 1u) { break; } } workgroupBarrier(); // Ensure we see data write let value = data; } } ``` ## Performance Considerations [Section titled “Performance Considerations”](#performance-considerations) ### Throughput Impact [Section titled “Throughput Impact”](#throughput-impact) | Operation | Relative Cost | | ------------------------ | ------------- | | Regular memory | 1× | | Atomic (low contention) | 10-100× | | Atomic (high contention) | 100-1000× | ## TypeGPU Atomic Support [Section titled “TypeGPU Atomic Support”](#typegpu-atomic-support) TypeGPU atomic types ```typescript import { d } from "typegpu/data"; import { std } from "typegpu/std"; const atomicCounter = d.atomic(d.u32); const CounterBuffer = d.struct({ hitCount: d.atomic(d.u32), missCount: d.atomic(d.u32), }); // In shader const shader = tgpu.computeFn([counterBuffer], () => { const old = std.atomicAdd(counterBuffer.value.hitCount, 1); }); ``` ## Common Pitfalls [Section titled “Common Pitfalls”](#common-pitfalls) Race on Non-Atomic Data ```wgsl // Atomics only protect the atomic variable itself! let idx = atomicAdd(&counter, 1u); data[idx] = value; // data array is NOT protected // Use workgroupBarrier() after atomics if other threads read data ``` Spinlock Deadlock ```wgsl // DANGEROUS: GPU threads in same warp execute in lockstep loop { if (atomicExchange(&lock, 1u) == 0u) { break; } } // One thread holding lock blocks others in same warp forever ``` **Solution**: Avoid locks on GPUs. Redesign for lock-free algorithms. Missing Barrier for Non-Atomic Memory ```wgsl // Thread A result = 42.0; atomicStore(&ready, 1u); // Thread B - might see old value! if (atomicLoad(&ready) == 1u) { let value = result; // Not guaranteed to be 42.0 } // Fix: Add workgroupBarrier() before and after the atomic ``` Unnecessary Atomics ```wgsl // Bad: Atomic when each thread writes unique location atomicStore(&output[gid.x], value); // Good: Regular store is sufficient output[gid.x] = value; ``` ## Resources [Section titled “Resources”](#resources)

# Coordinate Systems and Clip Space

## Overview [Section titled “Overview”](#overview) WebGPU adopts coordinate conventions aligned with modern APIs (Vulkan, Metal, Direct3D 12), differing from WebGL/OpenGL in two critical areas: ## Coordinate Spaces Pipeline [Section titled “Coordinate Spaces Pipeline”](#coordinate-spaces-pipeline) | Stage | Description | | ---------------- | ------------------------------------------- | | **Object Space** | Local coordinates relative to object origin | | **World Space** | Global scene coordinates via model matrix | | **View Space** | Camera-relative coordinates via view matrix | | **Clip Space** | 4D homogeneous output from vertex shader | | **NDC** | After perspective divide (x/w, y/w, z/w) | | **Screen Space** | Final pixel coordinates | ## WebGPU Coordinate Convention [Section titled “WebGPU Coordinate Convention”](#webgpu-coordinate-convention) WebGPU uses a **right-handed coordinate system**: ```plaintext +Y (up) | | +-------- +X (right) / +Z (toward viewer) ``` ### API Comparison [Section titled “API Comparison”](#api-comparison) | API | Depth Range (NDC) | Framebuffer Origin | | ---------------------- | ----------------- | ------------------ | | **WebGPU** | 0 to 1 | Top-left | | **Vulkan/Metal/D3D12** | 0 to 1 | Top-left | | **WebGL/OpenGL** | -1 to 1 | Bottom-left | ## Clip Space and NDC [Section titled “Clip Space and NDC”](#clip-space-and-ndc) ### Clip Space Output [Section titled “Clip Space Output”](#clip-space-output) The vertex shader outputs 4D homogeneous coordinates: Vertex shader clip space output ```wgsl @vertex fn vertexMain(input: Vertex) -> VertexOutput { var output: VertexOutput; let worldPos = uniforms.modelMatrix * vec4f(input.position, 1.0); let viewPos = uniforms.viewMatrix * worldPos; output.position = uniforms.projectionMatrix * viewPos; return output; } ``` ### NDC Ranges in WebGPU [Section titled “NDC Ranges in WebGPU”](#ndc-ranges-in-webgpu) After the perspective divide (automatic after vertex shader): | Axis | Range | Description | | ---- | ----------- | ------------- | | X | -1 to +1 | Left to right | | Y | -1 to +1 | Bottom to top | | Z | **0 to +1** | Near to far | WebGPU Depth Range Z ranges from 0 (near) to 1 (far), not -1 to 1 like WebGL. This requires different projection matrices. ## Screen Space [Section titled “Screen Space”](#screen-space) Origin at **top-left**, Y increases **downward**: ```plaintext (0,0) +---------------------+ | Y increases | | downward ↓ | +---------------------+ (width, height) X increases rightward → ``` ## Projection Matrices [Section titled “Projection Matrices”](#projection-matrices) Don’t Use WebGL Libraries Libraries like `gl-matrix` produce -1 to 1 depth range. Use `wgpu-matrix` for WebGPU. ### Using wgpu-matrix [Section titled “Using wgpu-matrix”](#using-wgpu-matrix) Correct projection for WebGPU ```javascript import { mat4 } from "wgpu-matrix"; // Perspective (0-to-1 depth) const proj = mat4.perspective(fov, aspect, near, far); // Orthographic (0-to-1 depth) const ortho = mat4.ortho(left, right, bottom, top, near, far); ``` ### Manual Perspective Matrix (WebGPU) [Section titled “Manual Perspective Matrix (WebGPU)”](#manual-perspective-matrix-webgpu) WebGPU perspective matrix ```javascript function perspectiveZO(fov, aspect, near, far) { const f = 1.0 / Math.tan(fov / 2); const rangeInv = 1.0 / (near - far); return [ f / aspect, 0, 0, 0, 0, f, 0, 0, 0, 0, far * rangeInv, -1, 0, 0, near * far * rangeInv, 0, ]; } ``` ### Converting WebGL Matrices [Section titled “Converting WebGL Matrices”](#converting-webgl-matrices) If you must convert existing WebGL matrices: Convert -1..1 depth to 0..1 ```javascript function convertWebGLToWebGPU(glMatrix) { const gpuMatrix = glMatrix.slice(); gpuMatrix[8] = (gpuMatrix[8] + gpuMatrix[12]) * 0.5; gpuMatrix[9] = (gpuMatrix[9] + gpuMatrix[13]) * 0.5; gpuMatrix[10] = (gpuMatrix[10] + gpuMatrix[14]) * 0.5; gpuMatrix[11] = (gpuMatrix[11] + gpuMatrix[15]) * 0.5; return gpuMatrix; } ``` ## Texture Coordinates [Section titled “Texture Coordinates”](#texture-coordinates) UV coordinates range 0 to 1: ```plaintext (0,0) +----------+ (1,0) | | | Texture | | | (0,1) +----------+ (1,1) ``` Texture Flipping Most image formats store data top-to-bottom, which may appear upside down. Solutions: 1. **Flip in shader**: `vec2f(uv.x, 1.0 - uv.y)` 2. **Flip on load**: `createImageBitmap(blob, { imageOrientation: "flipY" })` ## Viewport and Scissor [Section titled “Viewport and Scissor”](#viewport-and-scissor) Viewport configuration ```javascript // Full framebuffer renderPass.setViewport(0, 0, canvas.width, canvas.height, 0, 1); // Split-screen (left half) renderPass.setViewport(0, 0, canvas.width / 2, canvas.height, 0, 1); // Scissor rectangle (clips fragments) renderPass.setScissorRect(x, y, width, height); ``` | Function | Behavior | | ---------------- | ------------------------------------ | | `setViewport` | Scales and transforms NDC to screen | | `setScissorRect` | Discards fragments outside rectangle | ## Camera Setup [Section titled “Camera Setup”](#camera-setup) Complete camera setup ```javascript import { mat4 } from "wgpu-matrix"; const cameraPosition = [0, 10, 20]; const target = [0, 0, 0]; const up = [0, 1, 0]; // View matrix (world → view space) const viewMatrix = mat4.lookAt(cameraPosition, target, up); // Projection matrix (view → clip space) const projectionMatrix = mat4.perspective( Math.PI / 4, // 45° FOV canvas.width / canvas.height, 0.1, // Near plane 100.0 // Far plane ); // Combined view-projection const viewProjectionMatrix = mat4.multiply(projectionMatrix, viewMatrix); ``` ## Depth Precision [Section titled “Depth Precision”](#depth-precision) For extreme distances, consider **reverse-Z** (near=1, far=0): Reverse-Z for better precision ```javascript const reverseZProj = mat4.perspective(fov, aspect, far, near); // Requires depthCompare: 'greater' instead of 'less' ``` ## Migration from WebGL [Section titled “Migration from WebGL”](#migration-from-webgl) | Issue | Cause | Fix | | ---------------------- | ---------------------------- | ------------------------- | | Objects not visible | Wrong projection matrix | Use wgpu-matrix | | Textures upside down | Different framebuffer origin | Flip UV or texture data | | Depth test failures | Depth outside \[0,1] | Fix projection matrix | | Y-coordinates inverted | Assuming bottom-left origin | Top-left origin in WebGPU | ## Debugging Coordinates [Section titled “Debugging Coordinates”](#debugging-coordinates) Visualize coordinate spaces ```wgsl @fragment fn debugCoordinates(@builtin(position) fragCoord: vec4f) -> @location(0) vec4f { let depth = fragCoord.z; // [0, 1] in WebGPU let screenNorm = fragCoord.xy / vec2f(800.0, 600.0); return vec4f(screenNorm.x, screenNorm.y, depth, 1.0); } ``` ## Resources [Section titled “Resources”](#resources)

# Performance Optimization

## Overview [Section titled “Overview”](#overview) WebGPU performance optimization requires understanding GPU architecture fundamentals: massive parallelism, memory bandwidth limitations, and CPU-GPU synchronization overhead. ## GPU Architecture [Section titled “GPU Architecture”](#gpu-architecture) ### Memory Hierarchy [Section titled “Memory Hierarchy”](#memory-hierarchy) | Level | Speed | Scope | | ----------------------- | ------- | ------------- | | Registers | Fastest | Per-thread | | Shared/Workgroup Memory | Fast | Per-workgroup | | L1/L2 Cache | Medium | Automatic | | Global Memory | Slowest | All threads | ### Key Performance Factors [Section titled “Key Performance Factors”](#key-performance-factors) * **Parallelism**: GPUs run thousands of threads simultaneously * **Latency hiding**: GPUs switch thread groups when one stalls on memory * **Divergence**: Threads in same group taking different paths causes serialization * **Memory bandwidth**: Often the primary bottleneck ## Buffer Management [Section titled “Buffer Management”](#buffer-management) ### Buffer Pooling [Section titled “Buffer Pooling”](#buffer-pooling) ### Interleaved Vertex Attributes [Section titled “Interleaved Vertex Attributes”](#interleaved-vertex-attributes) ### Minimizing Buffer Updates [Section titled “Minimizing Buffer Updates”](#minimizing-buffer-updates) Batch uniform updates ```javascript // Group uniforms by update frequency const frameUniforms = device.createBuffer({ size: 256, ... }); // Per-frame const materialUniforms = device.createBuffer({ size: 1024, ... }); // Per-material const objectUniforms = device.createBuffer({ size: 65536, ... }); // Per-object with offsets // Single write per group instead of many small writes device.queue.writeBuffer(frameUniforms, 0, frameData); ``` Avoid Per-Frame Allocations Creating new buffers every frame triggers garbage collection and wastes GPU resources. Update existing buffers instead. ## Pipeline Optimization [Section titled “Pipeline Optimization”](#pipeline-optimization) ### Pipeline Caching [Section titled “Pipeline Caching”](#pipeline-caching) Cache pipeline objects ```javascript const pipelineCache = new Map(); function getPipeline(config) { const key = JSON.stringify(config); if (!pipelineCache.has(key)) { pipelineCache.set(key, device.createRenderPipeline(config)); } return pipelineCache.get(key); } ``` ### Async Pipeline Creation [Section titled “Async Pipeline Creation”](#async-pipeline-creation) Background pipeline compilation ```javascript // During initialization (prevents frame drops) const pipelines = await Promise.all([ device.createRenderPipelineAsync(desc1), device.createRenderPipelineAsync(desc2), device.createComputePipelineAsync(desc3), ]); ``` ### Reducing State Changes [Section titled “Reducing State Changes”](#reducing-state-changes) ## Memory Optimization [Section titled “Memory Optimization”](#memory-optimization) ### Texture Compression [Section titled “Texture Compression”](#texture-compression) | Format | Platform | Compression | | -------- | ------------------- | ------------- | | BC (DXT) | Desktop | \~75% smaller | | ETC2/EAC | Mobile | \~75% smaller | | ASTC | Mobile/Some Desktop | Variable | Check compression support ```javascript const hasBCTextures = adapter.features.has("texture-compression-bc"); const hasETC2 = adapter.features.has("texture-compression-etc2"); ``` ### Mipmaps [Section titled “Mipmaps”](#mipmaps) ### Staging Buffers [Section titled “Staging Buffers”](#staging-buffers) Ring buffer for streaming data ```javascript const RING_SIZE = 3; const stagingBuffers = Array.from({ length: RING_SIZE }, () => device.createBuffer({ size: bufferSize, usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC, }) ); let frameIndex = 0; function getNextStagingBuffer() { return stagingBuffers[frameIndex++ % RING_SIZE]; } ``` ## Compute Optimization [Section titled “Compute Optimization”](#compute-optimization) ### Workgroup Size [Section titled “Workgroup Size”](#workgroup-size) ### Memory Coalescing [Section titled “Memory Coalescing”](#memory-coalescing) Coalesced vs scattered access ```wgsl // Good: Adjacent threads access adjacent memory let value = input[global_id.x]; // Bad: Scattered access let value = input[global_id.x * stride]; ``` ### Shared Memory [Section titled “Shared Memory”](#shared-memory) Use workgroup memory for data reuse ```wgsl var<workgroup> cache: array<f32, 256>; @compute @workgroup_size(256) fn compute(@builtin(local_invocation_id) lid: vec3u) { // Load from global to shared cache[lid.x] = globalData[gid.x]; workgroupBarrier(); // Multiple reads from fast shared memory var sum = 0.0; for (var i = 0u; i < 256u; i++) { sum += cache[i]; } } ``` ## Instancing [Section titled “Instancing”](#instancing) Render thousands with one draw call ```javascript // Per-instance buffer const instanceBuffer = device.createBuffer({ size: instanceCount * 64, // 4x4 matrix per instance usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }); // Vertex layout with instance step mode buffers: [ { arrayStride: 12, stepMode: "vertex", ... }, { arrayStride: 64, stepMode: "instance", ... }, ] // Single draw call for all instances passEncoder.draw(vertexCount, instanceCount); ``` ## Shader Optimization [Section titled “Shader Optimization”](#shader-optimization) ### Avoid Branching [Section titled “Avoid Branching”](#avoid-branching) Replace branches with math ```wgsl // Bad: Causes divergence if (value > threshold) { result = a; } else { result = b; } // Good: No divergence result = mix(b, a, step(threshold, value)); ``` ### Use Built-in Functions [Section titled “Use Built-in Functions”](#use-built-in-functions) Built-ins like `dot()`, `normalize()`, `mix()`, `smoothstep()` map directly to hardware instructions and are significantly faster than manual implementations. ## Profiling [Section titled “Profiling”](#profiling) ### Timestamp Queries [Section titled “Timestamp Queries”](#timestamp-queries) Measure GPU execution time ```javascript const querySet = device.createQuerySet({ type: "timestamp", count: 2, }); const resolveBuffer = device.createBuffer({ size: 16, usage: GPUBufferUsage.QUERY_RESOLVE | GPUBufferUsage.COPY_SRC, }); // In render pass passEncoder.writeTimestamp(querySet, 0); // ... render commands ... passEncoder.writeTimestamp(querySet, 1); // Resolve and read encoder.resolveQuerySet(querySet, 0, 2, resolveBuffer, 0); ``` ### Browser DevTools [Section titled “Browser DevTools”](#browser-devtools) * **Chrome**: Performance panel, about://gpu * **Firefox**: about:support, Performance profiler * **Platform tools**: PIX (Windows), Instruments (macOS) ## Common Bottlenecks [Section titled “Common Bottlenecks”](#common-bottlenecks) ## Quick Checklist [Section titled “Quick Checklist”](#quick-checklist) | Area | Optimization | | -------------- | -------------------------------------------------------------- | | **Buffers** | Pool and reuse; interleave vertices; batch updates | | **Pipelines** | Cache objects; use async creation; sort by pipeline | | **Memory** | Compress textures; generate mipmaps; use staging buffers | | **Draw Calls** | Use instancing; batch by material; minimize state changes | | **Shaders** | Avoid branching; use built-ins; move work to vertex stage | | **Compute** | Use workgroup size 64; coalesce access; leverage shared memory | | **Profiling** | Measure before optimizing; use timestamp queries | ## Resources [Section titled “Resources”](#resources)

# React Integration

## Overview [Section titled “Overview”](#overview) WebGPU integrates with React through careful lifecycle management. This guide covers patterns for React web applications and React Native with `react-native-wgpu`. ## React Web Patterns [Section titled “React Web Patterns”](#react-web-patterns) ### useWebGPU Hook [Section titled “useWebGPU Hook”](#usewebgpu-hook) useWebGPU.ts ```typescript import { useEffect, useRef, useState } from "react"; interface WebGPUContext { device: GPUDevice; context: GPUCanvasContext; format: GPUTextureFormat; } export function useWebGPU(canvasRef: React.RefObject<HTMLCanvasElement>) { const [gpu, setGpu] = useState<WebGPUContext | null>(null); const [error, setError] = useState<string | null>(null); useEffect(() => { let destroyed = false; async function init() { if (!navigator.gpu) { setError("WebGPU not supported"); return; } const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { setError("No GPU adapter found"); return; } const device = await adapter.requestDevice(); if (destroyed) { device.destroy(); return; } const canvas = canvasRef.current; if (!canvas) return; const context = canvas.getContext("webgpu"); if (!context) { setError("Failed to get WebGPU context"); return; } const format = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device, format }); setGpu({ device, context, format }); } init(); return () => { destroyed = true; gpu?.device.destroy(); }; }, [canvasRef]); return { gpu, error }; } ``` ### Canvas Component [Section titled “Canvas Component”](#canvas-component) WebGPUCanvas.tsx ```tsx import { useRef, useEffect } from "react"; import { useWebGPU } from "./useWebGPU"; interface Props { onReady: (gpu: WebGPUContext) => void; onFrame?: (gpu: WebGPUContext, time: number) => void; } export function WebGPUCanvas({ onReady, onFrame }: Props) { const canvasRef = useRef<HTMLCanvasElement>(null); const { gpu, error } = useWebGPU(canvasRef); const frameRef = useRef<number>(); useEffect(() => { if (!gpu) return; onReady(gpu); if (onFrame) { function loop(time: number) { onFrame!(gpu!, time); frameRef.current = requestAnimationFrame(loop); } frameRef.current = requestAnimationFrame(loop); return () => { if (frameRef.current) cancelAnimationFrame(frameRef.current); }; } }, [gpu, onReady, onFrame]); if (error) return <div>Error: {error}</div>; return <canvas ref={canvasRef} width={800} height={600} />; } ``` ### TypeGPU with React [Section titled “TypeGPU with React”](#typegpu-with-react) TypeGPU React component ```tsx import { useEffect, useRef, useState } from "react"; import tgpu, { type TgpuRoot } from "typegpu"; import * as d from "typegpu/data"; export function ParticleSystem() { const canvasRef = useRef<HTMLCanvasElement>(null); const [root, setRoot] = useState<TgpuRoot | null>(null); useEffect(() => { let tgpuRoot: TgpuRoot | null = null; async function init() { tgpuRoot = await tgpu.init({ canvas: canvasRef.current!, }); setRoot(tgpuRoot); } init(); return () => { tgpuRoot?.destroy(); }; }, []); useEffect(() => { if (!root) return; // Create GPU resources const particles = root .createBuffer(d.arrayOf(d.vec4f, 1000)) .$usage("storage"); // Setup render loop... }, [root]); return <canvas ref={canvasRef} width={800} height={600} />; } ``` ### Resource Management [Section titled “Resource Management”](#resource-management) Cleanup is Critical GPU resources must be explicitly destroyed. Failing to cleanup causes memory leaks: ```tsx useEffect(() => { const buffer = device.createBuffer({ /* ... */ }); const texture = device.createTexture({ /* ... */ }); return () => { buffer.destroy(); texture.destroy(); }; }, [device]); ``` ### State Management [Section titled “State Management”](#state-management) GPU state with React state ```tsx function useGPUBuffer<T>(root: TgpuRoot, schema: TgpuDataType<T>) { const [data, setData] = useState<T | null>(null); const bufferRef = useRef<TgpuBuffer<T> | null>(null); useEffect(() => { bufferRef.current = root .createBuffer(schema) .$usage("storage", "uniform"); return () => { bufferRef.current?.destroy(); }; }, [root, schema]); const write = useCallback((newData: T) => { bufferRef.current?.write(newData); setData(newData); }, []); return { buffer: bufferRef.current, data, write }; } ``` ## React Native [Section titled “React Native”](#react-native) ### Requirements [Section titled “Requirements”](#requirements) * React Native 0.81+ * New Architecture enabled * `react-native-wgpu` package ### Setup [Section titled “Setup”](#setup) ```bash npm install react-native-wgpu typegpu npm install -D @webgpu/types ``` tsconfig.json ```json { "compilerOptions": { "types": ["@webgpu/types"] } } ``` babel.config.js (for TGSL) ```javascript module.exports = { plugins: ["unplugin-typegpu/babel"], }; ``` Expo Users React Native WebGPU is not supported by Expo Go. Run `expo prebuild` and build the native app. ### Basic Example [Section titled “Basic Example”](#basic-example) React Native WebGPU ```tsx import { Canvas, useDevice, useGPUContext } from "react-native-wgpu"; import tgpu from "typegpu"; import * as d from "typegpu/data"; function Triangle() { const device = useDevice(); const context = useGPUContext(); useEffect(() => { if (!device || !context) return; const format = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device, format }); // Create shaders and pipeline const vertexFn = tgpu.vertexFn({ idx: d.builtin.vertexIndex, }, d.vec4f).does`(input) -> vec4f { const positions = array<vec2f, 3>( vec2f(0.0, 0.5), vec2f(-0.5, -0.5), vec2f(0.5, -0.5) ); return vec4f(positions[input.idx], 0.0, 1.0); }`; const fragmentFn = tgpu.fragmentFn({}, d.vec4f) .does`() -> @location(0) vec4f { return vec4f(1.0, 0.5, 0.0, 1.0); }`; // Build pipeline and render... }, [device, context]); return <Canvas style={{ flex: 1 }} />; } ``` ### Worklets Integration [Section titled “Worklets Integration”](#worklets-integration) For smooth animations, use Reanimated worklets: WebGPU Worklets ```tsx import { useSharedContext } from "react-native-webgpu-worklets"; function AnimatedScene() { const sharedContext = useSharedContext(); // GPU work runs on UI thread via worklets const renderWorklet = useCallback(() => { "worklet"; const { device, context } = sharedContext.value; // Render frame... }, [sharedContext]); return <Canvas onFrame={renderWorklet} />; } ``` ## Performance Tips [Section titled “Performance Tips”](#performance-tips) ### Minimize Re-renders [Section titled “Minimize Re-renders”](#minimize-re-renders) Memoize GPU-dependent components ```tsx const ParticleRenderer = memo(function ParticleRenderer({ particleBuffer, }: { particleBuffer: GPUBuffer; }) { // Only re-renders when buffer reference changes }); ``` ### Batch State Updates [Section titled “Batch State Updates”](#batch-state-updates) Avoid per-frame state updates ```tsx // Bad: causes re-render every frame const [frameCount, setFrameCount] = useState(0); function render() { setFrameCount(c => c + 1); // Don't do this } // Good: use refs for frame-local data const frameCountRef = useRef(0); function render() { frameCountRef.current++; // No re-render } ``` ### Stable References [Section titled “Stable References”](#stable-references) Stable callback references ```tsx const renderFrame = useCallback((time: number) => { // Render logic }, []); // Empty deps = stable reference useEffect(() => { const id = requestAnimationFrame(renderFrame); return () => cancelAnimationFrame(id); }, [renderFrame]); ``` ## Common Patterns [Section titled “Common Patterns”](#common-patterns) ### Resize Handling [Section titled “Resize Handling”](#resize-handling) Handle canvas resize ```tsx function useCanvasSize(canvasRef: React.RefObject<HTMLCanvasElement>) { const [size, setSize] = useState({ width: 0, height: 0 }); useEffect(() => { const observer = new ResizeObserver(([entry]) => { const { width, height } = entry.contentRect; setSize({ width, height }); }); if (canvasRef.current) { observer.observe(canvasRef.current); } return () => observer.disconnect(); }, [canvasRef]); return size; } ``` ### Error Boundaries [Section titled “Error Boundaries”](#error-boundaries) GPU error boundary ```tsx class GPUErrorBoundary extends React.Component< { children: React.ReactNode; fallback: React.ReactNode }, { hasError: boolean } > { state = { hasError: false }; static getDerivedStateFromError() { return { hasError: true }; } render() { if (this.state.hasError) { return this.props.fallback; } return this.props.children; } } ``` ## Resources [Section titled “Resources”](#resources)

# Buffers and Memory Management

## Overview [Section titled “Overview”](#overview) Buffers are contiguous regions of GPU memory that enable data transfer between JavaScript and shader programs. Understanding buffers and memory management is essential for building efficient WebGPU applications. Buffers serve multiple purposes: * **Vertex buffers** — Store geometry data for rendering * **Index buffers** — Enable vertex reuse across triangles * **Uniform buffers** — Hold read-only shader constants * **Storage buffers** — Provide large read-write memory for compute The WebGPU specification initializes all resources to zero, preventing information leakage from other applications or previous GPU operations. ## Buffer Types [Section titled “Buffer Types”](#buffer-types) ### Vertex Buffers [Section titled “Vertex Buffers”](#vertex-buffers) Vertex buffers store per-vertex attribute data including positions, normals, texture coordinates, and colors. Unlike storage buffers accessed through array indexing, vertex buffers use an attribute system where WebGPU extracts data for each vertex automatically. Creating a vertex buffer ```javascript const vertices = new Float32Array([ // x, y, r, g, b, a -0.5, -0.5, 1.0, 0.0, 0.0, 1.0, 0.5, -0.5, 0.0, 1.0, 0.0, 1.0, 0.0, 0.5, 0.0, 0.0, 1.0, 1.0, ]); const vertexBuffer = device.createBuffer({ label: "triangle vertices", size: vertices.byteLength, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer(vertexBuffer, 0, vertices); ``` Vertex buffers support per-vertex and per-instance stepping modes for efficient instanced rendering. ### Index Buffers [Section titled “Index Buffers”](#index-buffers) Index buffers reduce memory by allowing vertex reuse. Instead of duplicating vertices, indices point to vertices in the vertex buffer: Creating an index buffer ```javascript const indices = new Uint32Array([ 0, 1, 2, // first triangle 2, 1, 3, // second triangle (reuses vertices 1 and 2) ]); const indexBuffer = device.createBuffer({ label: "quad indices", size: indices.byteLength, usage: GPUBufferUsage.INDEX | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer(indexBuffer, 0, indices); // Usage in render pass pass.setIndexBuffer(indexBuffer, "uint32"); pass.drawIndexed(6, 1); ``` | Format | Max Vertices | Use Case | | -------- | ------------ | ------------------------------ | | `uint16` | 65,536 | Smaller meshes, memory savings | | `uint32` | 4+ billion | Large geometries | ### Uniform Buffers [Section titled “Uniform Buffers”](#uniform-buffers) Uniform buffers hold small, read-only constants that remain unchanged during shader execution—transformation matrices, lighting parameters, material properties. Creating a uniform buffer ```javascript const uniformData = new Float32Array([ 1.0, 0.0, 0.0, 0.0, // matrix row 1 0.0, 1.0, 0.0, 0.0, // matrix row 2 0.0, 0.0, 1.0, 0.0, // matrix row 3 0.5, 0.5, 0.0, 1.0, // matrix row 4 (translation) ]); const uniformBuffer = device.createBuffer({ label: "transform uniforms", size: uniformData.byteLength, usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer(uniformBuffer, 0, uniformData); ``` Size Limit Uniform buffers are limited to 64 KiB (65,536 bytes). This constraint encourages efficient data packing and makes them unsuitable for large datasets—use storage buffers instead. Uniform buffers typically offer better performance than storage buffers for small, frequently accessed data due to aggressive GPU caching. ### Storage Buffers [Section titled “Storage Buffers”](#storage-buffers) Storage buffers provide large-capacity read-write memory with a guaranteed maximum of 128 MiB—over 2,000 times larger than uniform buffers: Creating a storage buffer ```javascript const particleCount = 10000; const particleData = new Float32Array(particleCount * 4); // x, y, vx, vy const storageBuffer = device.createBuffer({ label: "particle data", size: particleData.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST | GPUBufferUsage.COPY_SRC, }); device.queue.writeBuffer(storageBuffer, 0, particleData); ``` Storage buffers support both `var<storage, read>` (read-only) and `var<storage, read_write>` (read-write) access in WGSL. ## Buffer Creation [Section titled “Buffer Creation”](#buffer-creation) Buffer descriptor options ```javascript const buffer = device.createBuffer({ label: "descriptive name", // Aids debugging size: 1024, // Size in bytes (required) usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, mappedAtCreation: false, // Optional, default false }); ``` ### Usage Flags [Section titled “Usage Flags”](#usage-flags) Usage flags declare all ways a buffer will be used. Combine multiple flags with bitwise OR (`|`). Invalid Combinations `MAP_READ` and `MAP_WRITE` cannot be combined with `STORAGE` or `UNIFORM`. Mappable buffers reside in CPU-accessible memory, while storage/uniform buffers need fast GPU-local memory. Use staging buffers for transfers. Valid buffer configurations ```javascript // Staging buffer for reading compute results const readbackBuffer = device.createBuffer({ size: 256, usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST, }); // Storage buffer for GPU operations const computeBuffer = device.createBuffer({ size: 256, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST, }); ``` ### Size and Alignment [Section titled “Size and Alignment”](#size-and-alignment) | Buffer Type | Alignment Requirement | | ---------------- | -------------------------------------- | | Uniform bindings | 256 bytes | | Storage bindings | 32 bytes (some implementations) | | Copy operations | Source and destination offsets aligned | Always ensure buffers accommodate data plus required padding. Misaligned access causes validation errors or incorrect data interpretation. ### mappedAtCreation [Section titled “mappedAtCreation”](#mappedatcreation) Efficient initialization without separate mapping operations: Initialize buffer at creation ```javascript const buffer = device.createBuffer({ size: 256, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); // Buffer is immediately mapped for writing const arrayBuffer = buffer.getMappedRange(); const float32View = new Float32Array(arrayBuffer); float32View.set([1.0, 2.0, 3.0, 4.0]); // Unmap to make available for GPU buffer.unmap(); ``` This pattern is ideal for static geometry and lookup tables initialized once. ## TypeGPU Buffer Creation [Section titled “TypeGPU Buffer Creation”](#typegpu-buffer-creation) TypeGPU simplifies buffer management with typed schemas and automatic memory layout calculations. ### Basic Buffer Creation [Section titled “Basic Buffer Creation”](#basic-buffer-creation) TypeGPU buffer creation ```typescript import * as d from "typegpu/data"; // Single primitive value const counterBuffer = root.createBuffer(d.u32); // Array of values const positionsBuffer = root.createBuffer(d.arrayOf(d.vec3f, 100)); // Structured data const particleBuffer = root.createBuffer(d.struct({ position: d.vec3f, velocity: d.vec3f, health: d.f32, age: d.f32, })); ``` TypeGPU automatically calculates buffer size, applies alignment rules, and generates corresponding WGSL type definitions. ### Usage Flags [Section titled “Usage Flags”](#usage-flags-1) Chaining usage flags ```typescript const buffer = root .createBuffer(d.arrayOf(d.f32, 1000)) .$usage("storage") .$usage("vertex"); ``` | TypeGPU Usage | WebGPU Equivalent | | ------------- | ------------------------ | | `'uniform'` | `GPUBufferUsage.UNIFORM` | | `'storage'` | `GPUBufferUsage.STORAGE` | | `'vertex'` | `GPUBufferUsage.VERTEX` | ### Initial Values [Section titled “Initial Values”](#initial-values) Type-safe initialization ```typescript // Single value const timeBuffer = root.createBuffer(d.f32, 0.0); // Array initialization const colorsBuffer = root.createBuffer(d.arrayOf(d.vec3f, 3), [ d.vec3f(1, 0, 0), d.vec3f(0, 1, 0), d.vec3f(0, 0, 1), ]); // Struct initialization const cameraBuffer = root.createBuffer( d.struct({ position: d.vec3f, fov: d.f32, }), { position: d.vec3f(0, 5, 10), fov: 45.0, } ); ``` The type system validates initial values against the schema at compile time. ### Automatic Serialization [Section titled “Automatic Serialization”](#automatic-serialization) TypeGPU handles binary serialization following WebGPU alignment rules: Automatic binary conversion ```typescript const transformBuffer = root.createBuffer(d.struct({ scale: d.vec3f, rotation: d.vec4f, translation: d.vec3f, })); // Type-safe write with automatic serialization transformBuffer.write({ scale: d.vec3f(2, 2, 2), rotation: d.vec4f(0, 0, 0, 1), translation: d.vec3f(10, 0, 0), }); ``` This eliminates manual byte packing and ensures correct alignment. ## Reading and Writing Data [Section titled “Reading and Writing Data”](#reading-and-writing-data) ### WebGPU: queue.writeBuffer() [Section titled “WebGPU: queue.writeBuffer()”](#webgpu-queuewritebuffer) The simplest method for CPU-to-GPU data transfer: Direct buffer write ```javascript const data = new Float32Array([1.0, 2.0, 3.0, 4.0]); device.queue.writeBuffer(buffer, 0, data); ``` Data is copied immediately—you can safely modify the source array after the call. The write executes before any subsequently queued GPU operations referencing the buffer. ### Buffer Mapping for Read Access [Section titled “Buffer Mapping for Read Access”](#buffer-mapping-for-read-access) Reading GPU results requires a multi-step process with staging buffers: Reading GPU results ```javascript // 1. GPU-local result buffer const resultBuffer = device.createBuffer({ size: 256, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC, }); // 2. CPU-accessible staging buffer const stagingBuffer = device.createBuffer({ size: 256, usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST, }); // 3. Execute compute work... // 4. Copy GPU results to staging buffer const encoder = device.createCommandEncoder(); encoder.copyBufferToBuffer(resultBuffer, 0, stagingBuffer, 0, 256); device.queue.submit([encoder.finish()]); // 5. Map staging buffer for reading await stagingBuffer.mapAsync(GPUMapMode.READ); const data = new Float32Array(stagingBuffer.getMappedRange()); console.log("Results:", Array.from(data)); // 6. Unmap when done stagingBuffer.unmap(); ``` ### Map/Unmap Lifecycle [Section titled “Map/Unmap Lifecycle”](#mapunmap-lifecycle) | State | Description | | ------------------- | ------------------------------------------- | | **Unmapped** | Default state; available for GPU commands | | **Mapping Pending** | After `mapAsync()`, before promise resolves | | **Mapped** | CPU can access via `getMappedRange()` | | **Unmapped** | After `unmap()`, returns to GPU-accessible | Critical Rule A buffer can only be used in GPU operations while unmapped. Using a mapped buffer in commands causes validation errors. ### TypeGPU: Type-Safe I/O [Section titled “TypeGPU: Type-Safe I/O”](#typegpu-type-safe-io) TypeGPU read/write ```typescript const particleBuffer = root.createBuffer(d.struct({ position: d.vec2f, velocity: d.vec2f, mass: d.f32, })); // Type-checked write particleBuffer.write({ position: d.vec2f(100, 200), velocity: d.vec2f(1.5, -2.0), mass: 1.0, }); // Type-safe read const result = await particleBuffer.read(); console.log("Position:", result.position); console.log("Velocity:", result.velocity); ``` TypeGPU handles staging buffers, copies, mapping, and deserialization transparently. ## Memory Visibility [Section titled “Memory Visibility”](#memory-visibility) ### GPU vs CPU Memory [Section titled “GPU vs CPU Memory”](#gpu-vs-cpu-memory) Modern GPUs contain high-bandwidth memory (GDDR/HBM) optimized for parallel shader access. This GPU-local memory isn’t directly CPU-accessible. | Memory Type | Access | Speed | Use | | -------------- | -------------- | ------- | ------------------------------- | | GPU-local | GPU only | Fastest | Storage, uniform, vertex, index | | CPU-accessible | CPU + slow GPU | Slower | Mappable buffers | ### Staging Buffer Pattern [Section titled “Staging Buffer Pattern”](#staging-buffer-pattern) Complete staging buffer workflow ```javascript // GPU-side storage buffer const gpuBuffer = device.createBuffer({ size: 4096, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC | GPUBufferUsage.COPY_DST, }); // CPU-accessible staging buffers const uploadStaging = device.createBuffer({ size: 4096, usage: GPUBufferUsage.MAP_WRITE | GPUBufferUsage.COPY_SRC, }); const downloadStaging = device.createBuffer({ size: 4096, usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST, }); // Upload: CPU -> Staging -> GPU await uploadStaging.mapAsync(GPUMapMode.WRITE); new Float32Array(uploadStaging.getMappedRange()).set(cpuData); uploadStaging.unmap(); const encoder = device.createCommandEncoder(); encoder.copyBufferToBuffer(uploadStaging, 0, gpuBuffer, 0, 4096); device.queue.submit([encoder.finish()]); // Download: GPU -> Staging -> CPU const downloadEncoder = device.createCommandEncoder(); downloadEncoder.copyBufferToBuffer(gpuBuffer, 0, downloadStaging, 0, 4096); device.queue.submit([downloadEncoder.finish()]); await downloadStaging.mapAsync(GPUMapMode.READ); const results = new Float32Array(downloadStaging.getMappedRange()).slice(); downloadStaging.unmap(); ``` ## Buffer Update Patterns [Section titled “Buffer Update Patterns”](#buffer-update-patterns) ### Frame-by-Frame Updates [Section titled “Frame-by-Frame Updates”](#frame-by-frame-updates) Per-frame uniform update ```javascript function frame(time) { const uniforms = new Float32Array([ time * 0.001, Math.sin(time * 0.001), Math.cos(time * 0.001), 0.0, ]); device.queue.writeBuffer(uniformBuffer, 0, uniforms); const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass(renderPassDescriptor); // ... rendering commands ... pass.end(); device.queue.submit([encoder.finish()]); requestAnimationFrame(frame); } ``` ### Double Buffering [Section titled “Double Buffering”](#double-buffering) For larger updates or when GPU work spans multiple frames: Ring buffer pattern ```javascript const bufferSize = 65536; const numBuffers = 2; let currentBufferIndex = 0; const buffers = Array.from({ length: numBuffers }, (_, i) => device.createBuffer({ label: `ring buffer ${i}`, size: bufferSize, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }) ); function frame() { const buffer = buffers[currentBufferIndex]; const nextIndex = (currentBufferIndex + 1) % numBuffers; // Update NEXT buffer while GPU processes current device.queue.writeBuffer(buffers[nextIndex], 0, nextFrameData); // Render with current buffer... currentBufferIndex = nextIndex; requestAnimationFrame(frame); } ``` Write-While-In-Use Never write to a buffer currently in use by the GPU. Commands submitted via `queue.submit()` execute asynchronously—use separate buffers or wait for completion. ## Buffer Pooling [Section titled “Buffer Pooling”](#buffer-pooling) Creating and destroying buffers frequently causes overhead and fragmentation. Maintain pools of reusable buffers: Buffer pool implementation ```javascript class BufferPool { constructor(device, size, usage) { this.device = device; this.size = size; this.usage = usage; this.available = []; this.inUse = new Set(); } acquire() { let buffer = this.available.pop(); if (!buffer) { buffer = this.device.createBuffer({ size: this.size, usage: this.usage, }); } this.inUse.add(buffer); return buffer; } release(buffer) { if (this.inUse.delete(buffer)) { this.available.push(buffer); } } destroy() { [...this.available, ...this.inUse].forEach(b => b.destroy()); } } ``` ### Sub-allocation [Section titled “Sub-allocation”](#sub-allocation) Store multiple logical buffers in one physical buffer: Large buffer with sub-allocations ```javascript const largeBuffer = device.createBuffer({ size: 1048576, // 1 MiB usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); // Write different uniforms at different offsets device.queue.writeBuffer(largeBuffer, 0, cameraUniforms); device.queue.writeBuffer(largeBuffer, 256, lightUniforms); device.queue.writeBuffer(largeBuffer, 512, materialUniforms); // Bind with offsets const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [{ binding: 0, resource: { buffer: largeBuffer, offset: 0, size: 256 }, }], }); ``` ### Batching Updates [Section titled “Batching Updates”](#batching-updates) Batch vs individual writes ```javascript // ✗ Inefficient: many small writes for (let i = 0; i < 100; i++) { device.queue.writeBuffer(buffer, i * 16, objects[i].transform); } // ✓ Efficient: single batched write const batchedData = new Float32Array(100 * 4); for (let i = 0; i < 100; i++) { batchedData.set(objects[i].transform, i * 4); } device.queue.writeBuffer(buffer, 0, batchedData); ``` ## Alignment Considerations [Section titled “Alignment Considerations”](#alignment-considerations) WGSL Alignment Rules WGSL struct members have alignment requirements. `vec3<f32>` has 16-byte alignment despite being 12 bytes: ```wgsl struct Uniforms { color: vec3<f32>, // offset 0, takes 16 bytes intensity: f32, // offset 16 } ``` When creating typed arrays manually, account for padding: ```javascript // ✗ Wrong: only 4 floats const data = new Float32Array([1, 0, 0, 0.5]); // ✓ Correct: explicit padding for vec3 const data = new Float32Array([ 1, 0, 0, 0, // vec3 color (4th element is padding) 0.5, 0, 0, 0, // f32 intensity (3 padding floats) ]); ``` TypeGPU eliminates these issues by handling alignment automatically. ## Resource Cleanup [Section titled “Resource Cleanup”](#resource-cleanup) Memory Leaks Buffers aren’t automatically garbage collected. Destroy them explicitly: ```javascript const tempBuffer = device.createBuffer({ size: 1024, usage: GPUBufferUsage.STORAGE, }); // ... use buffer ... tempBuffer.destroy(); // Free GPU memory ``` Failing to destroy buffers leads to GPU memory exhaustion in long-running applications.

# Loading Images and Textures

## Overview [Section titled “Overview”](#overview) WebGPU provides multiple methods for loading image data into textures. The approach depends on your source: files, canvas elements, video frames, or compressed formats. ## Loading from URLs [Section titled “Loading from URLs”](#loading-from-urls) ### Basic Image Loading [Section titled “Basic Image Loading”](#basic-image-loading) Load image from URL ```javascript async function loadTexture(device, url) { // 1. Fetch and decode const response = await fetch(url); const blob = await response.blob(); const imageBitmap = await createImageBitmap(blob); // 2. Create texture const texture = device.createTexture({ size: [imageBitmap.width, imageBitmap.height], format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST, }); // 3. Upload to GPU device.queue.copyExternalImageToTexture( { source: imageBitmap }, { texture }, [imageBitmap.width, imageBitmap.height] ); return texture; } ``` ### createImageBitmap Options [Section titled “createImageBitmap Options”](#createimagebitmap-options) Control decode behavior ```javascript const imageBitmap = await createImageBitmap(blob, { // Prevent color space conversion (preserve raw values) colorSpaceConversion: "none", // Resize during decode (more efficient than GPU resize) resizeWidth: 512, resizeHeight: 512, resizeQuality: "high", // "pixelated" | "low" | "medium" | "high" // Pre-multiply alpha (matches WebGPU default) premultiplyAlpha: "premultiply", // "none" | "premultiply" | "default" }); ``` ## copyExternalImageToTexture [Section titled “copyExternalImageToTexture”](#copyexternalimagetotexture) ### Parameters [Section titled “Parameters”](#parameters) Full parameter options ```javascript device.queue.copyExternalImageToTexture( // Source { source: imageBitmap, // ImageBitmap, HTMLCanvasElement, HTMLVideoElement, etc. origin: [0, 0], // Optional: source region offset flipY: false, // Optional: flip vertically }, // Destination { texture: gpuTexture, origin: [0, 0, 0], // Optional: write location [x, y, layer] mipLevel: 0, // Optional: target mip level colorSpace: "srgb", // Optional: "srgb" | "display-p3" premultipliedAlpha: false, // Optional: premultiply RGB by alpha }, // Copy size [width, height] // Or { width, height, depthOrArrayLayers } ); ``` ### Valid Source Types [Section titled “Valid Source Types”](#valid-source-types) | Source | Notes | | ------------------- | ----------------------------------------------------- | | `ImageBitmap` | Preferred—async decode | | `HTMLCanvasElement` | Direct copy, no decode needed | | `OffscreenCanvas` | Worker-compatible | | `HTMLVideoElement` | Current frame (use `importExternalTexture` for video) | | `VideoFrame` | WebCodecs integration | | `ImageData` | From canvas `getImageData()` | Avoid HTMLImageElement Passing `HTMLImageElement` directly can cause synchronous decoding, blocking the main thread. Always use `createImageBitmap()` first. ### Handling Y-Axis Orientation [Section titled “Handling Y-Axis Orientation”](#handling-y-axis-orientation) WebGPU textures have origin at top-left. Some image formats store bottom-to-top: Flip image on upload ```javascript device.queue.copyExternalImageToTexture( { source: imageBitmap, flipY: true }, { texture }, [imageBitmap.width, imageBitmap.height] ); ``` ## Required Texture Usage Flags [Section titled “Required Texture Usage Flags”](#required-texture-usage-flags) Texture for sampling ```javascript const texture = device.createTexture({ size: [width, height], format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | // Use in shaders GPUTextureUsage.COPY_DST, // Required for copyExternalImageToTexture }); ``` If generating mipmaps via render pass, add `RENDER_ATTACHMENT`: Texture with mipmap support ```javascript const mipLevelCount = Math.floor(Math.log2(Math.max(width, height))) + 1; const texture = device.createTexture({ size: [width, height], format: "rgba8unorm", mipLevelCount, usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.RENDER_ATTACHMENT, // For mipmap generation }); ``` ## Canvas Textures [Section titled “Canvas Textures”](#canvas-textures) Canvas elements can be copied directly without `createImageBitmap()`: Copy from canvas ```javascript const canvas = document.querySelector("canvas"); const ctx = canvas.getContext("2d"); // Draw something ctx.fillStyle = "red"; ctx.fillRect(0, 0, 100, 100); // Copy to texture const texture = device.createTexture({ size: [canvas.width, canvas.height], format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST, }); device.queue.copyExternalImageToTexture( { source: canvas }, { texture }, [canvas.width, canvas.height] ); ``` ## Video Textures [Section titled “Video Textures”](#video-textures) ### External Textures (Recommended) [Section titled “External Textures (Recommended)”](#external-textures-recommended) For video playback, `importExternalTexture()` provides a zero-copy fast path: Video texture ```javascript const video = document.createElement("video"); video.src = "video.mp4"; video.loop = true; await video.play(); function frame() { // Create external texture each frame const externalTexture = device.importExternalTexture({ source: video }); // Use in render pass... // Must use immediately—destroyed at end of task requestAnimationFrame(frame); } ``` External Texture Lifetime External textures are automatically destroyed when JavaScript returns to the browser event loop. You must create and use them within the same callback: ```javascript // ✗ Wrong: texture destroyed before use const tex = device.importExternalTexture({ source: video }); await someAsyncOperation(); // Crossing await boundary renderWithTexture(tex); // Error: texture expired // ✓ Correct: create and use in same frame requestAnimationFrame(() => { const tex = device.importExternalTexture({ source: video }); renderWithTexture(tex); }); ``` ### WGSL for External Textures [Section titled “WGSL for External Textures”](#wgsl-for-external-textures) Sampling external texture ```wgsl @group(0) @binding(0) var externalTex: texture_external; @group(0) @binding(1) var samp: sampler; @fragment fn main(@location(0) uv: vec2f) -> @location(0) vec4f { return textureSampleBaseClampToEdge(externalTex, samp, uv); } ``` ### Video via copyExternalImageToTexture [Section titled “Video via copyExternalImageToTexture”](#video-via-copyexternalimagetotexture) For persistent video textures (slower but longer-lived): Copy video frame to regular texture ```javascript function frame() { if (video.readyState >= video.HAVE_CURRENT_DATA) { device.queue.copyExternalImageToTexture( { source: video }, { texture: videoTexture }, [video.videoWidth, video.videoHeight] ); } requestAnimationFrame(frame); } ``` ## Compressed Textures [Section titled “Compressed Textures”](#compressed-textures) Compressed textures reduce GPU memory and bandwidth. WebGPU supports three formats, but availability depends on hardware. ### Format Support [Section titled “Format Support”](#format-support) | Format | Desktop | Mobile | Feature Name | | ------------- | ------------- | -------------- | -------------------------- | | BC (DXT/S3TC) | ✓ Windows/Mac | ✗ | `texture-compression-bc` | | ETC2 | ✗ | ✓ iOS/Android | `texture-compression-etc2` | | ASTC | ✗ | ✓ Newer mobile | `texture-compression-astc` | ### Checking Feature Support [Section titled “Checking Feature Support”](#checking-feature-support) Query compression support ```javascript const adapter = await navigator.gpu.requestAdapter(); const hasBC = adapter.features.has("texture-compression-bc"); const hasETC2 = adapter.features.has("texture-compression-etc2"); const hasASTC = adapter.features.has("texture-compression-astc"); // Request device with needed features const device = await adapter.requestDevice({ requiredFeatures: hasBC ? ["texture-compression-bc"] : [], }); ``` ### Loading Compressed Data [Section titled “Loading Compressed Data”](#loading-compressed-data) Compressed textures use `writeTexture()` instead of `copyExternalImageToTexture()`: Load BC/DXT compressed texture ```javascript // Assume compressedData is Uint8Array from .dds or .ktx2 file const texture = device.createTexture({ size: [width, height], format: "bc1-rgba-unorm", // DXT1 usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST, }); device.queue.writeTexture( { texture }, compressedData, { bytesPerRow: Math.ceil(width / 4) * 8, // BC1: 8 bytes per 4x4 block rowsPerImage: Math.ceil(height / 4), }, [width, height] ); ``` ### Common Compressed Formats [Section titled “Common Compressed Formats”](#common-compressed-formats) ### Universal Texture Containers [Section titled “Universal Texture Containers”](#universal-texture-containers) **KTX2** files can contain multiple formats, transcoded at runtime: KTX2 loading pattern ```javascript // Using a KTX2 loader library import { KTX2Loader } from "ktx2-loader"; const loader = new KTX2Loader(device); const texture = await loader.load("texture.ktx2"); // Automatically transcodes to BC, ETC2, or ASTC based on device ``` ## TypeGPU Image Loading [Section titled “TypeGPU Image Loading”](#typegpu-image-loading) TypeGPU texture from image ```typescript import tgpu from "typegpu"; const root = await tgpu.init(); // Load image const response = await fetch("texture.png"); const blob = await response.blob(); const imageBitmap = await createImageBitmap(blob); // Create texture with TypeGPU const texture = root.createTexture({ size: [imageBitmap.width, imageBitmap.height], format: "rgba8unorm", }).$usage("sampled"); // Upload using raw device const device = root.unwrap().device; device.queue.copyExternalImageToTexture( { source: imageBitmap }, { texture: root.unwrap(texture) }, [imageBitmap.width, imageBitmap.height] ); ``` ## Complete Example [Section titled “Complete Example”](#complete-example) Texture loader utility ```javascript class TextureLoader { constructor(device) { this.device = device; } async fromURL(url, options = {}) { const { flipY = false, generateMipmaps = false } = options; const response = await fetch(url); const blob = await response.blob(); const imageBitmap = await createImageBitmap(blob, { colorSpaceConversion: "none", }); const { width, height } = imageBitmap; const mipLevelCount = generateMipmaps ? Math.floor(Math.log2(Math.max(width, height))) + 1 : 1; const texture = this.device.createTexture({ size: [width, height], format: "rgba8unorm", mipLevelCount, usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | (generateMipmaps ? GPUTextureUsage.RENDER_ATTACHMENT : 0), }); this.device.queue.copyExternalImageToTexture( { source: imageBitmap, flipY }, { texture }, [width, height] ); if (generateMipmaps) { this.generateMipmaps(texture, mipLevelCount); } return texture; } generateMipmaps(texture, levelCount) { // See textures-samplers.md for mipmap generation } } ``` ## Resources [Section titled “Resources”](#resources)

# Data Schemas in TypeGPU

## Overview [Section titled “Overview”](#overview) TypeGPU data schemas provide type-safe GPU data definitions with automatic serialization. Similar to how [Zod](https://zod.dev/) provides runtime type validation, TypeGPU schemas serve as both type definitions and runtime validators for GPU data structures. Schemas serve a dual purpose: 1. **Define TypeScript types** for compile-time checking 2. **Validate and serialize data** when moving between JavaScript and GPU memory ## Scalar Types [Section titled “Scalar Types”](#scalar-types) Scalar schemas represent single numeric or boolean values, mapping directly to WGSL primitives: | TypeGPU | WGSL | Description | | -------- | ------ | ----------------------- | | `d.f32` | `f32` | 32-bit float | | `d.f16` | `f16` | 16-bit float | | `d.u32` | `u32` | Unsigned 32-bit integer | | `d.i32` | `i32` | Signed 32-bit integer | | `d.bool` | `bool` | Boolean | ### Using Scalars as Constructors [Section titled “Using Scalars as Constructors”](#using-scalars-as-constructors) Creating scalar values ```typescript import * as d from "typegpu/data"; const radius = d.f32(3.14159); const particleCount = d.u32(1000); const temperature = d.i32(-40); const isActive = d.bool(true); ``` ### Scalars in Structs [Section titled “Scalars in Structs”](#scalars-in-structs) Scalar fields in structs ```typescript const Configuration = d.struct({ timeStep: d.f32, gravity: d.f32, damping: d.f32, maxParticles: d.u32, simulationEnabled: d.bool, }); ``` ## Vector Types [Section titled “Vector Types”](#vector-types) Vectors represent 2, 3, or 4 elements of the same scalar type—positions, velocities, colors. ### Constructor Patterns [Section titled “Constructor Patterns”](#constructor-patterns) Vector construction ```typescript // Zero initialization const origin = d.vec3f(); // (0.0, 0.0, 0.0) // Uniform initialization const ones = d.vec3f(1); // (1.0, 1.0, 1.0) // Component-wise const position = d.vec3f(1, 2, 3.5); const color = d.vec4f(1, 0, 0, 1); // RGBA red ``` ### Element Access [Section titled “Element Access”](#element-access) Accessing vector components ```typescript const velocity = d.vec3f(1.5, 2.0, -0.5); const x = velocity[0]; // 1.5 const y = velocity[1]; // 2.0 const z = velocity[2]; // -0.5 velocity[0] = 3.0; // Modify x ``` ### Practical Examples [Section titled “Practical Examples”](#practical-examples) Common vector use cases ```typescript const Particle = d.struct({ position: d.vec3f, velocity: d.vec3f, lifetime: d.f32, }); const ColorRGBA = d.struct({ color: d.vec4f, }); const GridCell = d.struct({ indices: d.vec3u, value: d.f32, }); ``` ## Matrix Types [Section titled “Matrix Types”](#matrix-types) Matrices handle transformations. TypeGPU supports square matrices of `f32`: | TypeGPU | WGSL | Size | | ----------- | ------------- | ---- | | `d.mat2x2f` | `mat2x2<f32>` | 2×2 | | `d.mat3x3f` | `mat3x3<f32>` | 3×3 | | `d.mat4x4f` | `mat4x4<f32>` | 4×4 | Column-Major Order Like WGSL and most graphics APIs, TypeGPU matrices use column-major order: ```typescript // Matrix stored as [column0, column1, column2] // [m00, m10, m20, m01, m11, m21, m02, m12, m22] ``` ### Integration with wgpu-matrix [Section titled “Integration with wgpu-matrix”](#integration-with-wgpu-matrix) TypeGPU works seamlessly with [wgpu-matrix](https://github.com/greggman/wgpu-matrix) via the `[]` operator: Using wgpu-matrix with TypeGPU ```typescript import { mat4, vec3 } from "wgpu-matrix"; import * as d from "typegpu/data"; const transform = d.mat4x4f(); mat4.identity(transform); const modelMatrix = d.mat4x4f(); mat4.translation([1, 2, 3], modelMatrix); mat4.rotateY(modelMatrix, Math.PI / 4, modelMatrix); const direction = d.vec3f(1, 2, 3); vec3.normalize(direction, direction); ``` ## Struct Schemas [Section titled “Struct Schemas”](#struct-schemas) Structs compose complex types from primitives and other structs, automatically handling alignment and padding. ### Basic Definition [Section titled “Basic Definition”](#basic-definition) Struct definitions ```typescript import * as d from "typegpu/data"; const Particle = d.struct({ position: d.vec3f, velocity: d.vec3f, lifetime: d.f32, }); const SimulationConfig = d.struct({ deltaTime: d.f32, gravity: d.f32, damping: d.f32, particleCount: d.u32, }); ``` ### Nested Structs [Section titled “Nested Structs”](#nested-structs) Hierarchical data structures ```typescript const Material = d.struct({ albedo: d.vec3f, metallic: d.f32, roughness: d.f32, ao: d.f32, }); const MeshInstance = d.struct({ transform: d.mat4x4f, material: Material, isVisible: d.bool, }); const SceneData = d.struct({ camera: d.struct({ viewMatrix: d.mat4x4f, projectionMatrix: d.mat4x4f, position: d.vec3f, }), ambientLight: d.vec3f, time: d.f32, }); ``` ### Automatic Alignment [Section titled “Automatic Alignment”](#automatic-alignment) vec3 Padding `vec3<f32>` requires 16-byte alignment despite being 12 bytes. TypeGPU handles this automatically: ```typescript const ExampleStruct = d.struct({ a: d.f32, // 4 bytes at offset 0 // 12 bytes padding inserted automatically b: d.vec3f, // 12 bytes at offset 16 (aligned to 16) c: d.f32, // 4 bytes at offset 28 // Padding to align struct size to 16 }); // Total: 48 bytes (not 20) ``` ### Custom Alignment [Section titled “Custom Alignment”](#custom-alignment) Override defaults when needed: Manual alignment control ```typescript const CustomAlignedStruct = d.struct({ a: d.align(16, d.f32), // Force 16-byte alignment b: d.size(32, d.vec4f), // Explicit size }); ``` ## Array Schemas [Section titled “Array Schemas”](#array-schemas) Arrays are created with `d.arrayOf()`: ### Fixed-Size Arrays [Section titled “Fixed-Size Arrays”](#fixed-size-arrays) Fixed-size arrays ```typescript const FloatArray = d.arrayOf(d.f32, 10); const ParticleArray = d.arrayOf( d.struct({ position: d.vec3f, velocity: d.vec3f, }), 100 ); const Grid = d.struct({ cells: d.arrayOf(d.f32, 64), width: d.u32, height: d.u32, }); ``` ### Runtime-Sized Arrays [Section titled “Runtime-Sized Arrays”](#runtime-sized-arrays) Runtime-sized arrays omit the length parameter: Runtime-sized arrays ```typescript const DynamicParticles = d.arrayOf( d.struct({ position: d.vec3f, velocity: d.vec3f, }) // No size - determined at runtime ); ``` Position Requirement Runtime-sized arrays must be the last field in a struct: ```typescript // ✗ Error: runtime-sized array not at end const Wrong = d.struct({ particles: d.arrayOf(d.vec3f), particleCount: d.u32, }); // ✓ Correct: runtime-sized array at end const Correct = d.struct({ particleCount: d.u32, particles: d.arrayOf(d.vec3f), }); ``` ### Nested Arrays [Section titled “Nested Arrays”](#nested-arrays) Multi-dimensional data ```typescript const HeightMap = (width: number, height: number) => d.arrayOf(d.arrayOf(d.f32, height), width); const BoneTransforms = d.struct({ bones: d.arrayOf(d.mat4x4f, 64), }); ``` ## Type Inference [Section titled “Type Inference”](#type-inference) Extract TypeScript types from schemas: Type inference ```typescript import * as d from "typegpu/data"; const Particle = d.struct({ position: d.vec3f, velocity: d.vec3f, lifetime: d.f32, }); // Infer TypeScript type type ParticleType = d.Infer<typeof Particle>; // Use in application code const particles: ParticleType[] = [ { position: [0, 0, 0], velocity: [1, 0, 0], lifetime: 1.0 }, { position: [1, 1, 1], velocity: [0, 1, 0], lifetime: 0.5 }, ]; ``` ### Schema Factory Functions [Section titled “Schema Factory Functions”](#schema-factory-functions) Dynamic schema generation ```typescript const HeightMap = (width: number, height: number) => d.arrayOf(d.arrayOf(d.f32, height), width); type HeightMapType = ReturnType<typeof HeightMap>; ``` ## Memory Layout [Section titled “Memory Layout”](#memory-layout) ### Alignment Rules [Section titled “Alignment Rules”](#alignment-rules) | Type | Size | Alignment | | ------------------- | -------- | ------------ | | `f32`, `i32`, `u32` | 4 bytes | 4 bytes | | `vec2<f32>` | 8 bytes | 8 bytes | | `vec3<f32>` | 12 bytes | **16 bytes** | | `vec4<f32>` | 16 bytes | 16 bytes | | `mat4x4<f32>` | 64 bytes | 16 bytes | TypeGPU automatically inserts padding to satisfy these requirements. ### Loose Schemas for Vertex Data [Section titled “Loose Schemas for Vertex Data”](#loose-schemas-for-vertex-data) When strict alignment isn’t required (vertex buffers), use loose schemas to save memory: Loose vs aligned schemas ```typescript // Standard struct with alignment (32 bytes) const ParticleGeometry = d.struct({ position: d.vec3f, // 16 bytes (aligned) color: d.vec4f, // 16 bytes }); // Loose struct without padding (28 bytes) const LooseParticleGeometry = d.unstruct({ position: d.vec3f, // 12 bytes (no padding) color: d.vec4f, // 16 bytes }); // Use d.disarrayOf for loose arrays const LooseVertexBuffer = d.disarrayOf(LooseParticleGeometry); ``` ## Schema Design Guidelines [Section titled “Schema Design Guidelines”](#schema-design-guidelines) ## Resources [Section titled “Resources”](#resources)

# Canvas Configuration and Context

## Overview [Section titled “Overview”](#overview) The HTML canvas element serves as the rendering surface for WebGPU applications. Before rendering can occur, you must configure the canvas context to establish the connection between your WebGPU device and the display surface. ## Obtaining the Context [Section titled “Obtaining the Context”](#obtaining-the-context) Getting WebGPU context ```javascript const canvas = document.querySelector("canvas"); const context = canvas.getContext("webgpu"); if (!context) { console.error("WebGPU not supported"); return; } ``` The `GPUCanvasContext` interface provides: | Method | Description | | --------------------- | -------------------------------------------------------- | | `configure(config)` | Sets up context with GPU device and rendering parameters | | `getCurrentTexture()` | Returns the next `GPUTexture` for the current frame | | `unconfigure()` | Removes configuration and destroys textures | | `getConfiguration()` | Returns the current configuration | ## Preferred Canvas Format [Section titled “Preferred Canvas Format”](#preferred-canvas-format) Different platforms prefer different texture formats. Use `navigator.gpu.getPreferredCanvasFormat()` for optimal performance: Get preferred format ```javascript const preferredFormat = navigator.gpu.getPreferredCanvasFormat(); // Returns 'bgra8unorm' or 'rgba8unorm' ``` | Platform | Preferred Format | | ------------- | ------------------------------------ | | macOS/Metal | `bgra8unorm` (IOSurface requirement) | | Windows/D3D12 | `rgba8unorm` | | Linux/Vulkan | Varies by driver | Caution Using the non-preferred format causes unnecessary format conversions and memory copies. ## Context Configuration [Section titled “Context Configuration”](#context-configuration) Configure the context once during initialization: Complete context configuration ```javascript const adapter = await navigator.gpu.requestAdapter(); const device = await adapter.requestDevice(); const canvas = document.querySelector("canvas"); const context = canvas.getContext("webgpu"); const format = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device: device, format: format, usage: GPUTextureUsage.RENDER_ATTACHMENT, alphaMode: "opaque", colorSpace: "srgb", }); ``` ### Configuration Options [Section titled “Configuration Options”](#configuration-options) ## Canvas Sizing [Section titled “Canvas Sizing”](#canvas-sizing) The canvas has two sizes: 1. **Display size** — CSS dimensions (how large it appears) 2. **Resolution** — `width`/`height` attributes (actual pixel dimensions) For sharp rendering, match resolution to device pixels: Match device pixel ratio ```javascript const dpr = window.devicePixelRatio || 1; canvas.width = Math.min( canvas.clientWidth * dpr, device.limits.maxTextureDimension2D ); canvas.height = Math.min( canvas.clientHeight * dpr, device.limits.maxTextureDimension2D ); ``` ### High-DPI Support with ResizeObserver [Section titled “High-DPI Support with ResizeObserver”](#high-dpi-support-with-resizeobserver) Use `ResizeObserver` with `devicePixelContentBoxSize` for pixel-perfect rendering: ResizeObserver for high-DPI ```javascript const observer = new ResizeObserver((entries) => { for (const entry of entries) { let width, height; if (entry.devicePixelContentBoxSize) { // Most accurate - actual device pixels width = entry.devicePixelContentBoxSize[0].inlineSize; height = entry.devicePixelContentBoxSize[0].blockSize; } else { // Fallback - CSS pixels × devicePixelRatio const dpr = window.devicePixelRatio; width = entry.contentBoxSize[0].inlineSize * dpr; height = entry.contentBoxSize[0].blockSize * dpr; } entry.target.width = Math.max( 1, Math.min(Math.floor(width), device.limits.maxTextureDimension2D) ); entry.target.height = Math.max( 1, Math.min(Math.floor(height), device.limits.maxTextureDimension2D) ); } }); observer.observe(canvas); ``` Caution Only update canvas dimensions when they actually change—setting `canvas.width` or `canvas.height` triggers expensive operations. ## Getting the Current Texture [Section titled “Getting the Current Texture”](#getting-the-current-texture) Call `getCurrentTexture()` once per frame within your render loop: Basic render loop ```javascript function render() { const texture = context.getCurrentTexture(); const view = texture.createView(); const encoder = device.createCommandEncoder(); const renderPass = encoder.beginRenderPass({ colorAttachments: [{ view: view, clearValue: { r: 0.0, g: 0.0, b: 0.5, a: 1.0 }, loadOp: "clear", storeOp: "store", }], }); // Rendering commands... renderPass.end(); device.queue.submit([encoder.finish()]); requestAnimationFrame(render); } ``` Don’t Cache Textures The texture is valid only for the current frame and automatically presented after queue submission. Get a fresh texture each frame—do not cache it. ## Alpha Compositing [Section titled “Alpha Compositing”](#alpha-compositing) ### Premultiplied Alpha [Section titled “Premultiplied Alpha”](#premultiplied-alpha) In premultiplied alpha, RGB values are multiplied by alpha before storage: | Type | Red at 50% | | ------------- | -------------------------------- | | Standard | `(1.0, 0.0, 0.0, 0.5)` | | Premultiplied | `(0.5, 0.0, 0.0, 0.5)` (RGB × A) | Configure for transparency: Enable premultiplied alpha ```javascript context.configure({ device, format, alphaMode: "premultiplied", }); ``` Fragment shader must output premultiplied colors: Premultiply in shader ```wgsl @fragment fn fragmentMain() -> @location(0) vec4f { var color = vec4f(1.0, 0.0, 0.0, 0.5); // Premultiply color = vec4f(color.rgb * color.a, color.a); return color; // (0.5, 0.0, 0.0, 0.5) } ``` Or use blend state: Blend state for premultiplied ```javascript blend: { color: { srcFactor: "one", dstFactor: "one-minus-src-alpha", operation: "add", }, alpha: { srcFactor: "one", dstFactor: "one-minus-src-alpha", operation: "add", }, } ``` ## Multiple Canvases [Section titled “Multiple Canvases”](#multiple-canvases) Render to multiple canvases with a single device: Multi-canvas rendering ```javascript const context1 = canvas1.getContext("webgpu"); const context2 = canvas2.getContext("webgpu"); const format = navigator.gpu.getPreferredCanvasFormat(); context1.configure({ device, format }); context2.configure({ device, format }); function render() { const encoder = device.createCommandEncoder(); const pass1 = encoder.beginRenderPass({ colorAttachments: [{ view: context1.getCurrentTexture().createView(), clearValue: { r: 1, g: 0, b: 0, a: 1 }, loadOp: "clear", storeOp: "store", }], }); pass1.end(); const pass2 = encoder.beginRenderPass({ colorAttachments: [{ view: context2.getCurrentTexture().createView(), clearValue: { r: 0, g: 1, b: 0, a: 1 }, loadOp: "clear", storeOp: "store", }], }); pass2.end(); device.queue.submit([encoder.finish()]); requestAnimationFrame(render); } ``` ## Storage Texture Usage [Section titled “Storage Texture Usage”](#storage-texture-usage) To write to canvas from compute shaders, check for `bgra8unorm-storage`: Enable storage texture usage ```javascript const adapter = await navigator.gpu.requestAdapter(); const hasStorage = adapter.features.has("bgra8unorm-storage"); const device = await adapter.requestDevice({ requiredFeatures: hasStorage ? ["bgra8unorm-storage"] : [], }); if (hasStorage || format === "rgba8unorm") { context.configure({ device, format, usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.STORAGE_BINDING, }); } ``` ## Complete Example [Section titled “Complete Example”](#complete-example) Full canvas setup ```javascript async function initWebGPU() { if (!navigator.gpu) { console.error("WebGPU not supported"); return null; } const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { console.error("No WebGPU adapter found"); return null; } const device = await adapter.requestDevice(); const canvas = document.querySelector("canvas"); const context = canvas.getContext("webgpu"); const format = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device, format, alphaMode: "opaque", }); const observer = new ResizeObserver((entries) => { for (const entry of entries) { let width, height; if (entry.devicePixelContentBoxSize) { width = entry.devicePixelContentBoxSize[0].inlineSize; height = entry.devicePixelContentBoxSize[0].blockSize; } else { const dpr = window.devicePixelRatio; width = entry.contentBoxSize[0].inlineSize * dpr; height = entry.contentBoxSize[0].blockSize * dpr; } canvas.width = Math.max(1, Math.min(Math.floor(width), device.limits.maxTextureDimension2D)); canvas.height = Math.max(1, Math.min(Math.floor(height), device.limits.maxTextureDimension2D)); } }); observer.observe(canvas); return { device, context, format }; } function render(device, context) { const encoder = device.createCommandEncoder(); const view = context.getCurrentTexture().createView(); const pass = encoder.beginRenderPass({ colorAttachments: [{ view, clearValue: { r: 0.0, g: 0.0, b: 0.4, a: 1.0 }, loadOp: "clear", storeOp: "store", }], }); pass.end(); device.queue.submit([encoder.finish()]); } async function main() { const webgpu = await initWebGPU(); if (!webgpu) return; function frame() { render(webgpu.device, webgpu.context); requestAnimationFrame(frame); } requestAnimationFrame(frame); } main(); ```

# Command Encoders and Submission

## Overview [Section titled “Overview”](#overview) WebGPU employs a command buffer architecture where GPU operations are recorded and then executed. Unlike immediate-mode graphics APIs, WebGPU follows a deferred execution model: you create a command encoder, record commands into it, finish to produce a command buffer, and submit to the device queue. ## Command Buffer Lifecycle [Section titled “Command Buffer Lifecycle”](#command-buffer-lifecycle) | Stage | Description | | -------------- | ----------------------------------------------------------- | | **Recording** | Create a command encoder and call methods to add operations | | **Finishing** | Call `encoder.finish()` to produce a GPUCommandBuffer | | **Submission** | Pass command buffers to `queue.submit()` | | **Execution** | GPU processes commands; buffers are single-use | ## Creating Command Encoders [Section titled “Creating Command Encoders”](#creating-command-encoders) Create command encoder ```javascript const encoder = device.createCommandEncoder({ label: "main-render-encoder", }); ``` ## Render Pass Commands [Section titled “Render Pass Commands”](#render-pass-commands) Render passes are bounded regions that specify which textures receive output and how their contents are handled. ### Beginning a Render Pass [Section titled “Beginning a Render Pass”](#beginning-a-render-pass) Render pass with depth ```javascript const renderPassDescriptor = { label: "main-render-pass", colorAttachments: [{ view: context.getCurrentTexture().createView(), clearValue: { r: 0.0, g: 0.0, b: 0.2, a: 1.0 }, loadOp: "clear", storeOp: "store", }], depthStencilAttachment: { view: depthTexture.createView(), depthClearValue: 1.0, depthLoadOp: "clear", depthStoreOp: "store", }, }; const passEncoder = encoder.beginRenderPass(renderPassDescriptor); ``` ### Color Attachments [Section titled “Color Attachments”](#color-attachments) | Property | Description | | --------------- | ----------------------------------------- | | `view` | GPUTextureView to render into | | `resolveTarget` | Optional, for multisample resolve | | `clearValue` | Color to clear if loadOp is ‘clear’ | | `loadOp` | `'clear'` or `'load'` (preserve existing) | | `storeOp` | `'store'` or `'discard'` | Multiple color attachments (up to 8) enable deferred rendering: Multiple render targets ```javascript colorAttachments: [ { view: albedoView, loadOp: "clear", storeOp: "store" }, { view: normalView, loadOp: "clear", storeOp: "store" }, { view: depthView, loadOp: "clear", storeOp: "store" }, ] ``` ### Depth-Stencil Attachment [Section titled “Depth-Stencil Attachment”](#depth-stencil-attachment) Only one depth-stencil attachment per pass: | Property | Description | | ------------------------------- | ----------------------- | | `depthLoadOp/depthStoreOp` | Depth buffer handling | | `stencilLoadOp/stencilStoreOp` | Stencil buffer handling | | `depthClearValue` | Typically 1.0 or 0.0 | | `stencilClearValue` | Typically 0 | | `depthReadOnly/stencilReadOnly` | Mark as read-only | ### Drawing Commands [Section titled “Drawing Commands”](#drawing-commands) Drawing commands ```javascript passEncoder.setPipeline(renderPipeline); passEncoder.setBindGroup(0, bindGroup); passEncoder.setVertexBuffer(0, vertexBuffer); passEncoder.setIndexBuffer(indexBuffer, "uint16"); // Non-indexed drawing passEncoder.draw(vertexCount, instanceCount, firstVertex, firstInstance); // Indexed drawing passEncoder.drawIndexed(indexCount, instanceCount, firstIndex, baseVertex, firstInstance); // Indirect drawing (parameters from GPU buffer) passEncoder.drawIndirect(indirectBuffer, indirectOffset); passEncoder.drawIndexedIndirect(indirectBuffer, indirectOffset); ``` Caution End the pass before encoding more commands. After `end()`, the pass encoder is invalid. ```javascript passEncoder.end(); ``` ## Compute Pass Commands [Section titled “Compute Pass Commands”](#compute-pass-commands) Compute passes execute general-purpose GPU computation. Compute pass with timestamp ```javascript const computePassDescriptor = { label: "particle-update-pass", timestampWrites: { querySet: timestampQuerySet, beginningOfPassWriteIndex: 0, endOfPassWriteIndex: 1, }, }; const passEncoder = encoder.beginComputePass(computePassDescriptor); passEncoder.setPipeline(computePipeline); passEncoder.setBindGroup(0, computeBindGroup); // Dispatch workgroups passEncoder.dispatchWorkgroups(workgroupCountX, workgroupCountY, workgroupCountZ); // Or indirect dispatch passEncoder.dispatchWorkgroupsIndirect(indirectBuffer, indirectOffset); passEncoder.end(); ``` ## Copy Commands [Section titled “Copy Commands”](#copy-commands) Copy operations transfer data between resources without shaders, often using specialized hardware (DMA engines). ### Buffer to Buffer [Section titled “Buffer to Buffer”](#buffer-to-buffer) Buffer copy ```javascript encoder.copyBufferToBuffer( sourceBuffer, sourceOffset, // bytes, multiple of 4 destinationBuffer, destinationOffset, // bytes, multiple of 4 size // bytes, multiple of 4 ); ``` ### Texture to Texture [Section titled “Texture to Texture”](#texture-to-texture) Texture copy ```javascript encoder.copyTextureToTexture( { texture: sourceTexture, mipLevel: 0, origin: { x: 0, y: 0, z: 0 } }, { texture: destinationTexture, mipLevel: 0, origin: { x: 0, y: 0, z: 0 } }, { width: 512, height: 512, depthOrArrayLayers: 1 } ); ``` ### Buffer to/from Texture [Section titled “Buffer to/from Texture”](#buffer-tofrom-texture) Texture upload and readback ```javascript // Upload texture data encoder.copyBufferToTexture( { buffer: stagingBuffer, offset: 0, bytesPerRow: 256 * 4, rowsPerImage: 256 }, { texture: targetTexture, mipLevel: 0, origin: { x: 0, y: 0, z: 0 } }, { width: 256, height: 256, depthOrArrayLayers: 1 } ); // Readback texture data encoder.copyTextureToBuffer( { texture: sourceTexture, mipLevel: 0, origin: { x: 0, y: 0, z: 0 } }, { buffer: readbackBuffer, offset: 0, bytesPerRow: 256 * 4, rowsPerImage: 256 }, { width: 256, height: 256, depthOrArrayLayers: 1 } ); ``` ## Finishing and Submission [Section titled “Finishing and Submission”](#finishing-and-submission) Finish and submit ```javascript const commandBuffer = encoder.finish({ label: "main-command-buffer", }); device.queue.submit([commandBuffer]); ``` Single-Use Buffers The encoder becomes invalid after `finish()`. Command buffers are single-use—submit once, then discard. ### Multiple Command Buffers [Section titled “Multiple Command Buffers”](#multiple-command-buffers) ### Direct Queue Writes [Section titled “Direct Queue Writes”](#direct-queue-writes) For convenience, write data directly without command encoders: Direct queue writes ```javascript device.queue.writeBuffer(targetBuffer, bufferOffset, arrayBufferData, dataOffset, size); device.queue.writeTexture( { texture: targetTexture, mipLevel: 0 }, imageData, { bytesPerRow: 256 * 4, rowsPerImage: 256 }, { width: 256, height: 256, depthOrArrayLayers: 1 } ); ``` ### Waiting for Completion [Section titled “Waiting for Completion”](#waiting-for-completion) Wait for GPU ```javascript await device.queue.onSubmittedWorkDone(); console.log("All GPU work completed"); ``` ## Synchronization [Section titled “Synchronization”](#synchronization) WebGPU handles synchronization through implicit barriers between passes and command buffers. ### Resource State Tracking [Section titled “Resource State Tracking”](#resource-state-tracking) | Rule | Description | | ------------------------ | ----------------------------------------------------------------------- | | Exclusive write | Resource used for writing cannot be read/written elsewhere in same pass | | Multiple read | Resources can be read from multiple bind groups simultaneously | | Subresource independence | Different mip levels or array layers can be used independently | ### Write-After-Read Safety [Section titled “Write-After-Read Safety”](#write-after-read-safety) Within a command buffer, writes followed by reads are automatically synchronized: Automatic synchronization ```javascript encoder.copyBufferToBuffer(srcBuffer, 0, dstBuffer, 0, 256); const computePass = encoder.beginComputePass(); computePass.setBindGroup(0, bindGroupUsingDstBuffer); computePass.dispatchWorkgroups(1); computePass.end(); ``` Between passes, all writes complete before subsequent reads: Pass-to-pass synchronization ```javascript const renderPass = encoder.beginRenderPass(/* writes to texture */); renderPass.end(); const computePass = encoder.beginComputePass(); // Can safely read the texture written above computePass.end(); ``` ## Complete Render Loop [Section titled “Complete Render Loop”](#complete-render-loop) Full render loop example ```javascript async function init() { const adapter = await navigator.gpu.requestAdapter(); const device = await adapter.requestDevice(); const canvas = document.querySelector("canvas"); const context = canvas.getContext("webgpu"); const format = navigator.gpu.getPreferredCanvasFormat(); context.configure({ device, format, alphaMode: "opaque" }); const pipeline = device.createRenderPipeline({ label: "render-pipeline", layout: "auto", vertex: { module: device.createShaderModule({ code: ` @vertex fn vertexMain(@builtin(vertex_index) i: u32) -> @builtin(position) vec4f { const pos = array( vec2f(-0.5, -0.5), vec2f( 0.5, -0.5), vec2f( 0.0, 0.5) ); return vec4f(pos[i], 0.0, 1.0); } `, }), entryPoint: "vertexMain", }, fragment: { module: device.createShaderModule({ code: ` @fragment fn fragmentMain() -> @location(0) vec4f { return vec4f(1.0, 0.5, 0.2, 1.0); } `, }), entryPoint: "fragmentMain", targets: [{ format }], }, primitive: { topology: "triangle-list" }, }); function frame() { const encoder = device.createCommandEncoder({ label: "frame-encoder" }); const pass = encoder.beginRenderPass({ label: "main-render-pass", colorAttachments: [{ view: context.getCurrentTexture().createView(), clearValue: { r: 0.1, g: 0.1, b: 0.15, a: 1.0 }, loadOp: "clear", storeOp: "store", }], }); pass.setPipeline(pipeline); pass.draw(3); pass.end(); device.queue.submit([encoder.finish()]); requestAnimationFrame(frame); } frame(); } init(); ```

# Error Handling and Validation

## Overview [Section titled “Overview”](#overview) WebGPU uses asynchronous error reporting and a stack-based error scope system, unlike WebGL’s synchronous `getError()` approach. All errors are detected asynchronously in a GPU process, avoiding synchronous validation overhead. ## Error Types [Section titled “Error Types”](#error-types) ### GPUValidationError [Section titled “GPUValidationError”](#gpuvalidationerror) Programming mistakes that violate API constraints: * Invalid resource descriptors * Binding mismatches * Usage flag violations * Shader compilation failures * Using destroyed resources Validation error example ```javascript // Validation error: missing COPY_DST flag for writeBuffer const buffer = device.createBuffer({ size: 256, usage: GPUBufferUsage.UNIFORM, }); device.queue.writeBuffer(buffer, 0, data); // Fails ``` ### GPUOutOfMemoryError [Section titled “GPUOutOfMemoryError”](#gpuoutofmemoryerror) Resource exhaustion despite valid API usage: Potential OOM scenario ```javascript // May fail on devices with limited VRAM const hugeTexture = device.createTexture({ size: { width: 16384, height: 16384 }, format: "rgba32float", usage: GPUTextureUsage.STORAGE_BINDING, }); ``` ### GPUInternalError [Section titled “GPUInternalError”](#gpuinternalerror) Implementation failures despite valid usage—driver crashes, hardware failures, or browser bugs. These are rare and cannot be prevented, but should be handled gracefully. ### Device Lost [Section titled “Device Lost”](#device-lost) Catastrophic Failure Device lost means the GPU becomes unavailable: * Hardware disconnection * Driver crash (TDR) * Application calls `device.destroy()` * System resource exhaustion Device lost requires complete recovery: recreate device and all resources. ## Error Scopes [Section titled “Error Scopes”](#error-scopes) Error scopes capture errors hermetically, preventing leakage between unrelated code. ### Basic Usage [Section titled “Basic Usage”](#basic-usage) Basic error scope ```javascript device.pushErrorScope("validation"); const buffer = device.createBuffer(descriptor); const error = await device.popErrorScope(); if (error) { console.error("Buffer creation failed:", error.message); } ``` Filter options: `'validation'`, `'out-of-memory'`, `'internal'` ### Multiple Error Types [Section titled “Multiple Error Types”](#multiple-error-types) Handle multiple error types ```javascript async function createBufferWithErrorHandling(device, descriptor) { device.pushErrorScope("validation"); device.pushErrorScope("out-of-memory"); const buffer = device.createBuffer(descriptor); // Pop in reverse order (LIFO) const outOfMemoryError = await device.popErrorScope(); const validationError = await device.popErrorScope(); if (validationError) { console.error("Validation:", validationError.message); return null; } if (outOfMemoryError) { console.error("Out of memory:", outOfMemoryError.message); return null; } return buffer; } ``` ### Nested Scopes [Section titled “Nested Scopes”](#nested-scopes) Inner scopes capture errors first; uncaptured errors propagate to outer scopes: Nested error scopes ```javascript async function createPipeline(device, shaderCode, pipelineDescriptor) { device.pushErrorScope("validation"); // Outer: pipeline device.pushErrorScope("validation"); // Inner: shader const shaderModule = device.createShaderModule({ code: shaderCode }); const shaderError = await device.popErrorScope(); if (shaderError) { await device.popErrorScope(); // Clean up outer scope throw new Error(`Shader compilation failed: ${shaderError.message}`); } const pipeline = device.createRenderPipeline({ ...pipelineDescriptor, vertex: { module: shaderModule, entryPoint: "vertex_main" }, fragment: { module: shaderModule, entryPoint: "fragment_main", targets: [{ format: "bgra8unorm" }] }, }); const pipelineError = await device.popErrorScope(); if (pipelineError) { throw new Error(`Pipeline creation failed: ${pipelineError.message}`); } return pipeline; } ``` ## Uncaptured Errors [Section titled “Uncaptured Errors”](#uncaptured-errors) Errors not captured by any scope trigger the `uncapturederror` event: Global error handler ```javascript device.addEventListener("uncapturederror", (event) => { console.error("Uncaptured GPU error:", event.error.message); if (event.error instanceof GPUValidationError) { console.error("Check API usage"); } else if (event.error instanceof GPUOutOfMemoryError) { console.error("Reduce resource usage"); } else if (event.error instanceof GPUInternalError) { console.error("Possible driver or hardware issue"); } }); ``` Errors become uncaptured when: * No error scope is active * Active scopes don’t match the error type * Scopes are popped before the error occurs ## Device Lost Handling [Section titled “Device Lost Handling”](#device-lost-handling) Set Up Immediately Monitor `device.lost` immediately after device creation: Device lost handler ```javascript const device = await adapter.requestDevice(); device.lost.then((info) => { console.error(`Device lost: ${info.reason} - ${info.message}`); if (info.reason !== "destroyed") { attemptRecovery(); } }); ``` The `reason` is either `'destroyed'` (explicit) or `'unknown'` (system failure). ### Recovery Strategies [Section titled “Recovery Strategies”](#recovery-strategies) **1. Minimal Recovery** — Display a message and suggest page refresh: Simple recovery ```javascript device.lost.then((info) => { if (info.reason !== "destroyed") { document.getElementById("gpu-canvas").innerHTML = '<p>GPU disconnected. <a href="">Reload page</a></p>'; } }); ``` **2. Automatic Restart** — Reinitialize WebGPU without full page reload: Automatic restart ```javascript async function initWebGPU() { // Always request fresh adapter—cached ones may be invalid const adapter = await navigator.gpu.requestAdapter(); if (!adapter) throw new Error("No adapter available"); const device = await adapter.requestDevice(); device.lost.then((info) => { console.warn("Device lost:", info.reason, info.message); if (info.reason !== "destroyed") { initWebGPU(); // Restart WebGPU } }); // Continue with initialization... } ``` **3. State Preservation** — Save and restore user state: State-preserving recovery ```javascript class WebGPUApp { constructor() { this.adapter = null; this.device = null; this.state = {}; // Serializable application state } async initialize() { this.adapter = await navigator.gpu.requestAdapter(); if (!this.adapter) throw new Error("No adapter"); await this.initializeDevice(); } async initializeDevice() { this.device = await this.adapter.requestDevice(); this.device.lost.then((info) => { console.warn("Device lost:", info); if (info.reason !== "destroyed") { this.handleDeviceLost(); } }); this.device.addEventListener("uncapturederror", (event) => { console.error("Uncaptured:", event.error); }); } async handleDeviceLost() { // Save state before cleanup this.saveState(); try { // Request fresh adapter (don't reuse cached) this.adapter = await navigator.gpu.requestAdapter(); await this.initializeDevice(); await this.recreateResources(); this.restoreState(); this.resume(); } catch (error) { console.error("Recovery failed:", error); this.showFallbackUI(); } } saveState() { // Sync GPU data to JavaScript before loss localStorage.setItem("app-state", JSON.stringify(this.state)); } restoreState() { const saved = localStorage.getItem("app-state"); if (saved) this.state = JSON.parse(saved); } async recreateResources() { // Re-create GPU resources from stored descriptors } resume() { // Resume rendering } } ``` ### Testing Device Loss [Section titled “Testing Device Loss”](#testing-device-loss) Simulate device loss ```javascript let simulatedLoss = false; device.lost.then((info) => { if (info.reason === "unknown" || simulatedLoss) { simulatedLoss = false; handleDeviceLoss(); } }); // Trigger simulated loss function testDeviceLoss() { simulatedLoss = true; device.destroy(); } ``` ## Memory Pressure Handling [Section titled “Memory Pressure Handling”](#memory-pressure-handling) GPU memory exhaustion can cause allocation failures. Handle proactively: ### Explicit Resource Destruction [Section titled “Explicit Resource Destruction”](#explicit-resource-destruction) Destroy resources explicitly ```javascript // Don't wait for garbage collection function cleanupTemporaryResources(textures, buffers) { for (const texture of textures) { texture.destroy(); } for (const buffer of buffers) { buffer.destroy(); } } // Call after each frame for temporary resources cleanupTemporaryResources(frameTextures, frameBuffers); ``` GC Doesn’t Know GPU Memory JavaScript’s garbage collector runs in the renderer process and doesn’t see GPU memory usage. A single `GPUTexture` can hold gigabytes without triggering GC. Always call `.destroy()` explicitly. ### Fallback Allocation [Section titled “Fallback Allocation”](#fallback-allocation) Graceful texture downscaling ```javascript async function createTextureWithFallback(device, descriptor) { device.pushErrorScope("out-of-memory"); const texture = device.createTexture(descriptor); const error = await device.popErrorScope(); if (error) { console.warn("Texture too large, trying half resolution"); // Retry with smaller size const fallbackDescriptor = { ...descriptor, size: { width: Math.floor(descriptor.size.width / 2), height: Math.floor(descriptor.size.height / 2), depthOrArrayLayers: descriptor.size.depthOrArrayLayers || 1, }, }; return createTextureWithFallback(device, fallbackDescriptor); } return texture; } ``` ### Memory Budgeting [Section titled “Memory Budgeting”](#memory-budgeting) Track GPU memory usage ```javascript class GPUMemoryTracker { constructor() { this.allocated = 0; this.budget = 512 * 1024 * 1024; // 512 MB budget } canAllocate(bytes) { return this.allocated + bytes <= this.budget; } track(resource, bytes) { this.allocated += bytes; return { resource, release: () => { resource.destroy(); this.allocated -= bytes; }, }; } } ``` ## Graceful Degradation [Section titled “Graceful Degradation”](#graceful-degradation) ### WebGPU → WebGL Fallback [Section titled “WebGPU → WebGL Fallback”](#webgpu--webgl-fallback) Progressive fallback ```javascript async function initGraphics() { // Try WebGPU if (navigator.gpu) { try { const adapter = await navigator.gpu.requestAdapter(); if (adapter) { const device = await adapter.requestDevice(); return { api: "webgpu", device }; } } catch (e) { console.warn("WebGPU failed:", e); } } // Fall back to WebGL2 const canvas = document.getElementById("canvas"); const gl = canvas.getContext("webgl2"); if (gl) { return { api: "webgl2", gl }; } // Final fallback return { api: "none", error: "No GPU API available" }; } ``` ### Feature-Based Degradation [Section titled “Feature-Based Degradation”](#feature-based-degradation) Degrade based on capabilities ```javascript async function selectQualityLevel(adapter) { const limits = adapter.limits; const features = adapter.features; // High quality: all features available if ( limits.maxTextureDimension2D >= 8192 && features.has("texture-compression-bc") && features.has("timestamp-query") ) { return "high"; } // Medium quality: basic features if (limits.maxTextureDimension2D >= 4096) { return "medium"; } // Low quality: minimum spec return "low"; } const qualityPresets = { high: { shadowMapSize: 4096, particleCount: 100000 }, medium: { shadowMapSize: 2048, particleCount: 10000 }, low: { shadowMapSize: 1024, particleCount: 1000 }, }; ``` ### Runtime Quality Adjustment [Section titled “Runtime Quality Adjustment”](#runtime-quality-adjustment) Adjust quality based on performance ```javascript class AdaptiveQuality { constructor() { this.frameTimes = []; this.targetFPS = 60; this.currentQuality = 1.0; } recordFrame(durationMs) { this.frameTimes.push(durationMs); if (this.frameTimes.length > 60) this.frameTimes.shift(); } shouldDegrade() { if (this.frameTimes.length < 30) return false; const avgMs = this.frameTimes.reduce((a, b) => a + b) / this.frameTimes.length; return avgMs > 1000 / this.targetFPS * 1.2; // 20% over budget } shouldUpgrade() { if (this.frameTimes.length < 60) return false; const avgMs = this.frameTimes.reduce((a, b) => a + b) / this.frameTimes.length; return avgMs < 1000 / this.targetFPS * 0.7; // 30% under budget } adjust() { if (this.shouldDegrade() && this.currentQuality > 0.25) { this.currentQuality *= 0.9; return "degraded"; } if (this.shouldUpgrade() && this.currentQuality < 1.0) { this.currentQuality *= 1.1; return "upgraded"; } return "stable"; } } ``` ## Debugging with Labels [Section titled “Debugging with Labels”](#debugging-with-labels) ## Common Validation Errors [Section titled “Common Validation Errors”](#common-validation-errors) ## TypeGPU Error Prevention [Section titled “TypeGPU Error Prevention”](#typegpu-error-prevention) TypeGPU catches many errors at compile time through TypeScript’s type system: TypeGPU compile-time validation ```typescript import * as d from "typegpu/data"; const Particle = d.struct({ position: d.vec3f, velocity: d.vec3f, mass: d.f32, }); // TypeScript error: missing z component const invalidData = { position: [1, 2], // Error! velocity: [0, 0, 0], mass: 1.0, }; // Correct const validData: (typeof Particle)["~repr"] = { position: [1, 2, 3], velocity: [0, 0, 0], mass: 1.0, }; ``` ## Complete Error Handling Example [Section titled “Complete Error Handling Example”](#complete-error-handling-example) Production error handling ```javascript class GPUResourceManager { constructor(device) { this.device = device; this.setupErrorHandling(); } setupErrorHandling() { this.device.addEventListener("uncapturederror", (event) => { console.error("Uncaptured:", event.error.message); }); this.device.lost.then((info) => { console.error("Device lost:", info.reason, info.message); }); } async createBuffer(descriptor) { this.device.pushErrorScope("validation"); this.device.pushErrorScope("out-of-memory"); const buffer = this.device.createBuffer(descriptor); const memError = await this.device.popErrorScope(); const valError = await this.device.popErrorScope(); if (valError) throw new Error(`Invalid descriptor: ${valError.message}`); if (memError) throw new Error(`Out of memory: ${memError.message}`); return buffer; } async createTexture(descriptor) { this.device.pushErrorScope("validation"); this.device.pushErrorScope("out-of-memory"); const texture = this.device.createTexture(descriptor); const memError = await this.device.popErrorScope(); const valError = await this.device.popErrorScope(); if (valError) { console.error("Texture validation:", valError.message); return null; } if (memError) { console.warn("Texture too large, trying half resolution"); return this.createTexture({ ...descriptor, size: { width: Math.floor(descriptor.size.width / 2), height: Math.floor(descriptor.size.height / 2), }, }); } return texture; } async debugShader(shaderCode) { this.device.pushErrorScope("validation"); const module = this.device.createShaderModule({ label: "Debug Shader", code: shaderCode, }); const error = await this.device.popErrorScope(); if (error) { console.error("Shader compilation failed:"); console.error(error.message); const lineMatch = error.message.match(/line (\d+)/); if (lineMatch) { const lineNum = parseInt(lineMatch[1]); const lines = shaderCode.split("\n"); console.error(`Error at line ${lineNum}:`, lines[lineNum - 1]); } return null; } return module; } } ```

# TypeGPU Getting Started

## Overview [Section titled “Overview”](#overview) TypeGPU is a modular toolkit for WebGPU that brings type safety and developer-friendly abstractions to GPU programming. Developed by Software Mansion, it provides advanced type inference and enables writing GPU shaders directly in TypeScript through TGSL (TypeGPU Shading Language). **Primary use cases:** * **Foundation for new projects** — Handles data serialization, buffer management, and shader composition * **Integration with existing code** — Type-safe APIs can be adopted independently * **Library interoperability** — Enables typed data sharing between WebGPU libraries ## Core Features [Section titled “Core Features”](#core-features) ### Type-Safe Data Schemas [Section titled “Type-Safe Data Schemas”](#type-safe-data-schemas) TypeGPU uses composable data schemas to manage data transfer between CPU and GPU. Every WGSL data type is represented as JavaScript schemas imported from `typegpu/data`: Defining a typed schema ```typescript import { struct, f32, vec3f, arrayOf } from "typegpu/data"; const Particle = struct({ position: vec3f, velocity: vec3f, mass: f32, }); // TypeScript type is automatically inferred type ParticleData = typeof Particle.infer; ``` Schemas provide automatic serialization/deserialization, compile-time validation, and self-documenting code. ### TGSL: TypeScript Shaders [Section titled “TGSL: TypeScript Shaders”](#tgsl-typescript-shaders) TGSL allows writing GPU shader code in TypeScript. Functions marked with `'use gpu'` are transpiled to WGSL at build time: Writing a TGSL shader ```typescript const squareNumbers = tgpu .fn([inputBuffer, outputBuffer]) .does(() => { "use gpu"; const idx = builtin.globalInvocationId.x; outputBuffer[idx] = inputBuffer[idx] * inputBuffer[idx]; }) .$name("squareNumbers"); ``` ### Bindless Resources [Section titled “Bindless Resources”](#bindless-resources) TypeGPU uses descriptive string keys instead of numeric binding indices: Named resource bindings ```typescript const resources = { particles: root.createBuffer(particleSchema).$usage("storage"), forces: root.createBuffer(forceSchema).$usage("storage"), }; ``` This improves code readability and reduces binding errors. ## Installation [Section titled “Installation”](#installation) Install TypeGPU ```bash npm install typegpu ``` Install bundler plugin for TGSL ```bash npm install --save-dev unplugin-typegpu ``` Add WebGPU type definitions ```bash npm install --save-dev @webgpu/types ``` ## Bundler Configuration [Section titled “Bundler Configuration”](#bundler-configuration) TypeGPU’s shader transpilation requires `unplugin-typegpu`. The plugin supports Vite, Webpack, Rollup, esbuild, and other bundlers via unplugin. ## First Program [Section titled “First Program”](#first-program) A complete example that squares an array of numbers on the GPU: complete-example.ts ```typescript import tgpu from "typegpu"; import { arrayOf, f32 } from "typegpu/data"; async function main() { const root = await tgpu.init(); const inputData = [1, 2, 3, 4, 5]; // Create buffers with type schemas const inputBuffer = root .createBuffer(arrayOf(f32, 5), inputData) .$usage("storage"); const outputBuffer = root .createBuffer(arrayOf(f32, 5)) .$usage("storage"); // Define compute shader const squareNumbers = tgpu .fn([inputBuffer, outputBuffer]) .does(() => { "use gpu"; const idx = builtin.globalInvocationId.x; outputBuffer[idx] = inputBuffer[idx] * inputBuffer[idx]; }) .$name("squareNumbers"); // Create and execute pipeline const pipeline = root.makeComputePipeline(squareNumbers).$workgroupSize(1); root .createCommandEncoder() .beginComputePass() .setPipeline(pipeline) .dispatchWorkgroups(5) .end() .submit(); const results = await outputBuffer.read(); console.log("Output:", Array.from(results)); // [1, 4, 9, 16, 25] root.destroy(); } main(); ``` ### Key Components [Section titled “Key Components”](#key-components) | Component | Description | | ----------------------- | ---------------------------------------------------------------------------------- | | `tgpu.init()` | Requests a GPU device and returns a root object managing all TypeGPU operations | | `arrayOf(f32, 5)` | Defines an array of 5 floats; used for buffer sizes, validation, and serialization | | `$usage("storage")` | Sets buffer usage flags (`storage`, `uniform`, `vertex`, `copy-src`, `copy-dst`) | | `"use gpu"` directive | Marks functions for TGSL transpilation | | `makeComputePipeline()` | Creates a compute pipeline from a TGSL function | | `buffer.read()` | Asynchronously retrieves data from GPU to CPU | ## Initialization Patterns [Section titled “Initialization Patterns”](#initialization-patterns) ## TypeScript Configuration [Section titled “TypeScript Configuration”](#typescript-configuration) tsconfig.json ```json { "compilerOptions": { "target": "ES2020", "module": "ESNext", "moduleResolution": "bundler", "types": ["@webgpu/types"], "strict": true } } ``` For Vite projects, add to `src/vite-env.d.ts`: src/vite-env.d.ts ```typescript /// <reference types="vite/client" /> /// <reference types="@webgpu/types" /> ``` ## Working with Schemas [Section titled “Working with Schemas”](#working-with-schemas) ### Primitive Types [Section titled “Primitive Types”](#primitive-types) Available primitives ```typescript import { f32, i32, u32, bool, vec2f, vec3f, vec4f, mat4x4f } from "typegpu/data"; ``` ### Structs [Section titled “Structs”](#structs) Defining a struct schema ```typescript import { struct, f32, vec3f } from "typegpu/data"; const Material = struct({ albedo: vec3f, roughness: f32, metallic: f32, }); ``` ### Arrays [Section titled “Arrays”](#arrays) Fixed-size arrays ```typescript import { arrayOf, f32 } from "typegpu/data"; const FloatArray = arrayOf(f32, 100); ``` ### Type Extraction [Section titled “Type Extraction”](#type-extraction) Extracting TypeScript types from schemas ```typescript const Particle = struct({ position: vec3f, velocity: vec3f, mass: f32, }); type ParticleData = typeof Particle.infer; const particle: ParticleData = { position: [0, 0, 0], velocity: [1, 0, 0], mass: 1.0, }; ``` ## Buffer Usage [Section titled “Buffer Usage”](#buffer-usage) Buffers require appropriate usage flags for different operations: Buffer usage patterns ```typescript // Storage buffer for shader read/write const storage = root.createBuffer(schema).$usage("storage"); // Uniform buffer for constants const uniform = root.createBuffer(schema).$usage("uniform"); // Buffer readable from CPU const readable = root .createBuffer(schema) .$usage("storage") .$usage("copy-src"); // Buffer writable from CPU after creation const writable = root .createBuffer(schema) .$usage("storage") .$usage("copy-dst"); ``` ## Naming Resources [Section titled “Naming Resources”](#naming-resources) Use `$name()` for debugging: Named resources for debugging ```typescript const particleBuffer = root .createBuffer(particleSchema) .$usage("storage") .$name("particlePositions"); const updateShader = tgpu .fn([particleBuffer]) .does(() => { "use gpu"; /* ... */ }) .$name("updateParticles"); ``` ## Project Structure [Section titled “Project Structure”](#project-structure) ```plaintext project-root/ ├── src/ │ ├── gpu/ │ │ ├── schemas/ # Data type definitions │ │ ├── shaders/ # TGSL functions │ │ │ ├── compute/ │ │ │ └── render/ │ │ └── pipelines/ # Pipeline configurations │ └── main.ts ├── vite.config.ts └── tsconfig.json ``` **Principles:** * Separate GPU and CPU code * Group shaders by purpose * Centralize schema definitions * Write reusable shader functions ## TGSL Requirements [Section titled “TGSL Requirements”](#tgsl-requirements) Required for TGSL TGSL functions require: 1. The `'use gpu'` directive as the first statement 2. The `unplugin-typegpu` bundler plugin configured 3. Buffer dependencies passed to `tgpu.fn()` Correct vs incorrect TGSL ```typescript // ✓ Correct const shader = tgpu.fn([buffer]).does(() => { "use gpu"; // Shader code }); // ✗ Missing directive - won't transpile const broken = tgpu.fn([buffer]).does(() => { // No 'use gpu' - this runs on CPU only }); ``` ## Resources [Section titled “Resources”](#resources)

# TypeGPU Tooling

## Overview [Section titled “Overview”](#overview) TypeGPU provides two official tools for enhanced development workflows: * **unplugin-typegpu**: Build plugin enabling JavaScript-to-WGSL transpilation * **tgpu-gen**: CLI for generating TypeGPU definitions from existing WGSL files ## unplugin-typegpu [Section titled “unplugin-typegpu”](#unplugin-typegpu) Build plugin that hooks into your bundler to enable TGSL (TypeGPU Shading Language) features. ### Installation [Section titled “Installation”](#installation) ```bash npm install --save-dev unplugin-typegpu ``` ### Bundler Configuration [Section titled “Bundler Configuration”](#bundler-configuration) ### Plugin Options [Section titled “Plugin Options”](#plugin-options) Configuration options ```javascript typegpuPlugin({ include: ["**/*.ts", "**/*.tsx"], // Files to process exclude: ["node_modules/**"], // Files to skip autoNamingEnabled: true, // Auto-name resources from variables earlyPruning: true, // Skip files without TypeGPU code }) ``` ### Features [Section titled “Features”](#features) #### 1. “use gpu” Directive [Section titled “1. “use gpu” Directive”](#1-use-gpu-directive) Write shader functions in TypeScript that transpile to WGSL: Shell-less GPU functions ```typescript const computeDistance = (a: Vec2f, b: Vec2f) => { "use gpu"; const diff = std.sub(a, b); return std.length(diff); }; // Works both on CPU and GPU const cpuResult = computeDistance(vec2f(0, 0), vec2f(3, 4)); // 5.0 // Also generates WGSL for GPU execution ``` #### 2. Automatic Resource Naming [Section titled “2. Automatic Resource Naming”](#2-automatic-resource-naming) Without plugin: ```typescript const positionBuffer = root.createBuffer(d.arrayOf(d.vec3f, 100)) .$name("positionBuffer"); // Manual naming required ``` With plugin: ```typescript const positionBuffer = root.createBuffer(d.arrayOf(d.vec3f, 100)); // Automatically named "positionBuffer" from variable name ``` #### 3. Automatic External Detection [Section titled “3. Automatic External Detection”](#3-automatic-external-detection) Without plugin: ```typescript const shader = tgpu.fn([d.u32], d.void).does`(idx: u32) { positions[idx] = velocities[idx]; }`.$uses({ positions: posBuffer, velocities: velBuffer }); // Manual ``` With plugin: ```typescript const shader = tgpu.fn([d.u32], d.void).does`(idx: u32) { ${posBuffer}[idx] = ${velBuffer}[idx]; }`; // Externals detected automatically ``` ## tgpu-gen CLI [Section titled “tgpu-gen CLI”](#tgpu-gen-cli) Generates TypeGPU TypeScript definitions from existing WGSL shader files. ### Installation [Section titled “Installation”](#installation-1) ```bash # Use directly with npx npx tgpu-gen shader.wgsl # Or install globally npm install -g tgpu-gen ``` ### Basic Usage [Section titled “Basic Usage”](#basic-usage) Generate from single file ```bash tgpu-gen path/to/shader.wgsl # Creates shader.ts in same directory ``` Batch processing with globs ```bash tgpu-gen "shaders/*.wgsl" -o "generated/*.ts" # Recursive tgpu-gen "src/**/*.wgsl" -o "types/**/*.ts" ``` ### Watch Mode [Section titled “Watch Mode”](#watch-mode) Continuous generation ```bash tgpu-gen "shaders/*.wgsl" --output "generated/*.ts" --watch ``` ### Output Options [Section titled “Output Options”](#output-options) | Option | Description | | -------------- | --------------------------------------- | | `-o, --output` | Output path pattern | | `--keep` | Skip existing files | | `--overwrite` | Replace existing files | | `--commonjs` | Generate CommonJS instead of ES modules | ### Supported Extensions [Section titled “Supported Extensions”](#supported-extensions) Input: `.wgsl` Output: `.ts`, `.js`, `.mjs`, `.cjs`, `.mts`, `.cts` ### Example [Section titled “Example”](#example) input.wgsl ```wgsl struct Particle { position: vec3f, velocity: vec3f, } @group(0) @binding(0) var<storage, read_write> particles: array<Particle>; @compute @workgroup_size(64) fn updateParticles(@builtin(global_invocation_id) id: vec3u) { particles[id.x].position += particles[id.x].velocity; } ``` ```bash tgpu-gen input.wgsl ``` input.ts (generated) ```typescript import * as d from "typegpu/data"; import tgpu from "typegpu"; export const Particle = d.struct({ position: d.vec3f, velocity: d.vec3f, }); export const updateParticles = tgpu .computeFn({ workgroupSize: [64] }) .does(/* ... */); ``` ## When to Use Each Tool [Section titled “When to Use Each Tool”](#when-to-use-each-tool) | Scenario | Tool | | ----------------------- | ---------------------- | | New TypeGPU project | unplugin-typegpu | | Migrating existing WGSL | tgpu-gen | | React Native | unplugin-typegpu/babel | | Maximum type safety | Both together |

# TypeGPU Utilities

## Overview [Section titled “Overview”](#overview) TypeGPU provides companion packages for common GPU programming tasks: * **@typegpu/noise**: Pseudo-random and Perlin noise generation * **@typegpu/color**: Color space conversions and manipulation These packages work with both TypeGPU and vanilla WebGPU projects. ## @typegpu/noise [Section titled “@typegpu/noise”](#typegpunoise) Pseudo-random utilities for procedural generation, visual effects, and simulations. ### Installation [Section titled “Installation”](#installation) ```bash npm install @typegpu/noise ``` ### Random Number Generation [Section titled “Random Number Generation”](#random-number-generation) Uniform random \[0, 1) ```typescript import { random } from "@typegpu/noise"; import * as d from "typegpu/data"; const computeShader = tgpu.fn([d.u32], d.f32).does`(seed: u32) -> f32 { // Returns value in [0, 1) return ${random.uniform}(seed); }`; ``` ### Perlin Noise [Section titled “Perlin Noise”](#perlin-noise) Smooth, natural-looking variations for terrain, textures, and effects. 2D Perlin noise ```typescript import { perlin2d } from "@typegpu/noise"; import * as d from "typegpu/data"; const generateTerrain = tgpu.fn([d.vec2f], d.f32).does`(pos: vec2f) -> f32 { // Returns value in [-1, 1] return ${perlin2d}.sample(pos); }`; ``` 3D Perlin noise ```typescript import { perlin3d } from "@typegpu/noise"; const volumetricNoise = tgpu.fn([d.vec3f], d.f32).does`(pos: vec3f) -> f32 { return ${perlin3d}.sample(pos); }`; ``` ### Fractal Brownian Motion (FBM) [Section titled “Fractal Brownian Motion (FBM)”](#fractal-brownian-motion-fbm) Layer multiple noise octaves for complex patterns: FBM pattern ```typescript import { perlin2d } from "@typegpu/noise"; const fbm = tgpu.fn([d.vec2f, d.i32], d.f32).does`(pos: vec2f, octaves: i32) -> f32 { var value = 0.0; var amplitude = 0.5; var frequency = 1.0; var p = pos; for (var i = 0; i < octaves; i++) { value += amplitude * ${perlin2d}.sample(p * frequency); amplitude *= 0.5; frequency *= 2.0; } return value; }`; ``` ### Use Cases [Section titled “Use Cases”](#use-cases) | Function | Output Range | Use Case | | ----------------- | ------------ | -------------------------------- | | `random.uniform` | \[0, 1) | Particle spawning, randomization | | `perlin2d.sample` | \[-1, 1] | Terrain, textures, 2D effects | | `perlin3d.sample` | \[-1, 1] | Volumetric effects, 3D textures | ## @typegpu/color [Section titled “@typegpu/color”](#typegpucolor) Color space conversions and manipulation for GPU shaders. ### Installation [Section titled “Installation”](#installation-1) ```bash npm install @typegpu/color ``` ### Color Spaces [Section titled “Color Spaces”](#color-spaces) RGB to OKLab conversion ```typescript import { rgbToOklab, oklabToRgb } from "@typegpu/color"; import * as d from "typegpu/data"; const adjustLightness = tgpu.fn([d.vec3f, d.f32], d.vec3f) .does`(rgb: vec3f, factor: f32) -> vec3f { // Convert to perceptually uniform space var lab = ${rgbToOklab}(rgb); // Adjust lightness (L channel) lab.x *= factor; // Convert back to RGB return ${oklabToRgb}(lab); }`; ``` ### Color Blending [Section titled “Color Blending”](#color-blending) Perceptually smooth blending ```typescript import { rgbToOklab, oklabToRgb } from "@typegpu/color"; const blendColors = tgpu.fn([d.vec3f, d.vec3f, d.f32], d.vec3f) .does`(a: vec3f, b: vec3f, t: f32) -> vec3f { let labA = ${rgbToOklab}(a); let labB = ${rgbToOklab}(b); // Blend in OKLab for perceptually uniform interpolation let blended = mix(labA, labB, t); return ${oklabToRgb}(blended); }`; ``` ## Zero-Initialized Values [Section titled “Zero-Initialized Values”](#zero-initialized-values) TypeGPU v0.7+ allows creating zero-initialized values from any schema: Zero initialization ```typescript import * as d from "typegpu/data"; // Define a struct schema const Particle = d.struct({ position: d.vec3f, velocity: d.vec3f, lifetime: d.f32, }); // Create zero-initialized value const emptyParticle = Particle(); // { position: [0,0,0], velocity: [0,0,0], lifetime: 0 } // Works with any type const zeroVec = d.vec3f(); // [0, 0, 0] const zeroMat = d.mat4x4f(); // Identity-like zero matrix const zeroArray = d.arrayOf(d.f32, 10)(); // [0,0,0,0,0,0,0,0,0,0] ``` ### Buffer Initialization [Section titled “Buffer Initialization”](#buffer-initialization) Initialize buffer with zeros ```typescript const particleBuffer = root .createBuffer(d.arrayOf(Particle, 1000)) .$usage("storage"); // Write zeros to clear buffer const zeros = d.arrayOf(Particle, 1000)(); particleBuffer.write(zeros); ``` ## Vanilla WebGPU Compatibility [Section titled “Vanilla WebGPU Compatibility”](#vanilla-webgpu-compatibility) All utility packages work with vanilla WebGPU (no TypeGPU required): Using @typegpu/noise with vanilla WebGPU ```typescript import { perlin2d } from "@typegpu/noise"; import tgpu from "typegpu"; // Get WGSL code for the function const wgslCode = tgpu.resolve({ noise: perlin2d, }); // Use in your existing WGSL shader const shaderCode = ` ${wgslCode} @compute @workgroup_size(8, 8) fn main(@builtin(global_invocation_id) id: vec3u) { let uv = vec2f(id.xy) / 512.0; let n = noise_sample(uv * 10.0); // Generated function name // ... use noise value } `; ``` ## Package Summary [Section titled “Package Summary”](#package-summary) | Package | Features | Version | | ---------------- | -------------------- | ------- | | `@typegpu/noise` | Random, Perlin 2D/3D | 0.7.0+ | | `@typegpu/color` | OKLab conversions | 0.7.0+ |

# WebGPU Core Concepts

## Overview [Section titled “Overview”](#overview) WebGPU is a modern graphics and compute API for the web that provides low-level, high-performance access to GPU hardware. Designed from the ground up to align with contemporary GPU architecture, WebGPU maps efficiently to native APIs including Vulkan (cross-platform), Metal (Apple), and Direct3D 12 (Windows). WebGPU enables high-fidelity 3D rendering with advanced techniques like physically-based rendering and complex post-processing pipelines. It treats general-purpose GPU computation (GPGPU) as a first-class feature, making it suitable for machine learning inference, scientific simulations, data visualization, and video processing. ## GPUAdapter: Hardware Abstraction [Section titled “GPUAdapter: Hardware Abstraction”](#gpuadapter-hardware-abstraction) The GPUAdapter represents physical GPU hardware and serves as the entry point for capability discovery. It abstracts the underlying graphics hardware and driver while exposing machine capabilities through a privacy-conscious binning mechanism. Requesting an adapter ```typescript const adapter = await navigator.gpu.requestAdapter({ powerPreference: "high-performance", forceFallbackAdapter: false, }); if (!adapter) { throw new Error("WebGPU adapter not available"); } ``` ### Querying Capabilities [Section titled “Querying Capabilities”](#querying-capabilities) Adapters expose capabilities through two interfaces: Checking features and limits ```typescript // Feature enumeration - optional capabilities as strings console.log("Features:", Array.from(adapter.features)); if (adapter.features.has("texture-compression-bc")) { // BC compressed textures supported } // Capability limits - numeric constraints console.log("Max texture size:", adapter.limits.maxTextureDimension2D); console.log("Max buffer size:", adapter.limits.maxBufferSize); ``` ## GPUDevice: Logical Connection [Section titled “GPUDevice: Logical Connection”](#gpudevice-logical-connection) The GPUDevice is a logical connection to the GPU through which almost all WebGPU operations are performed. While the adapter represents physical hardware, the device provides an isolated, secure interface for creating resources and submitting work. Creating a device with features ```typescript const device = await adapter.requestDevice({ requiredFeatures: ["texture-compression-bc"], requiredLimits: { maxStorageBufferBindingSize: 512 * 1024 * 1024, }, }); ``` Feature Requests If any requested feature is unavailable or any limit exceeds the adapter’s maximum, the Promise rejects. Always check adapter capabilities before requesting. The device manages all GPU resources: * **Buffers** — GPU-accessible memory * **Textures** — Image data * **Shader modules** — Compiled WGSL * **Pipelines** — Complete rendering/compute state * **Bind groups** — Resource binding configuration ### Device Lost Handling [Section titled “Device Lost Handling”](#device-lost-handling) A device can become “lost” due to GPU driver crashes, system sleep/wake cycles, or hardware removal. Handling device loss ```typescript const device = await adapter.requestDevice(); device.lost.then((info) => { console.error(`Device lost: ${info.message}`); if (info.reason !== "destroyed") { initializeWebGPU(); // Attempt recovery } }); device.addEventListener("uncapturederror", (event) => { console.error("Uncaptured WebGPU error:", event.error); }); ``` Always Handle Device Loss Set up `device.lost` and error handlers immediately after device creation. Without these handlers, your application will fail silently when the GPU becomes unavailable. ## GPUQueue: Command Execution [Section titled “GPUQueue: Command Execution”](#gpuqueue-command-execution) The GPUQueue controls submission and execution of GPU commands. Each device has a default queue accessible via `device.queue`. ### Submitting Commands [Section titled “Submitting Commands”](#submitting-commands) Basic command submission ```typescript const commandEncoder = device.createCommandEncoder(); // ... encode commands ... const commandBuffer = commandEncoder.finish(); device.queue.submit([commandBuffer]); ``` Multiple command buffers in a single submit execute in sequence. ### Direct Data Writes [Section titled “Direct Data Writes”](#direct-data-writes) For small to medium updates, use direct write methods: Writing data to GPU resources ```typescript // Write to buffer const uniformData = new Float32Array([1.0, 0.0, 0.0, 1.0]); device.queue.writeBuffer(uniformBuffer, 0, uniformData); // Write to texture device.queue.writeTexture( { texture: gpuTexture }, imageData, { bytesPerRow: 256 * 4 }, { width: 256, height: 256 } ); ``` ### Synchronization [Section titled “Synchronization”](#synchronization) Commands execute asynchronously. Use `onSubmittedWorkDone()` when you need to wait: Waiting for GPU completion ```typescript await device.queue.onSubmittedWorkDone(); console.log("All GPU work completed"); ``` ## Initialization Flow [Section titled “Initialization Flow”](#initialization-flow) A complete, production-ready initialization: Complete WebGPU initialization ```typescript async function initializeWebGPU(): Promise<{ adapter: GPUAdapter; device: GPUDevice; format: GPUTextureFormat; }> { if (!navigator.gpu) { throw new Error("WebGPU not supported"); } const adapter = await navigator.gpu.requestAdapter({ powerPreference: "high-performance", }); if (!adapter) { throw new Error("No WebGPU adapter found"); } const requiredFeatures: GPUFeatureName[] = []; if (adapter.features.has("texture-compression-bc")) { requiredFeatures.push("texture-compression-bc"); } const device = await adapter.requestDevice({ requiredFeatures, requiredLimits: { maxStorageBufferBindingSize: adapter.limits.maxStorageBufferBindingSize, }, }); device.lost.then((info) => { console.error(`Device lost: ${info.message}`); if (info.reason !== "destroyed") { initializeWebGPU(); } }); device.addEventListener("uncapturederror", (event) => { console.error("Uncaptured error:", event.error); }); const format = navigator.gpu.getPreferredCanvasFormat(); return { adapter, device, format }; } ``` ## Stateless Pipeline Architecture [Section titled “Stateless Pipeline Architecture”](#stateless-pipeline-architecture) WebGPU uses an explicit, immutable pipeline object model. Unlike WebGL’s state machine where you configure global state incrementally, WebGPU pipelines encapsulate complete GPU state at creation time. Creating and using a render pipeline ```typescript const pipeline = device.createRenderPipeline({ layout: pipelineLayout, vertex: { module: shaderModule, entryPoint: "vertexMain", buffers: [vertexBufferLayout], }, fragment: { module: shaderModule, entryPoint: "fragmentMain", targets: [{ format: "bgra8unorm" }], }, primitive: { topology: "triangle-list" }, }); // In render pass passEncoder.setPipeline(pipeline); passEncoder.setVertexBuffer(0, vertexBuffer); passEncoder.setBindGroup(0, bindGroup); passEncoder.drawIndexed(36); ``` ## Features and Limits [Section titled “Features and Limits”](#features-and-limits) ### Features [Section titled “Features”](#features) Optional capabilities checked via `adapter.features`: Checking optional features ```typescript const featureChecks = { textureCompression: { bc: adapter.features.has("texture-compression-bc"), etc2: adapter.features.has("texture-compression-etc2"), astc: adapter.features.has("texture-compression-astc"), }, shaderFeatures: { float16: adapter.features.has("shader-f16"), }, }; ``` Requesting available features ```typescript async function createDeviceWithFeatures(adapter: GPUAdapter) { const desired: GPUFeatureName[] = ["texture-compression-bc", "shader-f16"]; const available = desired.filter((f) => adapter.features.has(f)); return adapter.requestDevice({ requiredFeatures: available, }); } ``` ### Limits [Section titled “Limits”](#limits) Numeric constraints queried via `adapter.limits`: Querying device limits ```typescript const limits = adapter.limits; console.log({ maxTextureDimension2D: limits.maxTextureDimension2D, maxBufferSize: limits.maxBufferSize, maxBindGroups: limits.maxBindGroups, maxComputeWorkgroupSizeX: limits.maxComputeWorkgroupSizeX, }); ``` Requesting higher limits ```typescript const requestedLimit = 1024 * 1024 * 1024; const actualLimit = Math.min(requestedLimit, adapter.limits.maxStorageBufferBindingSize); const device = await adapter.requestDevice({ requiredLimits: { maxStorageBufferBindingSize: actualLimit }, }); ``` ## Resource Lifecycle [Section titled “Resource Lifecycle”](#resource-lifecycle) ### Creation [Section titled “Creation”](#creation) Resources are created through device methods with descriptor objects: Creating buffers and textures ```typescript const vertexBuffer = device.createBuffer({ size: 1024, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, mappedAtCreation: false, }); const texture = device.createTexture({ size: { width: 512, height: 512 }, format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST, }); ``` ### Usage Flags [Section titled “Usage Flags”](#usage-flags) The `usage` parameter declares how resources will be used. WebGPU validates actual usage matches declared usage. Combine flags with bitwise OR: `GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST` ### Destruction [Section titled “Destruction”](#destruction) Explicitly destroy large resources when done: Resource cleanup ```typescript buffer.destroy(); texture.destroy(); ``` Memory Management For loops creating many resources, explicit destruction prevents GPU memory accumulation. Always destroy resources you no longer need. ### Error Scopes [Section titled “Error Scopes”](#error-scopes) Use error scopes for expected failure points: Catching creation errors ```typescript device.pushErrorScope("validation"); const buffer = device.createBuffer({ /* potentially invalid */ }); const error = await device.popErrorScope(); if (error) { console.warn("Buffer creation failed:", error.message); } ``` ## GPU Process Architecture [Section titled “GPU Process Architecture”](#gpu-process-architecture) WebGPU runs in a dedicated GPU process separate from the content process where JavaScript executes.

# WGSL Address Spaces

## Overview [Section titled “Overview”](#overview) WGSL defines five address spaces representing distinct memory regions with different performance, visibility, and access characteristics. Each address space maps to specific hardware memory types and determines how GPU cores read and write data. ## function Address Space [Section titled “function Address Space”](#function-address-space) The `function` address space stores local variables that exist only during function execution, analogous to stack memory in CPU programming. Function-local variables ```wgsl fn compute_normal(pos: vec3f) -> vec3f { var temp_vec: vec3f = pos; // Implicitly function address space var result: vec3f; temp_vec = normalize(temp_vec); result = temp_vec * 2.0 - 1.0; return result; } ``` | Characteristic | Description | | -------------- | ----------------------------------------- | | Lifetime | Allocated on entry, deallocated on return | | Visibility | Private to invocation | | Speed | Fastest (registers or L1 cache) | | Size limit | 8KB per function | Caution Large arrays in function space may spill to slower global memory. Keep function-local arrays small; use storage buffers for large datasets. ## private Address Space [Section titled “private Address Space”](#private-address-space) The `private` address space stores module-scope variables that persist for the entire shader execution, with each invocation getting its own copy. Private module-scope variables ```wgsl var<private> invocation_counter: u32 = 0u; var<private> accumulated_color: vec4f; @fragment fn main(@location(0) uv: vec2f) -> @location(0) vec4f { invocation_counter += 1u; accumulated_color += texture_sample(uv); return accumulated_color; } ``` | Characteristic | Description | | -------------- | -------------------------- | | Scope | Module (outside functions) | | Visibility | Per-invocation isolation | | Mutability | Read and write | | Size limit | 8KB per shader | Use `private` for accumulators, caching expensive computations, and state that persists within a single invocation. ## workgroup Address Space [Section titled “workgroup Address Space”](#workgroup-address-space) The `workgroup` address space provides shared memory visible to all invocations within a compute shader workgroup. This is critical for parallel algorithms requiring thread cooperation. Shared workgroup memory ```wgsl var<workgroup> shared_data: array<f32, 256>; var<workgroup> group_sum: atomic<u32>; @compute @workgroup_size(256) fn main(@builtin(local_invocation_index) local_idx: u32) { // Each thread loads one element shared_data[local_idx] = input_buffer[local_idx]; // Barrier ensures all loads complete before any thread proceeds workgroupBarrier(); // Now all threads can safely read any element let neighbor = shared_data[(local_idx + 1u) % 256u]; } ``` | Characteristic | Description | | -------------- | --------------------------------------- | | Availability | Compute shaders only | | Scope | Visible to all invocations in workgroup | | Speed | Fast (on-chip shared memory) | | Size limit | \~16KB typically | Synchronization Required Missing or misplaced barriers cause race conditions. Always use `workgroupBarrier()` before reading data written by other threads. ### Synchronization Pattern [Section titled “Synchronization Pattern”](#synchronization-pattern) Three-phase synchronization ```wgsl var<workgroup> tile: array<f32, 256>; @compute @workgroup_size(16, 16) fn main(@builtin(local_invocation_id) local_id: vec3u) { let idx = local_id.x + local_id.y * 16u; // Phase 1: All threads load tile[idx] = compute_value(local_id); workgroupBarrier(); // Phase 2: All threads process let sum = tile[idx] + tile[(idx + 1u) % 256u]; workgroupBarrier(); // Phase 3: Write results output[idx] = sum; } ``` ### Atomics in Workgroup Memory [Section titled “Atomics in Workgroup Memory”](#atomics-in-workgroup-memory) Workgroup atomic counter ```wgsl var<workgroup> counter: atomic<u32>; @compute @workgroup_size(64) fn count_positives( @builtin(local_invocation_index) local_idx: u32, @builtin(workgroup_id) wg_id: vec3u ) { if (local_idx == 0u) { atomicStore(&counter, 0u); } workgroupBarrier(); let global_idx = wg_id.x * 64u + local_idx; if (data[global_idx] > 0.0) { atomicAdd(&counter, 1u); } workgroupBarrier(); if (local_idx == 0u) { result[wg_id.x] = atomicLoad(&counter); } } ``` ## uniform Address Space [Section titled “uniform Address Space”](#uniform-address-space) The `uniform` address space stores read-only data shared across all invocations. The GPU broadcasts uniform data efficiently, making it ideal for transformation matrices and material properties. Uniform buffer usage ```wgsl struct CameraUniforms { view_matrix: mat4x4f, projection_matrix: mat4x4f, camera_position: vec3f, near_plane: f32, far_plane: f32, } @group(0) @binding(0) var<uniform> camera: CameraUniforms; @vertex fn vertex_main(@location(0) position: vec3f) -> @builtin(position) vec4f { let world_pos = vec4f(position, 1.0); let view_pos = camera.view_matrix * world_pos; return camera.projection_matrix * view_pos; } ``` | Characteristic | Description | | -------------- | ----------------------------- | | Access | Read-only | | Visibility | All invocations see same data | | Optimization | GPU broadcasts efficiently | | Size limit | 16-64KB per binding | ### Uniform vs Storage [Section titled “Uniform vs Storage”](#uniform-vs-storage) | Aspect | `uniform` | `storage` | | -------------- | ---------------------- | ----------------------------- | | Access | Read-only | Read-only or read-write | | Size limit | \~64KB | GB+ | | Use case | Per-draw constants | Large datasets | | Access pattern | Same value all threads | Different elements per thread | ## storage Address Space [Section titled “storage Address Space”](#storage-address-space) The `storage` address space provides access to large buffers with optional write capability. Storage buffers can hold gigabytes of data and support per-element random access. Storage buffer read/write ```wgsl struct Particle { position: vec3f, velocity: vec3f, lifetime: f32, } @group(0) @binding(0) var<storage, read> particles_in: array<Particle>; @group(0) @binding(1) var<storage, read_write> particles_out: array<Particle>; @compute @workgroup_size(64) fn update_particles(@builtin(global_invocation_id) global_id: vec3u) { let idx = global_id.x; var p = particles_in[idx]; p.position += p.velocity * delta_time; p.lifetime -= delta_time; particles_out[idx] = p; } ``` ### Access Modes [Section titled “Access Modes”](#access-modes) | Mode | Description | | ------------ | --------------------------------------- | | `read` | Read-only access, enables optimizations | | `read_write` | Full read and write access | ### Runtime-Sized Arrays [Section titled “Runtime-Sized Arrays”](#runtime-sized-arrays) Storage buffers support runtime-sized arrays as the last struct member: Runtime-sized array ```wgsl struct ParticleBuffer { count: u32, _padding: u32, _padding2: u32, _padding3: u32, particles: array<Particle> // Size determined at runtime } @group(0) @binding(0) var<storage, read> data: ParticleBuffer; @compute @workgroup_size(64) fn process(@builtin(global_invocation_id) id: vec3u) { if (id.x >= data.count) { return; } let p = data.particles[id.x]; } ``` ### Atomics in Storage [Section titled “Atomics in Storage”](#atomics-in-storage) Global atomic counter ```wgsl @group(0) @binding(0) var<storage, read_write> global_counter: atomic<u32>; @compute @workgroup_size(64) fn count(@builtin(global_invocation_id) id: vec3u) { if (condition) { atomicAdd(&global_counter, 1u); } } ``` ## Address Space Comparison [Section titled “Address Space Comparison”](#address-space-comparison) | Address Space | Scope | Visibility | Access | Speed | Size | | ------------- | --------- | ---------- | ----------- | --------- | ------- | | `function` | Function | Invocation | R/W | Fastest | 8KB | | `private` | Module | Invocation | R/W | Very Fast | 8KB | | `workgroup` | Workgroup | Workgroup | R/W | Fast | 16KB | | `uniform` | Buffer | All | Read | Fast | 16-64KB | | `storage` | Buffer | All | Read or R/W | Variable | Large | ## Memory Access Patterns [Section titled “Memory Access Patterns”](#memory-access-patterns) ### Using Workgroup Memory for Irregular Access [Section titled “Using Workgroup Memory for Irregular Access”](#using-workgroup-memory-for-irregular-access) Blur kernel with workgroup cache ```wgsl var<workgroup> tile: array<f32, 256>; @compute @workgroup_size(16, 16) fn blur_kernel( @builtin(global_invocation_id) global_id: vec3u, @builtin(local_invocation_id) local_id: vec3u ) { // Coalesced load into shared memory let local_idx = local_id.x + local_id.y * 16u; tile[local_idx] = input[global_id.x + global_id.y * width]; workgroupBarrier(); // Random access within fast workgroup memory var sum: f32 = 0.0; for (var dy = -1; dy <= 1; dy++) { for (var dx = -1; dx <= 1; dx++) { let neighbor_idx = (local_id.x + dx) + (local_id.y + dy) * 16u; sum += tile[neighbor_idx]; } } output[global_id.x + global_id.y * width] = sum / 9.0; } ``` ## Choosing an Address Space [Section titled “Choosing an Address Space”](#choosing-an-address-space) ### Optimization Tips [Section titled “Optimization Tips”](#optimization-tips) Cache storage reads ```wgsl // ✗ Suboptimal: multiple reads let result = data[id.x] * 2.0 + data[id.x] * 3.0; // ✓ Better: cache in local variable let value = data[id.x]; let result = value * 2.0 + value * 3.0; ``` Use workgroup memory for reuse ```wgsl var<workgroup> cached: array<f32, 256>; cached[local_idx] = input_buffer[global_idx]; workgroupBarrier(); let value = cached[some_index]; // Fast access ``` ## TypeGPU Address Space Mapping [Section titled “TypeGPU Address Space Mapping”](#typegpu-address-space-mapping) TypeGPU abstracts address spaces through buffer usage declarations: TypeGPU buffer usage ```typescript import tgpu from "typegpu"; import { arrayOf, f32, struct, vec3f } from "typegpu/data"; // Uniform buffer (uniform address space) const CameraSchema = struct({ viewMatrix: mat4x4f, projMatrix: mat4x4f, }); const camera = root.createBuffer(CameraSchema, data).$usage("uniform"); // Storage buffer read-only (storage, read) const particles = root.createBuffer(arrayOf(ParticleSchema, 1000)).$usage("storage"); // Storage buffer read-write (storage, read_write) const output = root.createBuffer(arrayOf(f32, 1000)).$usage("storage"); ``` TypeGPU infers the appropriate WGSL address space from the usage pattern.

# WGSL Builtin Functions

## Overview [Section titled “Overview”](#overview) WGSL provides builtin functions for mathematical operations, vector/matrix manipulation, texture sampling, and more. This reference covers commonly used functions organized by category. ## Math Functions [Section titled “Math Functions”](#math-functions) ### Trigonometric [Section titled “Trigonometric”](#trigonometric) | Function | Description | Domain | | ------------- | -------------------------- | ----------------------- | | `sin(x)` | Sine | Any | | `cos(x)` | Cosine | Any | | `tan(x)` | Tangent | x ≠ π/2 + nπ | | `asin(x)` | Arc sine | \[-1, 1] | | `acos(x)` | Arc cosine | \[-1, 1] | | `atan(x)` | Arc tangent | Any | | `atan2(y, x)` | Arc tangent of y/x | Any (handles quadrants) | | `sinh(x)` | Hyperbolic sine | Any | | `cosh(x)` | Hyperbolic cosine | Any | | `tanh(x)` | Hyperbolic tangent | Any | | `asinh(x)` | Inverse hyperbolic sine | Any | | `acosh(x)` | Inverse hyperbolic cosine | x ≥ 1 | | `atanh(x)` | Inverse hyperbolic tangent | (-1, 1) | Trigonometry example ```wgsl let angle = atan2(direction.y, direction.x); let rotated = vec2f(cos(angle), sin(angle)); ``` ### Exponential [Section titled “Exponential”](#exponential) | Function | Description | Domain | | ---------------- | ------------------- | ------------------------- | | `pow(x, y)` | x raised to power y | x > 0, or x = 0 and y > 0 | | `exp(x)` | e^x | Any | | `exp2(x)` | 2^x | Any | | `log(x)` | Natural logarithm | x > 0 | | `log2(x)` | Base-2 logarithm | x > 0 | | `sqrt(x)` | Square root | x ≥ 0 | | `inverseSqrt(x)` | 1 / sqrt(x) | x > 0 | Undefined Behavior `pow(-2.0, 0.5)` is undefined (would produce imaginary number). Results vary by GPU. ### Rounding [Section titled “Rounding”](#rounding) | Function | Description | | ---------- | ------------------------------- | | `floor(x)` | Largest integer ≤ x | | `ceil(x)` | Smallest integer ≥ x | | `round(x)` | Nearest integer (ties to even) | | `trunc(x)` | Integer part (toward zero) | | `fract(x)` | Fractional part: `x - floor(x)` | Rounding examples ```wgsl floor(2.7) // 2.0 ceil(2.3) // 3.0 round(2.5) // 2.0 (ties to even) fract(2.7) // 0.7 trunc(-2.7) // -2.0 ``` ### Common [Section titled “Common”](#common) | Function | Description | | ------------------ | ---------------------------- | | `abs(x)` | Absolute value | | `sign(x)` | -1, 0, or 1 | | `min(a, b)` | Smaller value | | `max(a, b)` | Larger value | | `clamp(x, lo, hi)` | Constrain x to \[lo, hi] | | `saturate(x)` | Clamp to \[0, 1] | | `fma(a, b, c)` | Fused multiply-add: a\*b + c | ## Vector Functions [Section titled “Vector Functions”](#vector-functions) ### Geometric [Section titled “Geometric”](#geometric) | Function | Description | | ------------------------ | ------------------------- | | `dot(a, b)` | Dot product | | `cross(a, b)` | Cross product (vec3 only) | | `length(v)` | Vector magnitude | | `distance(a, b)` | Distance between points | | `normalize(v)` | Unit vector | | `faceForward(n, i, ref)` | Flip n if dot(ref, i) < 0 | | `reflect(i, n)` | Reflection vector | | `refract(i, n, eta)` | Refraction vector | Common vector operations ```wgsl let dir = normalize(target - position); let dist = distance(a, b); let reflection = reflect(incident, normal); ``` ### Component Access [Section titled “Component Access”](#component-access) | Function | Description | | -------------------- | --------------------------- | | `all(v)` | True if all components true | | `any(v)` | True if any component true | | `select(f, t, cond)` | Per-component ternary | select replaces ternary operator ```wgsl // WGSL has no ?: operator let result = select(falseValue, trueValue, condition); // Component-wise selection let mixed = select(vec3f(0), vec3f(1), vec3<bool>(true, false, true)); // Result: vec3f(1, 0, 1) ``` ## Interpolation [Section titled “Interpolation”](#interpolation) | Function | Description | | ----------------------- | ------------------------------------- | | `mix(a, b, t)` | Linear interpolation: a\*(1-t) + b\*t | | `step(edge, x)` | 0 if x < edge, else 1 | | `smoothstep(lo, hi, x)` | Smooth Hermite interpolation | Interpolation examples ```wgsl let blend = mix(colorA, colorB, 0.5); // smoothstep: 0 when x <= lo, 1 when x >= hi let fade = smoothstep(0.0, 1.0, t); // step: hard threshold let mask = step(0.5, value); // 0 or 1 ``` ## Texture Functions [Section titled “Texture Functions”](#texture-functions) ### Sampling (Fragment Only) [Section titled “Sampling (Fragment Only)”](#sampling-fragment-only) | Function | Description | | ------------------------------------------- | ------------------------------ | | `textureSample(t, s, coords)` | Sample with filtering | | `textureSampleBias(t, s, coords, bias)` | Sample with LOD bias | | `textureSampleLevel(t, s, coords, level)` | Sample specific mip level | | `textureSampleGrad(t, s, coords, ddx, ddy)` | Sample with explicit gradients | | `textureSampleCompare(t, s, coords, ref)` | Depth comparison sample | Texture sampling ```wgsl @group(0) @binding(0) var tex: texture_2d<f32>; @group(0) @binding(1) var samp: sampler; @fragment fn main(@location(0) uv: vec2f) -> @location(0) vec4f { return textureSample(tex, samp, uv); } ``` Fragment Shader Only `textureSample` requires implicit derivatives, available only in fragment shaders. Use `textureSampleLevel` in vertex/compute shaders. ### Direct Access [Section titled “Direct Access”](#direct-access) | Function | Description | | -------------------------------- | ------------------------ | | `textureLoad(t, coords, level)` | Load exact texel | | `textureStore(t, coords, value)` | Write to storage texture | | `textureDimensions(t)` | Get texture size | | `textureNumLayers(t)` | Get array layer count | | `textureNumLevels(t)` | Get mip level count | | `textureNumSamples(t)` | Get MSAA sample count | Direct texel access ```wgsl // Read exact pixel (no filtering) let texel = textureLoad(tex, vec2i(x, y), 0); // Get texture dimensions let size = textureDimensions(tex, 0); // Returns vec2u ``` ## Derivative Functions [Section titled “Derivative Functions”](#derivative-functions) Fragment shader only—compute rate of change across pixels. | Function | Description | | --------------- | ---------------------------- | | `dpdx(v)` | Partial derivative w\.r.t. x | | `dpdy(v)` | Partial derivative w\.r.t. y | | `fwidth(v)` | abs(dpdx) + abs(dpdy) | | `dpdxCoarse(v)` | Coarse x derivative | | `dpdyCoarse(v)` | Coarse y derivative | | `dpdxFine(v)` | Fine x derivative | | `dpdyFine(v)` | Fine y derivative | Anti-aliased edge detection ```wgsl @fragment fn main(@location(0) uv: vec2f) -> @location(0) vec4f { let value = someFunction(uv); // fwidth gives rate of change across pixel let edge = fwidth(value); let antialiased = smoothstep(0.0, edge, value); return vec4f(antialiased, antialiased, antialiased, 1.0); } ``` ## Pack/Unpack Functions [Section titled “Pack/Unpack Functions”](#packunpack-functions) Compress/decompress data for efficient storage. ### Packing (vec → u32) [Section titled “Packing (vec → u32)”](#packing-vec--u32) | Function | Description | | ------------------ | ----------------------------------- | | `pack4x8snorm(v)` | 4 floats \[-1,1] → 4 signed bytes | | `pack4x8unorm(v)` | 4 floats \[0,1] → 4 unsigned bytes | | `pack2x16snorm(v)` | 2 floats \[-1,1] → 2 signed shorts | | `pack2x16unorm(v)` | 2 floats \[0,1] → 2 unsigned shorts | | `pack2x16float(v)` | 2 floats → 2 half floats | ### Unpacking (u32 → vec) [Section titled “Unpacking (u32 → vec)”](#unpacking-u32--vec) | Function | Description | | -------------------- | -------------------------------- | | `unpack4x8snorm(u)` | 4 signed bytes → vec4f \[-1,1] | | `unpack4x8unorm(u)` | 4 unsigned bytes → vec4f \[0,1] | | `unpack2x16snorm(u)` | 2 signed shorts → vec2f \[-1,1] | | `unpack2x16unorm(u)` | 2 unsigned shorts → vec2f \[0,1] | | `unpack2x16float(u)` | 2 half floats → vec2f | Pack/unpack normals ```wgsl // Pack normal into single u32 let packed = pack4x8snorm(vec4f(normal, 0.0)); // Unpack back to vec4f let unpacked = unpack4x8snorm(packed); let normal = unpacked.xyz; ``` ## Matrix Functions [Section titled “Matrix Functions”](#matrix-functions) | Function | Description | | ---------------- | ------------------ | | `transpose(m)` | Transpose matrix | | `determinant(m)` | Matrix determinant | Matrix operations ```wgsl let normalMatrix = transpose(inverse3x3(modelMatrix)); ``` ## Atomic Functions [Section titled “Atomic Functions”](#atomic-functions) For `atomic<T>` types in workgroup or storage memory. | Function | Description | | -------------------------------------- | -------------------------- | | `atomicLoad(a)` | Read value | | `atomicStore(a, v)` | Write value | | `atomicAdd(a, v)` | Add and return old | | `atomicSub(a, v)` | Subtract and return old | | `atomicMax(a, v)` | Max and return old | | `atomicMin(a, v)` | Min and return old | | `atomicAnd(a, v)` | Bitwise AND and return old | | `atomicOr(a, v)` | Bitwise OR and return old | | `atomicXor(a, v)` | Bitwise XOR and return old | | `atomicExchange(a, v)` | Swap and return old | | `atomicCompareExchangeWeak(a, cmp, v)` | CAS operation | Atomic counter ```wgsl @group(0) @binding(0) var<storage, read_write> counter: atomic<u32>; @compute @workgroup_size(64) fn main() { let oldValue = atomicAdd(&counter, 1u); } ``` ## Synchronization [Section titled “Synchronization”](#synchronization) | Function | Description | | -------------------- | --------------------------------- | | `workgroupBarrier()` | Sync all invocations in workgroup | | `storageBarrier()` | Ensure storage writes visible | | `textureBarrier()` | Ensure texture writes visible | ## Bit Manipulation [Section titled “Bit Manipulation”](#bit-manipulation) | Function | Description | | --------------------------------- | --------------------------- | | `countOneBits(v)` | Count set bits | | `countLeadingZeros(v)` | Leading zero count | | `countTrailingZeros(v)` | Trailing zero count | | `firstLeadingBit(v)` | Position of highest set bit | | `firstTrailingBit(v)` | Position of lowest set bit | | `reverseBits(v)` | Reverse bit order | | `extractBits(v, offset, count)` | Extract bit field | | `insertBits(v, n, offset, count)` | Insert bit field | ## Resources [Section titled “Resources”](#resources)

# WGSL Shading Language

## Overview [Section titled “Overview”](#overview) WGSL (WebGPU Shading Language) is the shading language for WebGPU, designed to be safe, expressive, and portable across GPU architectures. Drawing from Rust’s syntax and safety principles, WGSL provides strong typing, eliminates undefined behavior, and compiles to SPIR-V, Metal Shading Language, or DXIL for cross-platform execution. ## Scalar Types [Section titled “Scalar Types”](#scalar-types) ### Boolean [Section titled “Boolean”](#boolean) Boolean values ```wgsl var is_active: bool = true; var should_render: bool = false; ``` ### Integers [Section titled “Integers”](#integers) 32-bit signed (`i32`) and unsigned (`u32`) integers: Integer types ```wgsl var signed_value: i32 = -42; var unsigned_value: u32 = 100u; // 'u' suffix for unsigned var sum: i32 = signed_value + 10; var product: u32 = unsigned_value * 2u; ``` ### Floating-Point [Section titled “Floating-Point”](#floating-point) Float values ```wgsl var pi: f32 = 3.14159265; var speed_of_light: f32 = 2.998e8; // Scientific notation ``` ### f16 (Optional Feature) [Section titled “f16 (Optional Feature)”](#f16-optional-feature) Half-precision floats ```wgsl enable f16; var half_precision: f16 = 1.5h; ``` ### Type Casting [Section titled “Type Casting”](#type-casting) WGSL requires explicit type conversions: Explicit type conversion ```wgsl var int_value: i32 = 42; var float_value: f32 = f32(int_value); var unsigned_value: u32 = u32(int_value); ``` ## Vector Types [Section titled “Vector Types”](#vector-types) Vectors represent positions, colors, and directions with 2, 3, or 4 components. ### Declaration [Section titled “Declaration”](#declaration) Vector declarations ```wgsl // Explicit types var position: vec3<f32> = vec3<f32>(1.0, 2.0, 3.0); var color: vec4<f32> = vec4<f32>(1.0, 0.5, 0.0, 1.0); // Shorthand types var pos2: vec2f = vec2f(1.0, 2.0); var pos3: vec3f = vec3f(1.0, 2.0, 3.0); var offset: vec3i = vec3i(0, 1, -1); var mask: vec4u = vec4u(1u, 1u, 0u, 1u); ``` ### Constructors [Section titled “Constructors”](#constructors) Vector construction ```wgsl // Scalar broadcast var ones: vec3f = vec3f(1.0); // (1.0, 1.0, 1.0) // Mixed construction var xy: vec2f = vec2f(1.0, 2.0); var xyz: vec3f = vec3f(xy, 3.0); // (1.0, 2.0, 3.0) var xyzw: vec4f = vec4f(xy, 3.0, 4.0); ``` ### Swizzling [Section titled “Swizzling”](#swizzling) Access and rearrange components using `xyzw` or `rgba`: Component swizzling ```wgsl var position: vec4f = vec4f(1.0, 2.0, 3.0, 4.0); var x: f32 = position.x; var xy: vec2f = position.xy; var zyx: vec3f = position.zyx; // Reversed var xxx: vec3f = position.xxx; // Repeated var color: vec4f = vec4f(1.0, 0.5, 0.0, 1.0); var rgb: vec3f = color.rgb; var bgr: vec3f = color.bgr; ``` Caution Do not mix `xyzw` and `rgba` in the same swizzle. ### Operations [Section titled “Operations”](#operations) Vector operations ```wgsl var a: vec3f = vec3f(1.0, 2.0, 3.0); var b: vec3f = vec3f(4.0, 5.0, 6.0); var sum: vec3f = a + b; // Component-wise var product: vec3f = a * b; // Component-wise var scaled: vec3f = a * 2.0; // Scalar multiply var len: f32 = length(a); var normalized: vec3f = normalize(a); var d: f32 = dot(a, b); var cross_prod: vec3f = cross(a, b); // vec3 only ``` ## Matrix Types [Section titled “Matrix Types”](#matrix-types) Matrices are used for transformations. WGSL uses column-major storage. Identity matrix ```wgsl var transform: mat4x4f = mat4x4f( 1.0, 0.0, 0.0, 0.0, // Column 0 0.0, 1.0, 0.0, 0.0, // Column 1 0.0, 0.0, 1.0, 0.0, // Column 2 0.0, 0.0, 0.0, 1.0 // Column 3 ); ``` Column-Major Storage Values are stored by column, not row. This is a common source of confusion: ```wgsl var m: mat3x3f = mat3x3f( 1.0, 2.0, 3.0, // Column 0 (not row!) 4.0, 5.0, 6.0, // Column 1 7.0, 8.0, 9.0 // Column 2 ); // Represents: // [ 1.0 4.0 7.0 ] // [ 2.0 5.0 8.0 ] // [ 3.0 6.0 9.0 ] var col0: vec3f = m[0]; // (1.0, 2.0, 3.0) var element: f32 = m[1][2]; // Column 1, row 2 = 6.0 ``` ### Matrix Operations [Section titled “Matrix Operations”](#matrix-operations) Matrix operations ```wgsl var mat_a: mat4x4f = mat4x4f(/* ... */); var position: vec4f = vec4f(1.0, 2.0, 3.0, 1.0); var combined: mat4x4f = mat_a * mat_b; var transformed: vec4f = mat_a * position; var transposed: mat4x4f = transpose(mat_a); ``` ## Arrays and Structs [Section titled “Arrays and Structs”](#arrays-and-structs) ### Fixed-Size Arrays [Section titled “Fixed-Size Arrays”](#fixed-size-arrays) Fixed-size array ```wgsl var colors: array<vec3f, 4> = array<vec3f, 4>( vec3f(1.0, 0.0, 0.0), vec3f(0.0, 1.0, 0.0), vec3f(0.0, 0.0, 1.0), vec3f(1.0, 1.0, 0.0) ); var first: vec3f = colors[0]; ``` ### Runtime-Sized Arrays [Section titled “Runtime-Sized Arrays”](#runtime-sized-arrays) Only allowed as the last member of a struct in storage buffers: Runtime-sized array in storage ```wgsl struct ParticleBuffer { count: u32, particles: array<vec4f> } @group(0) @binding(0) var<storage, read> data: ParticleBuffer; ``` ### Structs [Section titled “Structs”](#structs) Struct definitions ```wgsl struct Vertex { position: vec3f, normal: vec3f, uv: vec2f, color: vec4f } struct Light { position: vec3f, color: vec3f, intensity: f32, radius: f32 } var light: Light = Light( vec3f(10.0, 10.0, 10.0), vec3f(1.0, 1.0, 1.0), 100.0, 50.0 ); ``` ## Memory Alignment [Section titled “Memory Alignment”](#memory-alignment) WGSL enforces strict alignment rules that differ from typical CPU layouts. | Type | Alignment | | ------------------- | ------------------------------------- | | `f32`, `i32`, `u32` | 4 bytes | | `vec2<T>` | 8 bytes | | `vec3<T>` | **16 bytes** (despite being 12 bytes) | | `vec4<T>` | 16 bytes | | `mat4x4<f32>` | 16 bytes | The vec3 Padding Issue `vec3` aligns to 16 bytes, causing unexpected struct sizes: ```wgsl // Problematic layout struct TwoVec3s { a: vec3f, // Offset 0, size 12, padded to 16 b: vec3f // Offset 16, size 12, padded to 16 } // Total: 32 bytes (not 24) // Solution: Use vec4 or explicit padding struct BetterLayout { a: vec4f, // Set w to 0.0 if unused b: vec4f } ``` ## Functions [Section titled “Functions”](#functions) Function syntax ```wgsl fn add(a: f32, b: f32) -> f32 { return a + b; } fn transform_point(point: vec3f, matrix: mat4x4f, scale: f32) -> vec3f { let transformed = matrix * vec4f(point, 1.0); return transformed.xyz * scale; } ``` Parameters are passed by value. For multiple return values, use a struct. ### Entry Points [Section titled “Entry Points”](#entry-points) Shader entry points ```wgsl @vertex fn vertex_main(@builtin(vertex_index) idx: u32) -> @builtin(position) vec4f { var positions = array<vec2f, 3>( vec2f(0.0, 0.5), vec2f(-0.5, -0.5), vec2f(0.5, -0.5) ); return vec4f(positions[idx], 0.0, 1.0); } @fragment fn fragment_main(@builtin(position) coord: vec4f) -> @location(0) vec4f { return vec4f(1.0, 0.5, 0.0, 1.0); } @compute @workgroup_size(8, 8, 1) fn compute_main(@builtin(global_invocation_id) id: vec3u) { let index = id.x + id.y * 256u; } ``` ## Attributes [Section titled “Attributes”](#attributes) ### @location [Section titled “@location”](#location) Shader stage inputs and outputs: Location attributes ```wgsl struct VertexInput { @location(0) position: vec3f, @location(1) normal: vec3f, @location(2) uv: vec2f } struct VertexOutput { @builtin(position) clip_position: vec4f, @location(0) world_normal: vec3f, @location(1) tex_coords: vec2f } ``` ### @group and @binding [Section titled “@group and @binding”](#group-and-binding) Resource bindings: Resource bindings ```wgsl @group(0) @binding(0) var<uniform> camera: CameraUniforms; @group(0) @binding(1) var<storage, read> vertices: array<Vertex>; @group(0) @binding(2) var<storage, read_write> output: array<vec4f>; @group(1) @binding(0) var my_texture: texture_2d<f32>; @group(1) @binding(1) var my_sampler: sampler; ``` ### @builtin [Section titled “@builtin”](#builtin) ### @workgroup\_size [Section titled “@workgroup\_size”](#workgroup_size) Workgroup size examples ```wgsl @compute @workgroup_size(256) fn compute_1d(@builtin(global_invocation_id) id: vec3u) { } @compute @workgroup_size(16, 16) fn compute_2d(@builtin(global_invocation_id) id: vec3u) { } @compute @workgroup_size(8, 8, 4) fn compute_3d(@builtin(global_invocation_id) id: vec3u) { } ``` ## Control Flow [Section titled “Control Flow”](#control-flow) ### Conditionals [Section titled “Conditionals”](#conditionals) If/else statements ```wgsl fn classify(x: f32) -> i32 { if (x > 0.0) { return 1; } else if (x < 0.0) { return -1; } else { return 0; } } ``` ### Switch [Section titled “Switch”](#switch) Switch statement ```wgsl fn get_color(channel: u32) -> vec3f { switch (channel) { case 0u: { return vec3f(1.0, 0.0, 0.0); } case 1u: { return vec3f(0.0, 1.0, 0.0); } case 2u: { return vec3f(0.0, 0.0, 1.0); } default: { return vec3f(0.0, 0.0, 0.0); } } } // Multiple cases switch (channel) { case 0u, 1u, 2u: { return true; } default: { return false; } } ``` ### Loops [Section titled “Loops”](#loops) Loop constructs ```wgsl // For loop for (var i = 0; i < 100; i++) { sum += data[i]; } // While loop while (value < threshold) { value += 0.1; count++; } // Loop with break loop { if (i >= 10) { break; } i++; } ``` ### Loop with Continuing [Section titled “Loop with Continuing”](#loop-with-continuing) The `continuing` block executes at the end of each iteration (like `for` loop increment): Loop with continuing block ```wgsl var i = 0; loop { if (i >= 10) { break; } // Loop body sum += data[i]; continuing { i++; // Executes after each iteration } } ``` ### break if [Section titled “break if”](#break-if) Early exit with condition in `continuing` block: break if syntax ```wgsl var i = 0; loop { sum += data[i]; continuing { i++; break if i >= limit; // Exit after continuing } } ``` ### continue [Section titled “continue”](#continue) Skip to next iteration: continue statement ```wgsl for (var i = 0; i < 100; i++) { if (data[i] < 0.0) { continue; // Skip negative values } sum += data[i]; } ``` ### select() [Section titled “select()”](#select) Branchless conditional (often faster than `if`): Branchless select ```wgsl var max_val = select(a, b, b > a); // b > a ? b : a var clamped = select(x, 0.0, x < 0.0); // Component-wise for vectors var max_vec = select(a_vec, b_vec, b_vec > a_vec); ``` ### discard [Section titled “discard”](#discard) Kill fragment in fragment shaders: Alpha test with discard ```wgsl @fragment fn alpha_test(@location(0) uv: vec2f) -> @location(0) vec4f { let color = textureSample(tex, samp, uv); if (color.a < 0.5) { discard; } return color; } ``` ## Built-in Functions [Section titled “Built-in Functions”](#built-in-functions) ### Synchronization [Section titled “Synchronization”](#synchronization) Workgroup barrier ```wgsl var<workgroup> shared: array<f32, 256>; @compute @workgroup_size(256) fn compute(@builtin(local_invocation_index) idx: u32) { shared[idx] = f32(idx); workgroupBarrier(); // Wait for all writes let neighbor = shared[(idx + 1u) % 256u]; } ``` ### Atomics [Section titled “Atomics”](#atomics) Atomic operations ```wgsl @group(0) @binding(0) var<storage, read_write> counter: atomic<u32>; @compute @workgroup_size(64) fn atomic_ops(@builtin(global_invocation_id) id: vec3u) { let old = atomicAdd(&counter, 1u); atomicSub(&counter, 1u); atomicMax(&counter, 100u); atomicMin(&counter, 0u); atomicExchange(&counter, 42u); } ``` ## Uniformity Requirements [Section titled “Uniformity Requirements”](#uniformity-requirements) Uniform Control Flow Required Texture sampling with implicit derivatives requires uniform control flow—all threads in a group must take the same path: ```wgsl // ✗ Invalid: non-uniform control flow @fragment fn bad(@location(0) uv: vec2f, @location(1) flag: f32) -> @location(0) vec4f { if (flag > 0.5) { // Non-uniform! return textureSample(tex, samp, uv); // Error } return vec4f(0.0); } // ✓ Valid: sample unconditionally, then select @fragment fn good(@location(0) uv: vec2f, @location(1) flag: f32) -> @location(0) vec4f { let sampled = textureSample(tex, samp, uv); return select(vec4f(0.0), sampled, flag > 0.5); } ``` ## Performance Tips [Section titled “Performance Tips”](#performance-tips) ## Pointers and References [Section titled “Pointers and References”](#pointers-and-references) WGSL supports pointers for passing variables by reference. ### Pointer Syntax [Section titled “Pointer Syntax”](#pointer-syntax) Pointer types ```wgsl // ptr<address_space, type, access_mode> fn increment(p: ptr<function, i32>) { *p = *p + 1; // Dereference with * } fn main() { var x: i32 = 5; increment(&x); // Pass address with & // x is now 6 } ``` ### Address Spaces [Section titled “Address Spaces”](#address-spaces) | Space | Description | Pointer Access | | ----------- | ----------------------- | --------------------------------------- | | `function` | Local variables | `ptr<function, T>` | | `private` | Per-invocation globals | `ptr<private, T>` | | `workgroup` | Shared within workgroup | `ptr<workgroup, T>` | | `storage` | Buffer storage | `ptr<storage, T, read>` or `read_write` | | `uniform` | Uniform buffer | `ptr<uniform, T>` | Storage pointer example ```wgsl fn process(data: ptr<storage, array<f32>, read_write>, idx: u32) { (*data)[idx] *= 2.0; } ``` Pointer Restrictions * Cannot store pointers in variables (no pointer-to-pointer) * Cannot return pointers from functions * Pointers must point to concrete memory locations ## Constants and Overrides [Section titled “Constants and Overrides”](#constants-and-overrides) ### const [Section titled “const”](#const) Compile-time constant expressions: const declarations ```wgsl const PI: f32 = 3.14159265; const TAU: f32 = PI * 2.0; const WORKGROUP_SIZE: u32 = 256u; // Const arrays const VERTICES: array<vec2f, 3> = array<vec2f, 3>( vec2f(0.0, 0.5), vec2f(-0.5, -0.5), vec2f(0.5, -0.5) ); ``` ### override [Section titled “override”](#override) Pipeline-overridable constants (set at pipeline creation): override declarations ```wgsl @id(0) override BLOCK_SIZE: u32 = 64u; @id(1) override THRESHOLD: f32 = 0.5; override USE_FAST_PATH: bool = true; // ID auto-assigned @compute @workgroup_size(BLOCK_SIZE) fn main() { if (USE_FAST_PATH) { // Fast path } } ``` Override at pipeline creation ```javascript const pipeline = device.createComputePipeline({ layout: "auto", compute: { module: shaderModule, entryPoint: "main", constants: { 0: 128, // BLOCK_SIZE = 128 1: 0.75, // THRESHOLD = 0.75 }, }, }); ``` ### const\_assert [Section titled “const\_assert”](#const_assert) Compile-time assertions for catching errors early: const\_assert examples ```wgsl const BUFFER_SIZE: u32 = 1024u; const_assert BUFFER_SIZE >= 256u; // Fails compilation if false const_assert BUFFER_SIZE % 64u == 0u; // Must be multiple of 64 // Validate struct alignment const_assert sizeof(MyStruct) == 64u; ``` ## Resources [Section titled “Resources”](#resources)

# Getting Started

Warning The documentation in this site was generated by an LLM, and may contain incorrect information. Keep this in mind as you read the docs. You may need to verify the information twice. ## About This Guide [Section titled “About This Guide”](#about-this-guide) This documentation provides comprehensive guidance for building GPU-accelerated applications using WebGPU and TypeGPU in TypeScript. It’s designed for intermediate TypeScript developers who want to harness GPU computing power in web applications, covering everything from WebGPU API fundamentals to TypeGPU’s type-safe abstractions for shader development. WebGPU is the modern successor to WebGL, offering compute shader support, improved performance, and alignment with native graphics APIs like Vulkan, Metal, and Direct3D 12. TypeGPU is a type-safe toolkit that simplifies WebGPU development by letting you write shaders in TypeScript with full type checking and automatic WGSL generation. ## Docs for LLMs [Section titled “Docs for LLMs”](#docs-for-llms) This documentation follows the [llms.txt](https://llmstxt.org/) convention, making it accessible to large language models and their applications for code generation, debugging, and learning. You can prompt LLMs with specific questions about WebGPU and TypeGPU concepts, request code snippets, or seek explanations for complex topics covered in this guide. Currently, we have the following root-level files: * [/llms.txt](/llms.txt) — a listing of the available files * [/llms-full.txt ](/llms-full.txt)— complete documentation for WebGPU and TypeGPU * [/llms-small.txt](/llms-small.txt) — compressed documentation for use with smaller context windows

# Installing the CLI

> Use AI to learn easier with a CLI that searches the docs

The Starlight Search CLI (`sls`) lets you search this documentation and retrieve LLM-formatted content directly from your terminal. ## Prerequisites [Section titled “Prerequisites”](#prerequisites) * Node.js 20 or later ## Installation [Section titled “Installation”](#installation) ```sh npm install -g @ratiu5/starlight-search-cli ``` ## Commands [Section titled “Commands”](#commands) ### search [Section titled “search”](#search) Search the documentation for a keyword or phrase. Shorter queries work best. ```sh sls search -d https://webgpu-llm-docs.vercel.app "buffers" ``` **Options:** | Flag | Description | | -------------- | ----------------------------------------- | | `-d, --domain` | The root domain for the documentation API | **Example output:** ```json { "results": [ { "url": "https://webgpu-llm-docs.vercel.app/data-and-buffers/buffers-memory-management", "title": "Buffers & Memory Management", "excerpt": "Learn about WebGPU buffers...", "score": 0.95, "llmsTxt": "https://webgpu-llm-docs.vercel.app/data-and-buffers/buffers-memory-management/_llms-txt" } ], "totalResults": 1 } ``` ### show [Section titled “show”](#show) Fetch and display the LLM-formatted content for a specific documentation page. ```sh sls show "https://webgpu-llm-docs.vercel.app/data-and-buffers/buffers-memory-management/_llms-txt" ``` The URL must contain `_llms-txt` or `llms.txt` to be valid. ## Workflow Example [Section titled “Workflow Example”](#workflow-example) 1. Search for a topic: ```sh sls search -d https://webgpu-llm-docs.vercel.app "compute shaders" ``` 2. Copy the `llmsTxt` URL from a result 3. Fetch the full content: ```sh sls show "https://webgpu-llm-docs.vercel.app/shaders-and-pipelines/compute-pipelines/_llms-txt" ``` 4. Use the output with your preferred LLM ## Configuring AI Agents [Section titled “Configuring AI Agents”](#configuring-ai-agents) Add the following to your `AGENTS.md` or `CLAUDE.md` file to enable AI assistants to use the CLI for WebGPU/TypeGPU documentation lookup. ````markdown ## WebGPU/TypeGPU Documentation This project uses **sls** (Starlight Search) for WebGPU and TypeGPU documentation lookup. ### Quick Reference ```bash sls search -d https://webgpu-llm-docs.vercel.app "<query>" # Search docs sls show "<llmsTxt-url>" # Fetch full content ``` ### Usage Rules - **Always search before implementing** WebGPU/TypeGPU code - **Use 1-3 word queries**: "buffers", "compute pipelines", "bind groups" - **Search concepts, not sentences**: "texture sampling" not "how do I sample a texture" - **Fetch full docs** with `sls show` using the `llmsTxt` URL from results ### Example Session ```bash # Find buffer documentation sls search -d https://webgpu-llm-docs.vercel.app "buffers" # Fetch the full content sls show "https://webgpu-llm-docs.vercel.app/data-and-buffers/buffers-memory-management/_llms-txt" ``` ### Workflow 1. When working with WebGPU or TypeGPU code, search for the relevant concept 2. Extract the `llmsTxt` URL from search results 3. Fetch and read the full documentation with `sls show` 4. Apply the patterns and examples to the current task ### Topic Coverage | Area | Topics | | ------------ | ----------------------------------------------------- | | Fundamentals | Device initialization, adapters, canvas configuration | | Data | Buffers, memory management, data schemas | | Shaders | WGSL, TGSL functions, compute/render pipelines | | Resources | Bind groups, layouts, textures, samplers | | TypeGPU | Type-safe abstractions, slots, derived values | ### When to Search - Before implementing any WebGPU API calls - When encountering GPU-related errors - When optimizing render or compute pipelines - When working with TypeGPU's type system ```` To support both `AGENTS.md` and `CLAUDE.md`, create one file and symlink the other: ```sh # If you created AGENTS.md ln -s AGENTS.md CLAUDE.md # Or if you created CLAUDE.md ln -s CLAUDE.md AGENTS.md ```

# Browser Compatibility

## Overview [Section titled “Overview”](#overview) WebGPU achieved support across all major browsers in 2025, making desktop-class graphics available on the web. Chrome led with version 113 (April 2023), followed by Safari 26 (June 2025) and Firefox 141 (July 2025). ## Browser Support Status [Section titled “Browser Support Status”](#browser-support-status) | Browser | Version | Platforms | Status | Backend | | ------- | ------- | ---------------------------- | ------------- | ------------------------- | | Chrome | 113+ | Windows, macOS, ChromeOS | Stable | Dawn (D3D12/Metal/Vulkan) | | Chrome | 121+ | Android 12+ (Qualcomm/ARM) | Flag required | Dawn (Vulkan) | | Edge | 113+ | Windows, macOS | Stable | Dawn (D3D12/Metal) | | Firefox | 141+ | Windows | Stable | wgpu (D3D12) | | Firefox | 145+ | macOS (Apple Silicon) | Stable | wgpu (Metal) | | Safari | 26+ | macOS, iOS, iPadOS, visionOS | Stable | WebKit (Metal) | ### Platform-Specific Notes [Section titled “Platform-Specific Notes”](#platform-specific-notes) ## Feature Detection [Section titled “Feature Detection”](#feature-detection) ### Basic Support Check [Section titled “Basic Support Check”](#basic-support-check) Check WebGPU availability ```typescript // Step 1: Check API exists (requires HTTPS) if (!navigator.gpu) { throw new Error("WebGPU not supported"); } // Step 2: Request adapter const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { throw new Error("No GPU adapter found"); } // Step 3: Request device const device = await adapter.requestDevice(); ``` Secure Context Required WebGPU requires HTTPS. Pages served over HTTP won’t have access to `navigator.gpu`, even in supporting browsers. ### Optional Feature Detection [Section titled “Optional Feature Detection”](#optional-feature-detection) Check optional features ```typescript // Query supported features const hasTimestamps = adapter.features.has("timestamp-query"); const hasBC = adapter.features.has("texture-compression-bc"); const hasF16 = adapter.features.has("shader-f16"); // Request only available features const availableFeatures = ["timestamp-query", "shader-f16"] .filter(f => adapter.features.has(f)); const device = await adapter.requestDevice({ requiredFeatures: availableFeatures, }); ``` ### Common Optional Features [Section titled “Common Optional Features”](#common-optional-features) | Feature | Description | Typical Platform | | -------------------------- | ----------------------- | ---------------- | | `texture-compression-bc` | BC1-BC7 compression | Desktop | | `texture-compression-etc2` | ETC2/EAC compression | Mobile | | `texture-compression-astc` | ASTC compression | Mobile | | `timestamp-query` | GPU timing queries | Most GPUs | | `shader-f16` | 16-bit float in shaders | Modern GPUs | ### Query Limits [Section titled “Query Limits”](#query-limits) Check device limits ```typescript const limits = adapter.limits; console.log(`Max texture size: ${limits.maxTextureDimension2D}`); console.log(`Max buffer size: ${limits.maxBufferSize}`); if (limits.maxTextureDimension2D < 4096) { console.warn("Limited texture resolution support"); } ``` ## Fallback Strategies [Section titled “Fallback Strategies”](#fallback-strategies) ### WebGL Fallback [Section titled “WebGL Fallback”](#webgl-fallback) Progressive fallback ```typescript async function initGraphics() { // Try WebGPU first if (navigator.gpu) { const adapter = await navigator.gpu.requestAdapter(); if (adapter) { const device = await adapter.requestDevice(); return { type: "webgpu", device }; } } // Fall back to WebGL2 const canvas = document.getElementById("canvas"); const gl = canvas.getContext("webgl2") || canvas.getContext("webgl"); if (gl) { return { type: "webgl", gl }; } throw new Error("No graphics API available"); } ``` ### Library-Based Abstraction [Section titled “Library-Based Abstraction”](#library-based-abstraction) Three.js automatic fallback ```typescript import { WebGPURenderer } from "three/webgpu"; import { WebGLRenderer } from "three"; let renderer; try { renderer = new WebGPURenderer(); await renderer.init(); } catch { renderer = new WebGLRenderer(); } ``` ## Platform Considerations [Section titled “Platform Considerations”](#platform-considerations) ### Desktop Platforms [Section titled “Desktop Platforms”](#desktop-platforms) | Platform | Backend | Notes | | ------------- | ------- | ---------------------------------------------- | | Windows 10/11 | D3D12 | Best tested, widest hardware support | | macOS | Metal | Excellent performance, Intel and Apple Silicon | | Linux | Vulkan | Requires flags, driver quality varies | ### Mobile Platforms [Section titled “Mobile Platforms”](#mobile-platforms) | Platform | Requirements | Notes | | ---------- | ----------------------------- | -------------------------- | | Android | Android 12+, Qualcomm/ARM GPU | Flag required in Chrome | | iOS/iPadOS | iOS 26+ | Safari only, Metal backend | Mobile Constraints Mobile GPUs have stricter power and thermal limits. Test on multiple device tiers and implement quality presets. ## Testing Recommendations [Section titled “Testing Recommendations”](#testing-recommendations) Comprehensive compatibility test ```typescript async function testWebGPU() { if (!navigator.gpu) { return { supported: false, reason: "API unavailable" }; } const adapter = await navigator.gpu.requestAdapter(); if (!adapter) { return { supported: false, reason: "No adapter" }; } const device = await adapter.requestDevice(); return { supported: true, features: Array.from(adapter.features), limits: adapter.limits, }; } ``` ## Resources [Section titled “Resources”](#resources)

# Camera and View Matrices

## Overview [Section titled “Overview”](#overview) Cameras in WebGPU are implemented through matrix transformations. There’s no built-in camera—you construct matrices that transform vertices from world space through camera space into clip space. ## Matrix Types [Section titled “Matrix Types”](#matrix-types) ### View Matrix [Section titled “View Matrix”](#view-matrix) The **view matrix** transforms world coordinates into camera-relative coordinates. It’s the inverse of where the camera is positioned and oriented: View matrix concept ```javascript // Camera at position (10, 5, 20), looking at origin const cameraPosition = [10, 5, 20]; const cameraTarget = [0, 0, 0]; const up = [0, 1, 0]; // View matrix moves everything opposite to camera // So camera effectively sits at origin, looking down -Z ``` ### Projection Matrix [Section titled “Projection Matrix”](#projection-matrix) The **projection matrix** transforms camera-space coordinates into clip space: | Type | Use Case | Characteristics | | ---------------- | ---------------- | ---------------------------- | | **Perspective** | 3D scenes, games | Objects shrink with distance | | **Orthographic** | 2D, CAD, UI | No depth foreshortening | ## wgpu-matrix Library [Section titled “wgpu-matrix Library”](#wgpu-matrix-library) The recommended library for WebGPU matrix math: ```bash npm install wgpu-matrix ``` ### Basic Usage [Section titled “Basic Usage”](#basic-usage) wgpu-matrix basics ```javascript import { mat4, vec3 } from "wgpu-matrix"; // Perspective projection const fov = Math.PI / 4; // 45 degrees const aspect = canvas.width / canvas.height; const near = 0.1; const far = 1000; const projection = mat4.perspective(fov, aspect, near, far); // View matrix using lookAt const eye = [10, 5, 20]; const target = [0, 0, 0]; const up = [0, 1, 0]; const view = mat4.lookAt(eye, target, up); // Combined view-projection const viewProjection = mat4.multiply(projection, view); ``` ### Performance Pattern [Section titled “Performance Pattern”](#performance-pattern) Pre-allocated matrices ```javascript // Allocate once const view = mat4.create(); const projection = mat4.create(); const viewProjection = mat4.create(); function updateCamera() { // Reuse existing arrays (no allocation) mat4.lookAt(eye, target, up, view); mat4.perspective(fov, aspect, near, far, projection); mat4.multiply(projection, view, viewProjection); } ``` ## The lookAt Function [Section titled “The lookAt Function”](#the-lookat-function) `lookAt` constructs a view matrix from camera parameters: lookAt parameters ```javascript mat4.lookAt( eye, // Camera position [x, y, z] target, // Point camera looks at [x, y, z] up // Which direction is up [x, y, z], typically [0, 1, 0] ); ``` ### How lookAt Works [Section titled “How lookAt Works”](#how-lookat-works) 1. Compute forward vector: `normalize(target - eye)` 2. Compute right vector: `cross(forward, up)` 3. Compute true up: `cross(right, forward)` 4. Build rotation from these three perpendicular axes 5. Combine with translation to camera position lookAt internals (simplified) ```javascript function lookAt(eye, target, up) { const forward = vec3.normalize(vec3.subtract(target, eye)); const right = vec3.normalize(vec3.cross(forward, up)); const trueUp = vec3.cross(right, forward); // 4x4 matrix with rotation and translation return mat4.fromValues( right[0], right[1], right[2], 0, trueUp[0], trueUp[1], trueUp[2], 0, -forward[0], -forward[1], -forward[2], 0, -vec3.dot(right, eye), -vec3.dot(trueUp, eye), vec3.dot(forward, eye), 1 ); } ``` ## Projection Matrices [Section titled “Projection Matrices”](#projection-matrices) ### Perspective Projection [Section titled “Perspective Projection”](#perspective-projection) Perspective parameters ```javascript mat4.perspective( fovY, // Vertical field of view in radians aspect, // Width / height ratio near, // Near clipping plane (> 0) far // Far clipping plane (> near) ); ``` | Parameter | Typical Value | Effect | | --------- | ---------------------- | ------------------------------------- | | `fovY` | π/4 (45°) to π/2 (90°) | Wider = more visible, more distortion | | `near` | 0.1 to 1.0 | Too small causes Z-fighting | | `far` | 100 to 10000 | Limits visible distance | Near Plane Precision A near plane too close to 0 wastes depth buffer precision. Use the largest near value your scene allows. Ratio of far/near affects Z-fighting: keep it under 10000 when possible. ### Orthographic Projection [Section titled “Orthographic Projection”](#orthographic-projection) Orthographic projection ```javascript mat4.ortho( left, // Left edge of view right, // Right edge of view bottom, // Bottom edge of view top, // Top edge of view near, // Near clipping plane far // Far clipping plane ); ``` Common orthographic setups ```javascript // 2D UI (pixel coordinates) const uiProjection = mat4.ortho(0, canvas.width, canvas.height, 0, -1, 1); // Centered orthographic const size = 10; const aspect = canvas.width / canvas.height; const ortho = mat4.ortho(-size * aspect, size * aspect, -size, size, 0.1, 100); ``` ## Camera Patterns [Section titled “Camera Patterns”](#camera-patterns) ### Orbit Camera [Section titled “Orbit Camera”](#orbit-camera) Rotates around a target point—ideal for 3D viewers and editors: Orbit camera implementation ```javascript class OrbitCamera { constructor() { this.target = [0, 0, 0]; this.distance = 10; this.azimuth = 0; // Horizontal angle (radians) this.elevation = 0.3; // Vertical angle (radians) } getViewMatrix() { // Clamp elevation to avoid flipping const el = Math.max(-Math.PI / 2 + 0.01, Math.min(Math.PI / 2 - 0.01, this.elevation)); // Convert spherical to Cartesian const x = this.distance * Math.cos(el) * Math.sin(this.azimuth); const y = this.distance * Math.sin(el); const z = this.distance * Math.cos(el) * Math.cos(this.azimuth); const eye = [ this.target[0] + x, this.target[1] + y, this.target[2] + z, ]; return mat4.lookAt(eye, this.target, [0, 1, 0]); } rotate(deltaX, deltaY) { this.azimuth += deltaX * 0.01; this.elevation += deltaY * 0.01; } zoom(delta) { this.distance *= 1 + delta * 0.1; this.distance = Math.max(1, Math.min(100, this.distance)); } } ``` ### First-Person Camera [Section titled “First-Person Camera”](#first-person-camera) Free-moving camera controlled by position and look direction: First-person camera ```javascript class FirstPersonCamera { constructor() { this.position = [0, 1.7, 5]; // Eye height this.yaw = 0; // Horizontal look angle this.pitch = 0; // Vertical look angle } getViewMatrix() { // Calculate look direction from angles const forward = [ Math.cos(this.pitch) * Math.sin(this.yaw), Math.sin(this.pitch), Math.cos(this.pitch) * Math.cos(this.yaw), ]; const target = vec3.add(this.position, forward); return mat4.lookAt(this.position, target, [0, 1, 0]); } look(deltaX, deltaY) { this.yaw += deltaX * 0.002; this.pitch -= deltaY * 0.002; // Clamp pitch to avoid gimbal lock this.pitch = Math.max(-Math.PI / 2 + 0.01, Math.min(Math.PI / 2 - 0.01, this.pitch)); } move(forward, right) { // Move in look direction const dir = [ Math.sin(this.yaw), 0, Math.cos(this.yaw), ]; this.position[0] += dir[0] * forward + dir[2] * right; this.position[2] += dir[2] * forward - dir[0] * right; } } ``` ### Turntable Camera [Section titled “Turntable Camera”](#turntable-camera) Like orbit, but rotation is around a fixed up axis: Turntable (modeling software style) ```javascript class TurntableCamera { constructor() { this.target = [0, 0, 0]; this.distance = 10; this.theta = 0; // Horizontal rotation this.phi = Math.PI / 4; // Vertical angle from top } getViewMatrix() { const x = this.distance * Math.sin(this.phi) * Math.cos(this.theta); const y = this.distance * Math.cos(this.phi); const z = this.distance * Math.sin(this.phi) * Math.sin(this.theta); const eye = [ this.target[0] + x, this.target[1] + y, this.target[2] + z, ]; return mat4.lookAt(eye, this.target, [0, 1, 0]); } } ``` ## Mouse and Touch Controls [Section titled “Mouse and Touch Controls”](#mouse-and-touch-controls) Camera input handling ```javascript let isDragging = false; let lastX = 0, lastY = 0; canvas.addEventListener("mousedown", (e) => { isDragging = true; lastX = e.clientX; lastY = e.clientY; }); canvas.addEventListener("mousemove", (e) => { if (!isDragging) return; const deltaX = e.clientX - lastX; const deltaY = e.clientY - lastY; lastX = e.clientX; lastY = e.clientY; camera.rotate(deltaX, deltaY); }); canvas.addEventListener("mouseup", () => isDragging = false); canvas.addEventListener("mouseleave", () => isDragging = false); // Zoom with scroll wheel canvas.addEventListener("wheel", (e) => { e.preventDefault(); camera.zoom(e.deltaY > 0 ? 1 : -1); }); ``` ## Uploading to GPU [Section titled “Uploading to GPU”](#uploading-to-gpu) Camera uniform buffer ```javascript import { mat4 } from "wgpu-matrix"; // Create uniform buffer for camera matrices const cameraBuffer = device.createBuffer({ size: 64 * 3, // 3 mat4x4 matrices usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); function updateCamera() { const view = camera.getViewMatrix(); const projection = mat4.perspective(fov, aspect, near, far); const viewProjection = mat4.multiply(projection, view); // Upload all matrices device.queue.writeBuffer(cameraBuffer, 0, view); device.queue.writeBuffer(cameraBuffer, 64, projection); device.queue.writeBuffer(cameraBuffer, 128, viewProjection); } ``` ### WGSL Usage [Section titled “WGSL Usage”](#wgsl-usage) Camera in shader ```wgsl struct Camera { view: mat4x4f, projection: mat4x4f, viewProjection: mat4x4f, } @group(0) @binding(0) var<uniform> camera: Camera; @vertex fn main(@location(0) position: vec3f) -> @builtin(position) vec4f { return camera.viewProjection * vec4f(position, 1.0); } ``` ## TypeGPU Camera Buffer [Section titled “TypeGPU Camera Buffer”](#typegpu-camera-buffer) TypeGPU camera setup ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; import { mat4 } from "wgpu-matrix"; const CameraSchema = d.struct({ view: d.mat4x4f, projection: d.mat4x4f, viewProjection: d.mat4x4f, }); const cameraBuffer = root .createBuffer(CameraSchema) .$usage("uniform"); function updateCamera() { const view = camera.getViewMatrix(); const projection = mat4.perspective(fov, aspect, 0.1, 1000); const viewProjection = mat4.multiply(projection, view); cameraBuffer.write({ view: view as Float32Array, projection: projection as Float32Array, viewProjection: viewProjection as Float32Array, }); } ``` ## Common Issues [Section titled “Common Issues”](#common-issues) Gimbal Lock When pitch approaches ±90°, the camera can flip unexpectedly. Clamp pitch angles: ```javascript this.pitch = Math.max(-Math.PI/2 + 0.01, Math.min(Math.PI/2 - 0.01, this.pitch)); ``` For unrestricted rotation, use quaternions instead of Euler angles. Z-Fighting Objects at similar depths flicker. Solutions: * Increase near plane distance * Decrease far/near ratio * Use logarithmic depth buffer for large scenes Coordinate Handedness WebGPU uses a left-handed coordinate system in NDC (normalized device coordinates) with +Z pointing into the screen. Ensure your camera math matches. ## Resources [Section titled “Resources”](#resources)

# Debugging GPU Code

## Overview [Section titled “Overview”](#overview) GPU debugging differs fundamentally from CPU debugging due to massive parallelism, separate execution contexts, and limited introspection capabilities. This guide covers validation layers, TypeGPU debugging features, visual debugging techniques, and browser developer tools. ## Validation Layers [Section titled “Validation Layers”](#validation-layers) WebGPU validates all operations and reports detailed errors through the browser console: Error scope pattern ```typescript device.pushErrorScope("validation"); const buffer = device.createBuffer({ size: 256, usage: GPUBufferUsage.UNIFORM, // Missing COPY_DST }); device.queue.writeBuffer(buffer, 0, data); // Will fail const error = await device.popErrorScope(); if (error) { console.error("Validation error:", error.message); } ``` ### Common Validation Errors [Section titled “Common Validation Errors”](#common-validation-errors) | Error | Cause | Fix | | ------------------- | ---------------------------------------- | ------------------------------------------- | | Missing usage flag | Buffer created without required usage | Add `GPUBufferUsage.COPY_DST` for writes | | Binding mismatch | Shader bindings don’t match bind group | Ensure `@group`/`@binding` match JavaScript | | Format incompatible | Texture view format differs from texture | Use matching formats | | Size mismatch | Buffer too small for shader expectations | Check minimum binding sizes | ## TypeGPU Console.log [Section titled “TypeGPU Console.log”](#typegpu-consolelog) TypeGPU enables `console.log` directly in GPU shaders: GPU console.log ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; const processData = tgpu.fn([d.f32], d.f32).does((input) => { console.log("Processing input:", input); const result = input * 2.0; console.log("Computed result:", result); return result; }); ``` Console.log Limitations * **Performance overhead**: Buffer writes consume GPU time * **Fixed buffer size**: Excessive logging truncates output * **Deferred output**: Messages appear after execution completes * **Parallel flood**: Millions of invocations can overwhelm the log Use conditional logging to limit output volume: ```typescript if (pixelIndex < 10) { console.log("Processing pixel:", pixelIndex); } ``` ## CPU Simulation [Section titled “CPU Simulation”](#cpu-simulation) TypeGPU’s `tgpu.simulate()` runs shader code on CPU with full debugging: CPU simulation with debugging ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; const computeGradient = tgpu.fn([d.f32, d.f32], d.f32).does((x, y) => { const dx = x * 2.0; const dy = y * 3.0; return Math.sqrt(dx * dx + dy * dy); }); // Simulate on CPU - set breakpoints, inspect variables const result = tgpu["~unstable"].simulate(() => { return computeGradient(5.0, 3.0); }); console.log("Simulated result:", result); ``` ## Visual Debugging [Section titled “Visual Debugging”](#visual-debugging) Encode shader values as colors to visualize program state: Visualize normals as RGB ```wgsl @fragment fn debugNormals(@location(0) normal: vec3<f32>) -> @location(0) vec4<f32> { // Transform [-1,1] to [0,1] range let r = normal.x * 0.5 + 0.5; let g = normal.y * 0.5 + 0.5; let b = normal.z * 0.5 + 0.5; return vec4<f32>(r, g, b, 1.0); } ``` ### Common Visualization Techniques [Section titled “Common Visualization Techniques”](#common-visualization-techniques) ## Shader Compilation Errors [Section titled “Shader Compilation Errors”](#shader-compilation-errors) ### Error Types [Section titled “Error Types”](#error-types) | Type | Example | Fix | | -------- | ---------------------------------------- | --------------------------- | | Parser | Missing semicolon, invalid token | Check WGSL syntax | | Type | Returning `f32` from `i32` function | Use explicit casts | | Semantic | Duplicate binding, invalid address space | Check resource declarations | ### Debugging Strategy [Section titled “Debugging Strategy”](#debugging-strategy) 1. **Isolate**: Comment out code sections until error disappears 2. **Minimize**: Create smallest shader that reproduces the error 3. **Verify types**: Add explicit type annotations 4. **Check bindings**: Ensure `@group`/`@binding` are unique and match JavaScript ## Runtime Errors [Section titled “Runtime Errors”](#runtime-errors) ### Device Lost [Section titled “Device Lost”](#device-lost) Handle device loss ```typescript device.lost.then((info) => { console.error("Device lost:", info.reason, info.message); if (info.reason !== "destroyed") { // Attempt recovery reinitializeWebGPU(); } }); ``` **Common causes**: Driver crash, GPU timeout, resource exhaustion ### Out of Memory [Section titled “Out of Memory”](#out-of-memory) Handle OOM errors ```typescript device.pushErrorScope("out-of-memory"); const largeTexture = device.createTexture({ size: { width: 8192, height: 8192 }, format: "rgba16float", usage: GPUTextureUsage.STORAGE_BINDING, }); const error = await device.popErrorScope(); if (error) { // Use smaller texture createFallbackTexture(); } ``` ## Browser Developer Tools [Section titled “Browser Developer Tools”](#browser-developer-tools) ### WebGPU Inspector [Section titled “WebGPU Inspector”](#webgpu-inspector) Available for Chrome, Firefox, and Safari: * **Inspection Mode**: View live GPU objects * **Capture Mode**: Record GPU commands per frame * **Shader Editing**: Edit and reload shaders live * **Performance**: Plot frame times and object counts ### Platform-Specific Tools [Section titled “Platform-Specific Tools”](#platform-specific-tools) | Browser | Tools | | ------- | ---------------------------------------- | | Chrome | DevTools Performance tab, `chrome://gpu` | | Firefox | Graphics inspector, about:support | | Safari | Web Inspector, Metal frame capture | ## Debugging Workflow [Section titled “Debugging Workflow”](#debugging-workflow) Debug vs Release pattern ```typescript const DEBUG = import.meta.env.DEV; if (DEBUG) { device.pushErrorScope("validation"); // Enable detailed logging } // Create resources with labels const buffer = device.createBuffer({ label: "Particle Position Buffer", // Shows in error messages size: particleCount * 16, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }); ``` ### Incremental Development [Section titled “Incremental Development”](#incremental-development) 1. Start with minimal working shader 2. Add one feature at a time 3. Test after each addition 4. Use simulation for algorithm verification 5. Validate on GPU with visual debugging 6. Optimize only after confirming correctness Asynchronous Error Handling GPU errors may appear later than the causative code. Always use error scopes: ```typescript device.pushErrorScope("validation"); // ... operations that might fail ... const error = await device.popErrorScope(); ``` ## Resources [Section titled “Resources”](#resources)

# Timestamp Queries and Profiling

## Overview [Section titled “Overview”](#overview) GPU performance profiling requires specialized mechanisms because GPU operations execute asynchronously on separate hardware. WebGPU provides timestamp queries to measure GPU execution time with high precision. CPU Timing Won’t Work ```typescript const startTime = performance.now(); device.queue.submit([commandEncoder.finish()]); const endTime = performance.now(); // WRONG! ``` This only measures command submission time, not GPU execution. The GPU may not even start executing until milliseconds later. ## The timestamp-query Feature [Section titled “The timestamp-query Feature”](#the-timestamp-query-feature) Timestamp queries are an optional feature requiring explicit request: Request timestamp-query feature ```typescript const adapter = await navigator.gpu.requestAdapter(); if (!adapter.features.has("timestamp-query")) { console.warn("Timestamp queries not supported"); return; } const device = await adapter.requestDevice({ requiredFeatures: ["timestamp-query"], }); ``` Security Consideration Timestamp queries are optional because high-precision timing can potentially be exploited for side-channel attacks. Not all hardware supports them. ## Query Sets [Section titled “Query Sets”](#query-sets) Query sets are GPU-managed objects that store timestamp values: Create query set ```typescript const querySet = device.createQuerySet({ type: "timestamp", count: 2, // Start and end timestamps label: "render-pass-timing", }); ``` ### Writing Timestamps [Section titled “Writing Timestamps”](#writing-timestamps) Timestamps are written via the `timestampWrites` parameter when beginning passes: Timestamp writes in compute pass ```typescript const passEncoder = commandEncoder.beginComputePass({ timestampWrites: { querySet: querySet, beginningOfPassWriteIndex: 0, endOfPassWriteIndex: 1, }, }); passEncoder.setPipeline(computePipeline); passEncoder.dispatchWorkgroups(64, 64); passEncoder.end(); ``` Timestamp writes in render pass ```typescript const passEncoder = commandEncoder.beginRenderPass({ colorAttachments: [{ /* ... */ }], timestampWrites: { querySet: querySet, beginningOfPassWriteIndex: 0, endOfPassWriteIndex: 1, }, }); ``` ## Reading Query Results [Section titled “Reading Query Results”](#reading-query-results) Query results must be resolved to a buffer, then mapped for CPU reading: Resolve and read timestamps ```typescript // Create buffers for resolution const queryBuffer = device.createBuffer({ size: 2 * 8, // 2 timestamps × 8 bytes (64-bit) usage: GPUBufferUsage.QUERY_RESOLVE | GPUBufferUsage.COPY_SRC, }); const resultBuffer = device.createBuffer({ size: 2 * 8, usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST, }); // After recording commands, resolve queries commandEncoder.resolveQuerySet(querySet, 0, 2, queryBuffer, 0); commandEncoder.copyBufferToBuffer(queryBuffer, 0, resultBuffer, 0, 16); // Submit and wait device.queue.submit([commandEncoder.finish()]); await device.queue.onSubmittedWorkDone(); // Read results await resultBuffer.mapAsync(GPUMapMode.READ); const timestamps = new BigInt64Array(resultBuffer.getMappedRange()); const durationNs = timestamps[1] - timestamps[0]; const durationMs = Number(durationNs) / 1_000_000; console.log(`GPU execution: ${durationMs.toFixed(3)}ms`); resultBuffer.unmap(); ``` ## TypeGPU Query API [Section titled “TypeGPU Query API”](#typegpu-query-api) TypeGPU provides higher-level abstractions for profiling: ### Performance Callbacks [Section titled “Performance Callbacks”](#performance-callbacks) Automatic performance measurement ```typescript const pipeline = root["~unstable"] .withCompute(computeShader) .createPipeline() .withPerformanceCallback((start, end) => { const durationMs = Number(end - start) / 1_000_000; console.log(`Execution: ${durationMs.toFixed(3)}ms`); }); pipeline.execute(); ``` ### Manual Query Sets [Section titled “Manual Query Sets”](#manual-query-sets) TypeGPU query set ```typescript const querySet = root.createQuerySet("timestamp", 2); const pipeline = root["~unstable"] .withCompute(computeShader) .createPipeline() .withTimestampWrites({ querySet: querySet, beginningOfPassWriteIndex: 0, endOfPassWriteIndex: 1, }); pipeline.execute(); if (querySet.available) { querySet.resolve(); const timestamps = await querySet.read(); const durationNs = Number(timestamps[1] - timestamps[0]); console.log(`Execution: ${durationNs / 1_000_000}ms`); } ``` ## Profiling Patterns [Section titled “Profiling Patterns”](#profiling-patterns) ### Frame Time Breakdown [Section titled “Frame Time Breakdown”](#frame-time-breakdown) Profile multiple passes ```typescript const querySet = root.createQuerySet("timestamp", 10); // Shadow pass: indices 0-1 shadowPipeline.withTimestampWrites({ querySet, beginningOfPassWriteIndex: 0, endOfPassWriteIndex: 1, }); // Geometry pass: indices 2-3 geometryPipeline.withTimestampWrites({ querySet, beginningOfPassWriteIndex: 2, endOfPassWriteIndex: 3, }); // Lighting pass: indices 4-5 lightingPipeline.withTimestampWrites({ querySet, beginningOfPassWriteIndex: 4, endOfPassWriteIndex: 5, }); // Execute all passes, then read if (querySet.available) { querySet.resolve(); const times = await querySet.read(); console.log(`Shadow: ${toMs(times[1] - times[0])}ms`); console.log(`Geometry: ${toMs(times[3] - times[2])}ms`); console.log(`Lighting: ${toMs(times[5] - times[4])}ms`); } function toMs(ns) { return (Number(ns) / 1_000_000).toFixed(2); } ``` ### Statistical Analysis [Section titled “Statistical Analysis”](#statistical-analysis) Collect performance statistics ```typescript class PerformanceTracker { private samples: number[] = []; private maxSamples = 100; addSample(durationMs: number) { this.samples.push(durationMs); if (this.samples.length > this.maxSamples) { this.samples.shift(); } } getStats() { if (this.samples.length === 0) return null; const sorted = [...this.samples].sort((a, b) => a - b); const sum = sorted.reduce((a, b) => a + b, 0); return { min: sorted[0], max: sorted[sorted.length - 1], mean: sum / sorted.length, p95: sorted[Math.floor(sorted.length * 0.95)], p99: sorted[Math.floor(sorted.length * 0.99)], }; } } ``` ## Occlusion Queries [Section titled “Occlusion Queries”](#occlusion-queries) Occlusion queries count how many fragment samples pass depth/stencil tests, enabling visibility-based optimizations. ### Setup [Section titled “Setup”](#setup) Create occlusion query set ```javascript const occlusionQuerySet = device.createQuerySet({ type: "occlusion", count: 32, // Number of queries available label: "occlusion-queries", }); ``` ### Usage in Render Pass [Section titled “Usage in Render Pass”](#usage-in-render-pass) Occlusion query in render pass ```javascript const renderPassDescriptor = { colorAttachments: [{ /* ... */ }], occlusionQuerySet: occlusionQuerySet, // Attach query set }; const pass = commandEncoder.beginRenderPass(renderPassDescriptor); pass.setPipeline(pipeline); // Query 0: Check if bounding box is visible pass.beginOcclusionQuery(0); pass.setVertexBuffer(0, boundingBoxBuffer); pass.draw(36); // Draw bounding box pass.endOcclusionQuery(); // Query 1: Check another object pass.beginOcclusionQuery(1); pass.setVertexBuffer(0, anotherBoundingBox); pass.draw(36); pass.endOcclusionQuery(); pass.end(); ``` ### Reading Results [Section titled “Reading Results”](#reading-results) Read occlusion query results ```javascript // Resolve queries to buffer const queryBuffer = device.createBuffer({ size: 32 * 8, // 32 queries × 8 bytes (u64) usage: GPUBufferUsage.QUERY_RESOLVE | GPUBufferUsage.COPY_SRC, }); commandEncoder.resolveQuerySet(occlusionQuerySet, 0, 32, queryBuffer, 0); // Copy to mappable buffer and read // Results are sample counts (0 = fully occluded) await resultBuffer.mapAsync(GPUMapMode.READ); const results = new BigUint64Array(resultBuffer.getMappedRange()); for (let i = 0; i < 32; i++) { if (results[i] === 0n) { console.log(`Object ${i} is fully occluded`); } } resultBuffer.unmap(); ``` ### Visibility Culling Pattern [Section titled “Visibility Culling Pattern”](#visibility-culling-pattern) Two-pass occlusion culling ```javascript // Pass 1: Render bounding boxes with occlusion queries const occlusionPass = encoder.beginRenderPass({ colorAttachments: [{ loadOp: "load", storeOp: "store", view }], depthStencilAttachment: { /* depth buffer from previous frame */ }, occlusionQuerySet, }); for (let i = 0; i < objects.length; i++) { occlusionPass.beginOcclusionQuery(i); drawBoundingBox(occlusionPass, objects[i]); occlusionPass.endOcclusionQuery(); } occlusionPass.end(); // Resolve and read queries encoder.resolveQuerySet(occlusionQuerySet, 0, objects.length, queryBuffer, 0); device.queue.submit([encoder.finish()]); // Wait for results await device.queue.onSubmittedWorkDone(); await resultBuffer.mapAsync(GPUMapMode.READ); const visibility = new BigUint64Array(resultBuffer.getMappedRange()); // Pass 2: Only render visible objects const renderEncoder = device.createCommandEncoder(); const renderPass = renderEncoder.beginRenderPass(renderDescriptor); for (let i = 0; i < objects.length; i++) { if (visibility[i] > 0n) { drawFullObject(renderPass, objects[i]); } } renderPass.end(); device.queue.submit([renderEncoder.finish()]); resultBuffer.unmap(); ``` Latency Considerations Occlusion query results aren’t available until the GPU finishes. Reading results introduces CPU-GPU synchronization. For real-time rendering, use results from the previous frame to avoid stalls. ### Use Cases [Section titled “Use Cases”](#use-cases) | Application | Technique | | -------------------- | -------------------------------------------- | | **LOD selection** | Use sample count to estimate screen coverage | | **Culling** | Skip objects with 0 visible samples | | **Portal rendering** | Only render rooms visible through portals | | **Lens flare** | Fade based on sun visibility | ## Browser Developer Tools [Section titled “Browser Developer Tools”](#browser-developer-tools) ### Chrome GPU Profiling [Section titled “Chrome GPU Profiling”](#chrome-gpu-profiling) * **`chrome://gpu`**: View GPU capabilities and driver info * **DevTools Performance tab**: GPU process activity and frame timeline ### Firefox Graphics Tools [Section titled “Firefox Graphics Tools”](#firefox-graphics-tools) * **`about:support`**: Graphics section shows GPU and feature status * **Performance profiler**: GPU timing in development builds ## Complete Example [Section titled “Complete Example”](#complete-example) Full profiling setup ```typescript import tgpu from "typegpu"; const root = await tgpu.init(); class GPUProfiler { private frameTimes: number[] = []; recordFrame(durationMs: number) { this.frameTimes.push(durationMs); if (this.frameTimes.length > 60) this.frameTimes.shift(); } getAvgFPS() { if (this.frameTimes.length === 0) return 0; const avg = this.frameTimes.reduce((a, b) => a + b) / this.frameTimes.length; return 1000 / avg; } } const profiler = new GPUProfiler(); const pipeline = root["~unstable"] .withCompute(computeShader) .createPipeline() .withPerformanceCallback((start, end) => { profiler.recordFrame(Number(end - start) / 1_000_000); }); function renderLoop() { pipeline.execute(); if (frameCount % 60 === 0) { console.log(`FPS: ${profiler.getAvgFPS().toFixed(1)}`); } requestAnimationFrame(renderLoop); } renderLoop(); ``` Production Considerations * **Development**: Enable comprehensive profiling * **Production**: Disable or use selective profiling * **User-triggered**: Provide opt-in profiling via keyboard shortcut ```typescript const ENABLE_PROFILING = import.meta.env.DEV || localStorage.getItem("enable-gpu-profiling"); ``` ## Resources [Section titled “Resources”](#resources)

# WebGPU vs WebGL Migration Guide

## Overview [Section titled “Overview”](#overview) WebGPU represents a fundamental shift from WebGL’s OpenGL-based design toward a modern, low-level graphics API inspired by Vulkan, Metal, and Direct3D 12. It offers explicit control over GPU resources, better performance through reduced CPU overhead, and compute shader support. ## Architectural Differences [Section titled “Architectural Differences”](#architectural-differences) ### State Management [Section titled “State Management”](#state-management) **WebGL** uses a global state machine where settings persist until changed: WebGL global state ```javascript gl.bindBuffer(gl.ARRAY_BUFFER, buffer); gl.enableVertexAttribArray(0); // State remains for all subsequent draws ``` **WebGPU** uses immutable pipeline objects created upfront: WebGPU stateless pipeline ```javascript const pipeline = device.createRenderPipeline({ layout: "auto", vertex: { module: shaderModule, entryPoint: "vertexMain", buffers: [...] }, fragment: { module: shaderModule, entryPoint: "fragmentMain", targets: [...] }, primitive: { topology: "triangle-list" }, }); // Switch pipelines explicitly pass.setPipeline(pipeline); ``` ### Command Model [Section titled “Command Model”](#command-model) **WebGL** executes commands immediately: ```javascript gl.drawArrays(gl.TRIANGLES, 0, 3); // Executes immediately ``` **WebGPU** records commands into buffers, then submits: WebGPU command buffer pattern ```javascript const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass(descriptor); pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertexBuffer); pass.draw(3); pass.end(); device.queue.submit([encoder.finish()]); // Actual execution ``` ## Shading Languages [Section titled “Shading Languages”](#shading-languages) ### GLSL to WGSL [Section titled “GLSL to WGSL”](#glsl-to-wgsl) WebGL GLSL ```glsl attribute vec3 position; uniform mat4 modelViewProjection; varying vec2 vTexCoord; void main() { gl_Position = modelViewProjection * vec4(position, 1.0); } ``` WebGPU WGSL ```wgsl struct VertexOutput { @builtin(position) position: vec4<f32>, @location(0) texCoord: vec2<f32>, } @group(0) @binding(0) var<uniform> mvp: mat4x4<f32>; @vertex fn vertexMain(@location(0) position: vec3<f32>) -> VertexOutput { var output: VertexOutput; output.position = mvp * vec4<f32>(position, 1.0); return output; } ``` ### Key Translation Points [Section titled “Key Translation Points”](#key-translation-points) | GLSL | WGSL | | ------------------- | ----------------------------------------- | | `attribute` | `@location(n)` input | | `varying` | `@location(n)` output | | `uniform sampler2D` | `texture_2d` + `sampler` | | `texture2D()` | `textureSample()` | | `gl_Position` | `@builtin(position)` | | `gl_FragColor` | Return with `@location(0)` | | `vec4` | `vec4<f32>` | | `main()` | Named function with `@vertex`/`@fragment` | ## Coordinate System Changes [Section titled “Coordinate System Changes”](#coordinate-system-changes) ### Depth Range [Section titled “Depth Range”](#depth-range) Critical Migration Issue WebGPU uses 0 to 1 depth range (not -1 to 1). Using WebGL projection matrices causes incorrect clipping. WebGPU perspective matrix (0-1 depth) ```javascript function perspectiveZO(fov, aspect, near, far) { const f = 1.0 / Math.tan(fov / 2); const rangeInv = 1.0 / (near - far); return [ f / aspect, 0, 0, 0, 0, f, 0, 0, 0, 0, far * rangeInv, -1, 0, 0, near * far * rangeInv, 0, ]; } ``` ### Framebuffer Origin [Section titled “Framebuffer Origin”](#framebuffer-origin) | Aspect | WebGL | WebGPU | | ----------- | ----------- | -------- | | Origin | Bottom-left | Top-left | | Y Direction | Up | Down | **Texture coordinate fix:** Flip Y in shader ```wgsl let flippedUV = vec2<f32>(texCoord.x, 1.0 - texCoord.y); let color = textureSample(tex, samp, flippedUV); ``` ## Compute Shaders [Section titled “Compute Shaders”](#compute-shaders) WebGPU’s major feature addition over WebGL: WebGPU compute shader ```wgsl @group(0) @binding(0) var<storage, read> input: array<f32>; @group(0) @binding(1) var<storage, read_write> output: array<f32>; @compute @workgroup_size(64) fn main(@builtin(global_invocation_id) id: vec3<u32>) { if (id.x < arrayLength(&input)) { output[id.x] = input[id.x] * 2.0; } } ``` ## Common Migration Issues [Section titled “Common Migration Issues”](#common-migration-issues) ## Side-by-Side Comparison [Section titled “Side-by-Side Comparison”](#side-by-side-comparison) ### WebGL Triangle [Section titled “WebGL Triangle”](#webgl-triangle) WebGL triangle ```javascript const gl = canvas.getContext("webgl"); // Compile shaders const vs = gl.createShader(gl.VERTEX_SHADER); gl.shaderSource(vs, vertexSource); gl.compileShader(vs); const fs = gl.createShader(gl.FRAGMENT_SHADER); gl.shaderSource(fs, fragmentSource); gl.compileShader(fs); // Link program const program = gl.createProgram(); gl.attachShader(program, vs); gl.attachShader(program, fs); gl.linkProgram(program); gl.useProgram(program); // Create buffer const buffer = gl.createBuffer(); gl.bindBuffer(gl.ARRAY_BUFFER, buffer); gl.bufferData(gl.ARRAY_BUFFER, vertices, gl.STATIC_DRAW); // Configure attributes const loc = gl.getAttribLocation(program, "position"); gl.enableVertexAttribArray(loc); gl.vertexAttribPointer(loc, 2, gl.FLOAT, false, 0, 0); // Draw gl.drawArrays(gl.TRIANGLES, 0, 3); ``` ### WebGPU Triangle [Section titled “WebGPU Triangle”](#webgpu-triangle) WebGPU triangle ```javascript const adapter = await navigator.gpu.requestAdapter(); const device = await adapter.requestDevice(); const context = canvas.getContext("webgpu"); context.configure({ device, format: navigator.gpu.getPreferredCanvasFormat() }); const pipeline = device.createRenderPipeline({ layout: "auto", vertex: { module: device.createShaderModule({ code: shaderCode }), entryPoint: "vertexMain", buffers: [{ arrayStride: 8, attributes: [{ shaderLocation: 0, offset: 0, format: "float32x2" }], }], }, fragment: { module: device.createShaderModule({ code: shaderCode }), entryPoint: "fragmentMain", targets: [{ format: navigator.gpu.getPreferredCanvasFormat() }], }, }); const vertexBuffer = device.createBuffer({ size: vertices.byteLength, usage: GPUBufferUsage.VERTEX | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer(vertexBuffer, 0, vertices); const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: "clear", storeOp: "store", }], }); pass.setPipeline(pipeline); pass.setVertexBuffer(0, vertexBuffer); pass.draw(3); pass.end(); device.queue.submit([encoder.finish()]); ``` ## When to Migrate [Section titled “When to Migrate”](#when-to-migrate) ### Good Candidates [Section titled “Good Candidates”](#good-candidates) | Scenario | Reason | | ----------------------------- | ---------------------------------- | | Compute-heavy applications | GPGPU not possible in WebGL | | Performance-critical projects | Reduced CPU overhead | | New projects | Better architecture, future-proof | | Complex rendering | Explicit control, fewer state bugs | ### Stay with WebGL [Section titled “Stay with WebGL”](#stay-with-webgl) | Scenario | Reason | | ----------------------------- | ------------------------------------ | | Maximum browser compatibility | WebGL works on older devices | | Simple 2D graphics | WebGL is simpler for basic use | | Stable existing codebases | Migration cost may outweigh benefits | | Limited development resources | WebGPU has steeper learning curve | ## Migration Strategies [Section titled “Migration Strategies”](#migration-strategies) ### Gradual Migration [Section titled “Gradual Migration”](#gradual-migration) Feature detection with fallback ```javascript if (navigator.gpu) { const adapter = await navigator.gpu.requestAdapter(); if (adapter) { return initWebGPURenderer(await adapter.requestDevice()); } } return initWebGLRenderer(); ``` ### Feature-by-Feature [Section titled “Feature-by-Feature”](#feature-by-feature) 1. Start with compute shaders (new functionality) 2. Move particle systems (performance benefit) 3. Convert main rendering pipeline (largest impact) 4. Migrate post-processing effects 5. Update UI rendering last Alignment Requirements WebGPU has strict buffer alignment (typically 256 bytes for uniforms). Calculate sizes carefully: ```javascript const alignedSize = Math.ceil(dataSize / 256) * 256; ``` ## Resources [Section titled “Resources”](#resources)

# Bind Groups and Layouts

## Overview [Section titled “Overview”](#overview) Bind groups and layouts form the cornerstone of WebGPU’s resource binding model, providing a structured way to connect GPU resources to shader programs. ## Key Concepts [Section titled “Key Concepts”](#key-concepts) ### Bind Group [Section titled “Bind Group”](#bind-group) A collection of GPU resources (buffers, textures, samplers) bundled as a single unit: Conceptual bind group ```javascript // Instead of binding resources individually: bindResource(0, uniformBuffer); bindResource(1, storageBuffer); bindResource(2, texture); // Bind them together: passEncoder.setBindGroup(0, myBindGroup); ``` ### Bind Group Layout [Section titled “Bind Group Layout”](#bind-group-layout) Defines the structure bind groups must follow: * Number of bindings * Resource type at each binding (buffer, texture, sampler) * Shader stages that can access each resource * Buffer types, texture formats, access modes ### WGSL Binding Indices [Section titled “WGSL Binding Indices”](#wgsl-binding-indices) Resources are referenced using two attributes: WGSL resource declarations ```wgsl @group(0) @binding(0) var<uniform> camera: Camera; @group(0) @binding(1) var<storage, read> positions: array<vec3f>; @group(1) @binding(0) var diffuseTexture: texture_2d<f32>; @group(1) @binding(1) var textureSampler: sampler; ``` ### Shader Stage Visibility [Section titled “Shader Stage Visibility”](#shader-stage-visibility) | Stage | Flag | Use Case | | -------- | ------------------------- | ---------------------------- | | Vertex | `GPUShaderStage.VERTEX` | Transformations, vertex data | | Fragment | `GPUShaderStage.FRAGMENT` | Textures, lighting | | Compute | `GPUShaderStage.COMPUTE` | General computation | Combine with bitwise OR: `GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT` ## Creating Bind Group Layouts [Section titled “Creating Bind Group Layouts”](#creating-bind-group-layouts) Compute bind group layout ```javascript const bindGroupLayout = device.createBindGroupLayout({ label: "Compute Bind Group Layout", entries: [ { binding: 0, visibility: GPUShaderStage.COMPUTE, buffer: { type: "uniform", minBindingSize: 16, }, }, { binding: 1, visibility: GPUShaderStage.COMPUTE, buffer: { type: "read-only-storage", }, }, { binding: 2, visibility: GPUShaderStage.COMPUTE, buffer: { type: "storage", // Read-write }, }, ], }); ``` ### Entry Types [Section titled “Entry Types”](#entry-types) ## Creating Bind Groups [Section titled “Creating Bind Groups”](#creating-bind-groups) Create bind group from layout ```javascript const bindGroup = device.createBindGroup({ label: "Compute Bind Group", layout: bindGroupLayout, entries: [ { binding: 0, resource: { buffer: uniformBuffer, offset: 0, size: 16, }, }, { binding: 1, resource: { buffer: inputBuffer }, }, { binding: 2, resource: { buffer: outputBuffer }, }, ], }); ``` For textures and samplers: Texture and sampler bind group ```javascript const renderBindGroup = device.createBindGroup({ label: "Render Bind Group", layout: renderBindGroupLayout, entries: [ { binding: 0, resource: sampler, // GPUSampler object }, { binding: 1, resource: texture.createView(), // GPUTextureView }, ], }); ``` Validation Requirements Entries must exactly match the layout: * Same binding numbers * Correct resource types * All bindings provided ## Pipeline Layouts [Section titled “Pipeline Layouts”](#pipeline-layouts) Combines multiple bind group layouts for a complete pipeline interface: Create pipeline layout ```javascript const pipelineLayout = device.createPipelineLayout({ label: "Main Pipeline Layout", bindGroupLayouts: [ bindGroupLayout0, // @group(0) bindGroupLayout1, // @group(1) bindGroupLayout2, // @group(2) ], }); const computePipeline = device.createComputePipeline({ label: "Compute Pipeline", layout: pipelineLayout, compute: { module: shaderModule, entryPoint: "main", }, }); ``` ### Automatic Layout [Section titled “Automatic Layout”](#automatic-layout) Use `layout: "auto"` to infer layout from shader: Auto layout pipeline ```javascript const pipeline = device.createComputePipeline({ layout: "auto", compute: { module: shaderModule, entryPoint: "main", }, }); // Retrieve inferred layout for bind group creation const bindGroupLayout = pipeline.getBindGroupLayout(0); ``` ## TypeGPU Bind Groups [Section titled “TypeGPU Bind Groups”](#typegpu-bind-groups) TypeGPU provides type-safe abstractions, eliminating manual index management. ### Type-safe Layouts [Section titled “Type-safe Layouts”](#type-safe-layouts) TypeGPU bind group layout ```typescript import { tgpu, d } from "typegpu"; const UniformsSchema = d.struct({ viewProjection: d.mat4x4f, time: d.f32, lightDirection: d.vec3f, }); const sceneLayout = tgpu.bindGroupLayout({ uniforms: { uniform: UniformsSchema }, instanceData: { storage: d.arrayOf(d.mat4x4f) }, }); ``` ### Resource Types in TypeGPU [Section titled “Resource Types in TypeGPU”](#resource-types-in-typegpu) TypeGPU resource type syntax ```typescript const layout = tgpu.bindGroupLayout({ // Uniform buffer (read-only) uniforms: { uniform: d.struct({ value: d.f32 }) }, // Storage buffer (read-only by default) readData: { storage: d.arrayOf(d.f32) }, // Mutable storage buffer writeData: { storage: d.f32, access: "mutable" }, // Filtering sampler linearSampler: { sampler: "filtering" }, // Texture diffuseTexture: { texture: d.tex2d<"f32"> }, // Storage texture outputTexture: { storageTexture: d.storageTex2d<"rgba8unorm"> }, }); ``` ### Creating TypeGPU Bind Groups [Section titled “Creating TypeGPU Bind Groups”](#creating-typegpu-bind-groups) Type-safe bind group creation ```typescript const uniformBuffer = root.createBuffer(UniformsSchema, { viewProjection: mat4.identity(), time: 0, lightDirection: vec3(0, -1, 0), }); const storageBuffer = root.createBuffer( d.arrayOf(d.f32), new Float32Array(1000) ); const bindGroup = root.createBindGroup(sceneLayout, { uniforms: uniformBuffer, instanceData: storageBuffer, }); ``` ## Dynamic Binding [Section titled “Dynamic Binding”](#dynamic-binding) Dynamic offsets allow using different buffer sections without creating multiple bind groups. ### Enable Dynamic Offsets [Section titled “Enable Dynamic Offsets”](#enable-dynamic-offsets) Layout with dynamic offset ```javascript const layout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.VERTEX, buffer: { type: "uniform", hasDynamicOffset: true, minBindingSize: 256, }, }, ], }); ``` ### Use Dynamic Offsets [Section titled “Use Dynamic Offsets”](#use-dynamic-offsets) Render loop with dynamic offsets ```javascript const passEncoder = commandEncoder.beginRenderPass(/* ... */); passEncoder.setPipeline(pipeline); for (let i = 0; i < objectCount; i++) { const offset = i * 256; // Must be 256-byte aligned passEncoder.setBindGroup(0, bindGroup, [offset]); passEncoder.draw(vertexCount, 1, 0, 0); } passEncoder.end(); ``` Alignment Requirements * **Uniform buffers**: 256-byte alignment * **Storage buffers**: 32-byte alignment ```javascript // Calculate aligned offset const alignedOffset = Math.ceil(dataSize / 256) * 256; ``` ## Resource Visibility [Section titled “Resource Visibility”](#resource-visibility) ### Visibility Patterns [Section titled “Visibility Patterns”](#visibility-patterns) | Pattern | Visibility | Use Case | | --------------- | -------------------- | ------------------------------ | | Per-frame data | `VERTEX \| FRAGMENT` | Camera, time, global settings | | Transformations | `VERTEX` | Model matrices, bone data | | Materials | `FRAGMENT` | Textures, lighting parameters | | Compute-only | `COMPUTE` | Storage buffers, dispatch data | ## Efficient Binding [Section titled “Efficient Binding”](#efficient-binding) ### Group by Update Frequency [Section titled “Group by Update Frequency”](#group-by-update-frequency) Organized pipeline layout ```javascript // Group 0: Per-frame (updated once per frame) const frameLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.VERTEX | GPUShaderStage.FRAGMENT, buffer: { type: "uniform" } }, // Camera, time ], }); // Group 1: Per-material (updated when material changes) const materialLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.FRAGMENT, sampler: { type: "filtering" } }, { binding: 1, visibility: GPUShaderStage.FRAGMENT, texture: { sampleType: "float" } }, ], }); // Group 2: Per-object (updated every draw call) const objectLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.VERTEX, buffer: { type: "uniform", hasDynamicOffset: true } }, ], }); ``` ### Efficient Render Loop [Section titled “Efficient Render Loop”](#efficient-render-loop) Minimizing bind group switches ```javascript const frameBindGroup = /* created once per frame */; passEncoder.setBindGroup(0, frameBindGroup); for (const material of materials) { passEncoder.setBindGroup(1, material.bindGroup); for (const object of objectsWithMaterial(material)) { // Use dynamic offset instead of new bind group passEncoder.setBindGroup(2, objectBindGroup, [object.uniformOffset]); passEncoder.draw(/* ... */); } } ``` ### Share Layouts Between Pipelines [Section titled “Share Layouts Between Pipelines”](#share-layouts-between-pipelines) Reusable bind group layout ```javascript const sharedLayout = device.createBindGroupLayout({ /* ... */ }); const pipeline1 = device.createRenderPipeline({ layout: device.createPipelineLayout({ bindGroupLayouts: [sharedLayout], }), // ... }); const pipeline2 = device.createRenderPipeline({ layout: device.createPipelineLayout({ bindGroupLayouts: [sharedLayout], // Same layout }), // ... }); // One bind group works with both pipelines const bindGroup = device.createBindGroup({ layout: sharedLayout, entries: [/* ... */] }); ``` ## Complete Example [Section titled “Complete Example”](#complete-example) Full compute pipeline with bind groups ```javascript // WGSL compute shader const shaderCode = ` struct Params { multiplier: f32, offset: f32 } @group(0) @binding(0) var<uniform> params: Params; @group(0) @binding(1) var<storage, read> input: array<f32>; @group(0) @binding(2) var<storage, read_write> output: array<f32>; @compute @workgroup_size(64) fn main(@builtin(global_invocation_id) gid: vec3u) { let index = gid.x; if (index < arrayLength(&input)) { output[index] = input[index] * params.multiplier + params.offset; } } `; const shaderModule = device.createShaderModule({ code: shaderCode }); // Create bind group layout const bindGroupLayout = device.createBindGroupLayout({ entries: [ { binding: 0, visibility: GPUShaderStage.COMPUTE, buffer: { type: "uniform" } }, { binding: 1, visibility: GPUShaderStage.COMPUTE, buffer: { type: "read-only-storage" } }, { binding: 2, visibility: GPUShaderStage.COMPUTE, buffer: { type: "storage" } }, ], }); // Create pipeline const pipeline = device.createComputePipeline({ layout: device.createPipelineLayout({ bindGroupLayouts: [bindGroupLayout], }), compute: { module: shaderModule, entryPoint: "main", }, }); // Create buffers const paramsBuffer = device.createBuffer({ size: 8, usage: GPUBufferUsage.UNIFORM | GPUBufferUsage.COPY_DST, }); const inputData = new Float32Array(1024); for (let i = 0; i < inputData.length; i++) inputData[i] = i; const inputBuffer = device.createBuffer({ size: inputData.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }); const outputBuffer = device.createBuffer({ size: inputData.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC, }); // Write data device.queue.writeBuffer(paramsBuffer, 0, new Float32Array([2.0, 1.0])); device.queue.writeBuffer(inputBuffer, 0, inputData); // Create bind group const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [ { binding: 0, resource: { buffer: paramsBuffer } }, { binding: 1, resource: { buffer: inputBuffer } }, { binding: 2, resource: { buffer: outputBuffer } }, ], }); // Execute const commandEncoder = device.createCommandEncoder(); const passEncoder = commandEncoder.beginComputePass(); passEncoder.setPipeline(pipeline); passEncoder.setBindGroup(0, bindGroup); passEncoder.dispatchWorkgroups(Math.ceil(inputData.length / 64)); passEncoder.end(); device.queue.submit([commandEncoder.finish()]); ``` ## Resources [Section titled “Resources”](#resources)

# Multisampling (MSAA)

## Overview [Section titled “Overview”](#overview) Multisample Anti-Aliasing (MSAA) reduces jagged edges by sampling each pixel multiple times. WebGPU v1 supports 4x MSAA, where each pixel stores 4 samples that are resolved to a single color. ## Sample Counts [Section titled “Sample Counts”](#sample-counts) | Count | Description | | ----- | ---------------------------------------- | | 1 | No multisampling (default) | | 4 | 4x MSAA (only other option in WebGPU v1) | ## Setup Requirements [Section titled “Setup Requirements”](#setup-requirements) MSAA requires matching configuration across: 1. Multisampled color texture 2. Multisampled depth texture (if used) 3. Render pipeline 4. Render pass with resolve target ## Creating Multisampled Textures [Section titled “Creating Multisampled Textures”](#creating-multisampled-textures) ### Color Texture [Section titled “Color Texture”](#color-texture) 4x MSAA color texture ```javascript const msaaTexture = device.createTexture({ size: [canvas.width, canvas.height], format: navigator.gpu.getPreferredCanvasFormat(), sampleCount: 4, usage: GPUTextureUsage.RENDER_ATTACHMENT, }); ``` ### Depth Texture [Section titled “Depth Texture”](#depth-texture) 4x MSAA depth texture ```javascript const msaaDepthTexture = device.createTexture({ size: [canvas.width, canvas.height], format: "depth24plus", sampleCount: 4, usage: GPUTextureUsage.RENDER_ATTACHMENT, }); ``` Matching Sample Counts Color and depth textures must have the same `sampleCount`. Mismatched counts cause validation errors. ## Pipeline Configuration [Section titled “Pipeline Configuration”](#pipeline-configuration) MSAA pipeline ```javascript const pipeline = device.createRenderPipeline({ layout: "auto", // ...vertex and fragment config... multisample: { count: 4, }, depthStencil: { format: "depth24plus", depthWriteEnabled: true, depthCompare: "less", }, }); ``` ## Render Pass Configuration [Section titled “Render Pass Configuration”](#render-pass-configuration) ### With Resolve Target [Section titled “With Resolve Target”](#with-resolve-target) MSAA render pass with resolve ```javascript const renderPass = encoder.beginRenderPass({ colorAttachments: [{ view: msaaTexture.createView(), // Multisampled texture resolveTarget: context.getCurrentTexture().createView(), // Single-sample output loadOp: "clear", storeOp: "store", clearValue: { r: 0, g: 0, b: 0, a: 1 }, }], depthStencilAttachment: { view: msaaDepthTexture.createView(), depthLoadOp: "clear", depthStoreOp: "store", depthClearValue: 1.0, }, }); ``` ### Resolution Process [Section titled “Resolution Process”](#resolution-process) The `resolveTarget` receives the averaged result automatically when the render pass ends. No manual resolve step needed. ## Complete Example [Section titled “Complete Example”](#complete-example) Full MSAA setup ```javascript const format = navigator.gpu.getPreferredCanvasFormat(); // 1. Create multisampled textures const msaaColor = device.createTexture({ size: [canvas.width, canvas.height], format: format, sampleCount: 4, usage: GPUTextureUsage.RENDER_ATTACHMENT, }); const msaaDepth = device.createTexture({ size: [canvas.width, canvas.height], format: "depth24plus", sampleCount: 4, usage: GPUTextureUsage.RENDER_ATTACHMENT, }); // 2. Configure pipeline const pipeline = device.createRenderPipeline({ layout: "auto", vertex: { module: shaderModule, entryPoint: "vertexMain" }, fragment: { module: shaderModule, entryPoint: "fragmentMain", targets: [{ format }], }, multisample: { count: 4 }, depthStencil: { format: "depth24plus", depthWriteEnabled: true, depthCompare: "less", }, }); // 3. Render with MSAA function render() { const encoder = device.createCommandEncoder(); const pass = encoder.beginRenderPass({ colorAttachments: [{ view: msaaColor.createView(), resolveTarget: context.getCurrentTexture().createView(), loadOp: "clear", storeOp: "store", }], depthStencilAttachment: { view: msaaDepth.createView(), depthLoadOp: "clear", depthStoreOp: "store", depthClearValue: 1.0, }, }); pass.setPipeline(pipeline); pass.draw(vertexCount); pass.end(); device.queue.submit([encoder.finish()]); } ``` ## Reading Multisampled Textures in Shaders [Section titled “Reading Multisampled Textures in Shaders”](#reading-multisampled-textures-in-shaders) Multisampled textures cannot be sampled normally. Use `textureLoad` with sample index: Reading multisampled texture ```wgsl @group(0) @binding(0) var msaaTex: texture_multisampled_2d<f32>; @fragment fn main(@builtin(position) pos: vec4f) -> @location(0) vec4f { let coord = vec2i(pos.xy); // Read each sample manually var color = vec4f(0.0); color += textureLoad(msaaTex, coord, 0); color += textureLoad(msaaTex, coord, 1); color += textureLoad(msaaTex, coord, 2); color += textureLoad(msaaTex, coord, 3); return color / 4.0; // Average } ``` ### Multisampled Depth [Section titled “Multisampled Depth”](#multisampled-depth) Reading multisampled depth ```wgsl @group(0) @binding(0) var msaaDepth: texture_depth_multisampled_2d; @fragment fn main(@builtin(position) pos: vec4f) -> @location(0) vec4f { let coord = vec2i(pos.xy); let depth = textureLoad(msaaDepth, coord, 0); // Sample 0 return vec4f(depth, depth, depth, 1.0); } ``` No Depth Resolve Target Unlike color attachments, depth attachments don’t support `resolveTarget`. To access resolved depth, manually resolve in a compute or fragment shader. ## Resize Handling [Section titled “Resize Handling”](#resize-handling) Recreate multisampled textures when canvas size changes: Handle resize ```javascript function onResize() { msaaColor.destroy(); msaaDepth.destroy(); msaaColor = device.createTexture({ size: [canvas.width, canvas.height], format: format, sampleCount: 4, usage: GPUTextureUsage.RENDER_ATTACHMENT, }); msaaDepth = device.createTexture({ size: [canvas.width, canvas.height], format: "depth24plus", sampleCount: 4, usage: GPUTextureUsage.RENDER_ATTACHMENT, }); } ``` ## Performance Considerations [Section titled “Performance Considerations”](#performance-considerations) | Aspect | Impact | | --------- | ---------------------------------------------------- | | Memory | 4x for color, 4x for depth | | Bandwidth | Higher due to sample storage | | Fill rate | Fragment shader runs once per pixel (not per sample) | | Resolve | Automatic, minimal overhead | ## Render Bundles with MSAA [Section titled “Render Bundles with MSAA”](#render-bundles-with-msaa) MSAA render bundle ```javascript const bundleEncoder = device.createRenderBundleEncoder({ colorFormats: [format], sampleCount: 4, depthStencilFormat: "depth24plus", }); // Record commands... const bundle = bundleEncoder.finish(); ``` ## Format Support [Section titled “Format Support”](#format-support) Not all formats support multisampling. Common supported formats: | Format | Multisampling | | ----------------- | ------------------- | | `bgra8unorm` | Yes | | `rgba8unorm` | Yes | | `rgba16float` | Yes | | `depth24plus` | Yes | | `depth32float` | Yes | | `bgra8unorm-srgb` | No (some platforms) |

# Storage Textures

## Overview [Section titled “Overview”](#overview) Storage textures allow shaders to write directly to texture memory using `textureStore()`. Unlike render attachments that write through the rasterization pipeline, storage textures enable arbitrary writes from compute and fragment shaders. ## Creating Storage Textures [Section titled “Creating Storage Textures”](#creating-storage-textures) Create storage texture ```javascript const storageTexture = device.createTexture({ size: [512, 512], format: "rgba8unorm", usage: GPUTextureUsage.STORAGE_BINDING | GPUTextureUsage.TEXTURE_BINDING, }); ``` Required Usage Flags * `STORAGE_BINDING`: Required for storage texture binding * `TEXTURE_BINDING`: Add if you also need to sample the result * `COPY_SRC`: Add if you need to read back to CPU ## Supported Formats [Section titled “Supported Formats”](#supported-formats) ### Write-Only Formats [Section titled “Write-Only Formats”](#write-only-formats) All storage texture formats support write-only access: | Format | Components | Bits | WGSL Type | | ------------- | ---------- | ---- | ----------- | | `rgba8unorm` | 4 | 32 | `vec4<f32>` | | `rgba8snorm` | 4 | 32 | `vec4<f32>` | | `rgba8uint` | 4 | 32 | `vec4<u32>` | | `rgba8sint` | 4 | 32 | `vec4<i32>` | | `rgba16uint` | 4 | 64 | `vec4<u32>` | | `rgba16sint` | 4 | 64 | `vec4<i32>` | | `rgba16float` | 4 | 64 | `vec4<f32>` | | `r32uint` | 1 | 32 | `u32` | | `r32sint` | 1 | 32 | `i32` | | `r32float` | 1 | 32 | `f32` | | `rg32uint` | 2 | 64 | `vec2<u32>` | | `rg32sint` | 2 | 64 | `vec2<i32>` | | `rg32float` | 2 | 64 | `vec2<f32>` | | `rgba32uint` | 4 | 128 | `vec4<u32>` | | `rgba32sint` | 4 | 128 | `vec4<i32>` | | `rgba32float` | 4 | 128 | `vec4<f32>` | ### Read-Write Formats [Section titled “Read-Write Formats”](#read-write-formats) Only R32 formats support `read_write` access: | Format | Access Modes | | ---------- | ----------------------------- | | `r32float` | `read`, `write`, `read_write` | | `r32uint` | `read`, `write`, `read_write` | | `r32sint` | `read`, `write`, `read_write` | Read-Write Limitations Read-write storage textures require: ```wgsl requires readonly_and_readwrite_storage_textures; ``` Other formats cannot be read and written in the same shader invocation. ## WGSL Declaration [Section titled “WGSL Declaration”](#wgsl-declaration) ### Write-Only [Section titled “Write-Only”](#write-only) Write-only storage texture ```wgsl @group(0) @binding(0) var outputTex: texture_storage_2d<rgba8unorm, write>; @compute @workgroup_size(8, 8) fn main(@builtin(global_invocation_id) id: vec3u) { let color = vec4f( f32(id.x) / 512.0, f32(id.y) / 512.0, 0.5, 1.0 ); textureStore(outputTex, id.xy, color); } ``` ### Read-Write [Section titled “Read-Write”](#read-write) Read-write storage texture (R32 only) ```wgsl requires readonly_and_readwrite_storage_textures; @group(0) @binding(0) var tex: texture_storage_2d<r32float, read_write>; @compute @workgroup_size(8, 8) fn blur(@builtin(global_invocation_id) id: vec3u) { let current = textureLoad(tex, id.xy); let left = textureLoad(tex, id.xy - vec2u(1, 0)); let right = textureLoad(tex, id.xy + vec2u(1, 0)); let avg = (current + left + right) / 3.0; textureBarrier(); // Sync before writing textureStore(tex, id.xy, vec4f(avg, 0, 0, 1)); } ``` ## Binding Configuration [Section titled “Binding Configuration”](#binding-configuration) Bind group layout for storage texture ```javascript const bindGroupLayout = device.createBindGroupLayout({ entries: [{ binding: 0, visibility: GPUShaderStage.COMPUTE, storageTexture: { access: "write-only", // or "read-only", "read-write" format: "rgba8unorm", viewDimension: "2d", }, }], }); const bindGroup = device.createBindGroup({ layout: bindGroupLayout, entries: [{ binding: 0, resource: storageTexture.createView(), }], }); ``` ## Use Cases [Section titled “Use Cases”](#use-cases) ### Procedural Generation [Section titled “Procedural Generation”](#procedural-generation) Procedural noise texture ```wgsl @group(0) @binding(0) var output: texture_storage_2d<rgba8unorm, write>; fn hash(p: vec2f) -> f32 { return fract(sin(dot(p, vec2f(127.1, 311.7))) * 43758.5453); } @compute @workgroup_size(8, 8) fn generateNoise(@builtin(global_invocation_id) id: vec3u) { let uv = vec2f(id.xy) / 512.0; let n = hash(uv * 10.0); textureStore(output, id.xy, vec4f(n, n, n, 1.0)); } ``` ### Image Processing [Section titled “Image Processing”](#image-processing) Grayscale conversion ```wgsl @group(0) @binding(0) var inputTex: texture_2d<f32>; @group(0) @binding(1) var outputTex: texture_storage_2d<rgba8unorm, write>; @compute @workgroup_size(8, 8) fn grayscale(@builtin(global_invocation_id) id: vec3u) { let color = textureLoad(inputTex, id.xy, 0); let gray = dot(color.rgb, vec3f(0.299, 0.587, 0.114)); textureStore(outputTex, id.xy, vec4f(gray, gray, gray, 1.0)); } ``` ### Compute Shader Output [Section titled “Compute Shader Output”](#compute-shader-output) Physics simulation visualization ```wgsl @group(0) @binding(0) var<storage, read> particles: array<vec4f>; @group(0) @binding(1) var output: texture_storage_2d<rgba8unorm, write>; @compute @workgroup_size(64) fn renderParticles(@builtin(global_invocation_id) id: vec3u) { let p = particles[id.x]; let pixel = vec2u(u32(p.x), u32(p.y)); textureStore(output, pixel, vec4f(1.0, 0.5, 0.0, 1.0)); } ``` ## Synchronization [Section titled “Synchronization”](#synchronization) ### Within Workgroup [Section titled “Within Workgroup”](#within-workgroup) Workgroup synchronization ```wgsl textureBarrier(); // Ensure all writes visible within workgroup ``` ### Between Dispatches [Section titled “Between Dispatches”](#between-dispatches) Separate compute passes automatically synchronize: Multi-pass processing ```javascript // Pass 1: Generate computePass1.setPipeline(generatePipeline); computePass1.dispatchWorkgroups(64, 64); computePass1.end(); // Pass 2: Process (automatically waits for Pass 1) computePass2.setPipeline(processPipeline); computePass2.dispatchWorkgroups(64, 64); computePass2.end(); ``` ## TypeGPU Storage Textures [Section titled “TypeGPU Storage Textures”](#typegpu-storage-textures) TypeGPU storage texture binding ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; const outputTexture = root.createTexture({ size: [512, 512], format: "rgba8unorm", }).$usage("storage", "sampled"); const processImage = tgpu .computeFn({ workgroupSize: [8, 8] }) .does`(@builtin(global_invocation_id) id: vec3u) { let color = vec4f(f32(id.x) / 512.0, f32(id.y) / 512.0, 0.5, 1.0); textureStore(${outputTexture}, id.xy, color); }`; ``` ## Limitations [Section titled “Limitations”](#limitations) Storage Texture Constraints * **No multisampling**: Storage textures cannot have `sampleCount > 1` * **No mipmaps**: Only mip level 0 accessible * **Format in shader**: Must specify exact format in WGSL declaration * **No filtering**: Use `textureLoad()` for exact texel access only * **Limited read-write**: Only R32 formats support simultaneous read/write

# Textures and Samplers

## Overview [Section titled “Overview”](#overview) Textures are multi-dimensional arrays of data stored in GPU memory, optimized for spatial access patterns. Unlike buffers that store linear arrays, textures provide built-in interpolation, filtering, and addressing modes through samplers. Textures and samplers work together: * **Textures** hold pixel data in GPU memory * **Samplers** define how data is read—filtering quality, coordinate wrapping, mipmap selection ## Texture Types [Section titled “Texture Types”](#texture-types) ### 1D, 2D, 3D Textures [Section titled “1D, 2D, 3D Textures”](#1d-2d-3d-textures) ### Cube Maps [Section titled “Cube Maps”](#cube-maps) Six 2D textures arranged as cube faces for environment mapping: Cube map for skybox ```javascript const skyboxTexture = device.createTexture({ size: [512, 512, 6], // Width, height, 6 faces dimension: "2d", format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST, }); const cubeView = skyboxTexture.createView({ dimension: "cube", }); ``` Sampled using 3D direction vectors rather than 2D coordinates. ### Texture Arrays [Section titled “Texture Arrays”](#texture-arrays) Multiple independent 2D images in a single texture object: Texture array for sprites ```javascript const textureArray = device.createTexture({ size: [256, 256, 10], // 10 layers of 256×256 images dimension: "2d", format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST, }); const arrayView = textureArray.createView({ dimension: "2d-array", }); ``` Ideal for sprite animations and terrain splatmaps. ### Multisampled Textures [Section titled “Multisampled Textures”](#multisampled-textures) Multiple samples per pixel for antialiasing: 4× MSAA texture ```javascript const msaaTexture = device.createTexture({ size: [800, 600, 1], dimension: "2d", format: "rgba8unorm", sampleCount: 4, usage: GPUTextureUsage.RENDER_ATTACHMENT, }); ``` Caution Multisampled textures can only be render attachments. Resolve to regular textures before sampling. ## Creating Textures [Section titled “Creating Textures”](#creating-textures) ### GPUTextureDescriptor [Section titled “GPUTextureDescriptor”](#gputexturedescriptor) Complete texture creation ```javascript const texture = device.createTexture({ label: "Main Color Texture", size: { width: 512, height: 512, depthOrArrayLayers: 1 }, dimension: "2d", format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.RENDER_ATTACHMENT, mipLevelCount: 1, sampleCount: 1, }); ``` | Property | Description | | --------------- | -------------------------------------------------- | | `label` | Debug string for error messages | | `size` | Dimensions as object or `[width, height?, depth?]` | | `dimension` | `'1d'`, `'2d'`, or `'3d'` | | `format` | Pixel format (see formats section) | | `usage` | Bitwise combination of usage flags | | `mipLevelCount` | Number of mipmap levels | | `sampleCount` | 1 (default) or 4 (MSAA) | ### Usage Flags [Section titled “Usage Flags”](#usage-flags) | Flag | Purpose | | ----------------------- | -------------------------- | | `TEXTURE_BINDING` | Sample in shaders | | `STORAGE_BINDING` | Read-write storage texture | | `RENDER_ATTACHMENT` | Render target | | `COPY_SRC` / `COPY_DST` | Copy operations | ### Mipmap Levels [Section titled “Mipmap Levels”](#mipmap-levels) For a full mipmap chain: Calculate mip levels ```javascript const maxDimension = Math.max(width, height); const mipLevelCount = Math.floor(Math.log2(maxDimension)) + 1; // 512×512 → 10 levels: 512→256→128→64→32→16→8→4→2→1 ``` ## Common Texture Formats [Section titled “Common Texture Formats”](#common-texture-formats) | Format | Description | Use Case | | -------------- | ---------------------- | ---------------------------- | | `rgba8unorm` | 8-bit RGBA, normalized | Standard color | | `bgra8unorm` | 8-bit BGRA, normalized | Canvas render targets | | `depth24plus` | ≥24-bit depth | Depth buffers | | `depth32float` | 32-bit float depth | Shader-accessible depth | | `r32float` | Single-channel float | Height maps, distance fields | | `rgba16float` | 16-bit float RGBA | HDR rendering | | `r8unorm` | Single-channel 8-bit | Masks, grayscale | Get preferred canvas format ```javascript const preferredFormat = navigator.gpu.getPreferredCanvasFormat(); // 'bgra8unorm' on macOS/Metal, 'rgba8unorm' on Windows/D3D12 ``` Format Compatibility Not all formats work with all usages. `r32float` cannot be a `RENDER_ATTACHMENT` without checking device features. ## Loading Images [Section titled “Loading Images”](#loading-images) ### From URL [Section titled “From URL”](#from-url) Load texture from image file ```javascript async function loadTexture(device, url) { const response = await fetch(url); const blob = await response.blob(); const imageBitmap = await createImageBitmap(blob); const texture = device.createTexture({ size: [imageBitmap.width, imageBitmap.height, 1], format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST | GPUTextureUsage.RENDER_ATTACHMENT, }); device.queue.copyExternalImageToTexture( { source: imageBitmap }, { texture: texture }, [imageBitmap.width, imageBitmap.height] ); return texture; } ``` ### From Canvas [Section titled “From Canvas”](#from-canvas) Procedural texture from canvas ```javascript function createProceduralTexture(device, size) { const canvas = document.createElement("canvas"); canvas.width = size; canvas.height = size; const ctx = canvas.getContext("2d"); const gradient = ctx.createLinearGradient(0, 0, size, size); gradient.addColorStop(0, "#ff0000"); gradient.addColorStop(1, "#0000ff"); ctx.fillStyle = gradient; ctx.fillRect(0, 0, size, size); const texture = device.createTexture({ size: [size, size], format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST, }); device.queue.copyExternalImageToTexture( { source: canvas }, { texture: texture }, [size, size] ); return texture; } ``` ### copyExternalImageToTexture Sources [Section titled “copyExternalImageToTexture Sources”](#copyexternalimagetotexture-sources) * `ImageBitmap` * `HTMLCanvasElement` * `OffscreenCanvas` * `HTMLVideoElement` Flip image on upload ```javascript device.queue.copyExternalImageToTexture( { source: imageBitmap, flipY: true }, { texture: texture }, [width, height] ); ``` ## Samplers [Section titled “Samplers”](#samplers) ### Creating Samplers [Section titled “Creating Samplers”](#creating-samplers) Complete sampler configuration ```javascript const sampler = device.createSampler({ label: "Linear Repeat Sampler", addressModeU: "repeat", addressModeV: "repeat", addressModeW: "repeat", magFilter: "linear", minFilter: "linear", mipmapFilter: "linear", maxAnisotropy: 1, lodMinClamp: 0, lodMaxClamp: 10, }); ``` ### Filtering Modes [Section titled “Filtering Modes”](#filtering-modes) | Property | `'nearest'` | `'linear'` | | -------------- | ------------------------------ | -------------------- | | `magFilter` | Pixelated (good for pixel art) | Smooth interpolation | | `minFilter` | May cause flickering | Reduces aliasing | | `mipmapFilter` | Abrupt mip transitions | Smooth mip blending | Common sampler presets ```javascript // Pixel art / retro const pixelSampler = device.createSampler({ magFilter: "nearest", minFilter: "nearest", }); // Smooth texturing const linearSampler = device.createSampler({ magFilter: "linear", minFilter: "linear", mipmapFilter: "linear", }); // High quality with anisotropic filtering const qualitySampler = device.createSampler({ magFilter: "linear", minFilter: "linear", mipmapFilter: "linear", maxAnisotropy: 16, }); ``` ### Address Modes [Section titled “Address Modes”](#address-modes) | Mode | Behavior | | ----------------- | ------------------------------ | | `'clamp-to-edge'` | Edge texel repeated infinitely | | `'repeat'` | Texture tiles (1.5 → 0.5) | | `'mirror-repeat'` | Alternates direction at edges | Address mode examples ```javascript // Tiling textures (brick walls, terrain) const tiledSampler = device.createSampler({ addressModeU: "repeat", addressModeV: "repeat", magFilter: "linear", minFilter: "linear", }); // UI elements, skyboxes (no repeat) const clampedSampler = device.createSampler({ addressModeU: "clamp-to-edge", addressModeV: "clamp-to-edge", magFilter: "linear", minFilter: "linear", }); ``` ### Anisotropic Filtering [Section titled “Anisotropic Filtering”](#anisotropic-filtering) Improves quality for surfaces viewed at oblique angles: Enable anisotropic filtering ```javascript const sampler = device.createSampler({ magFilter: "linear", minFilter: "linear", mipmapFilter: "linear", maxAnisotropy: 16, // 1, 2, 4, 8, or 16 }); ``` ## Mipmapping [Section titled “Mipmapping”](#mipmapping) Mipmaps are pre-computed, progressively smaller texture versions: ```plaintext Level 0: 512×512 (original) Level 1: 256×256 Level 2: 128×128 ... Level 9: 1×1 ``` ### Manual Mipmap Data [Section titled “Manual Mipmap Data”](#manual-mipmap-data) Create texture with mipmaps ```javascript const mipLevelCount = Math.floor(Math.log2(Math.max(width, height))) + 1; const texture = device.createTexture({ size: [width, height], format: "rgba8unorm", mipLevelCount: mipLevelCount, usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST, }); // Write each mip level for (let mipLevel = 0; mipLevel < mipLevelCount; mipLevel++) { const mipWidth = Math.max(1, width >> mipLevel); const mipHeight = Math.max(1, height >> mipLevel); const mipData = generateMipData(mipLevel); device.queue.writeTexture( { texture: texture, mipLevel: mipLevel }, mipData, { bytesPerRow: mipWidth * 4 }, [mipWidth, mipHeight] ); } ``` Caution WebGPU doesn’t provide built-in mipmap generation. Use compute shaders or render passes to downsample. ### Generating Mipmaps [Section titled “Generating Mipmaps”](#generating-mipmaps) Unlike WebGL’s `generateMipmap()`, WebGPU requires manual mipmap generation. ## Sampling in Shaders [Section titled “Sampling in Shaders”](#sampling-in-shaders) ### textureSample [Section titled “textureSample”](#texturesample) Filtered texture lookup (uses sampler settings): Basic texture sampling ```wgsl @group(0) @binding(0) var textureSampler: sampler; @group(0) @binding(1) var colorTexture: texture_2d<f32>; @fragment fn fragmentMain(@location(0) texCoord: vec2f) -> @location(0) vec4f { let color = textureSample(colorTexture, textureSampler, texCoord); return color; } ``` ### textureLoad [Section titled “textureLoad”](#textureload) Direct texel access without filtering: Load exact texel value ```wgsl @group(0) @binding(0) var inputTexture: texture_2d<f32>; @fragment fn fragmentMain(@location(0) texCoord: vec2f) -> @location(0) vec4f { let dimensions = textureDimensions(inputTexture); let pixelCoord = vec2<i32>(texCoord * vec2f(dimensions)); let color = textureLoad(inputTexture, pixelCoord, 0); return color; } ``` ### Complete Shader Example [Section titled “Complete Shader Example”](#complete-shader-example) Full texture sampling shader ```wgsl struct VertexOutput { @builtin(position) position: vec4f, @location(0) texCoord: vec2f, }; @vertex fn vertexMain( @location(0) position: vec3f, @location(1) texCoord: vec2f ) -> VertexOutput { var output: VertexOutput; output.position = vec4f(position, 1.0); output.texCoord = texCoord; return output; } @group(0) @binding(0) var textureSampler: sampler; @group(0) @binding(1) var diffuseTexture: texture_2d<f32>; @fragment fn fragmentMain(input: VertexOutput) -> @location(0) vec4f { return textureSample(diffuseTexture, textureSampler, input.texCoord); } ``` ## TypeGPU Texture Support [Section titled “TypeGPU Texture Support”](#typegpu-texture-support) TypeGPU texture binding ```typescript import tgpu from "typegpu"; // Define texture and sampler slots const diffuseTextureSlot = tgpu.textureSlot({ format: "rgba8unorm", dimension: "2d", }); const samplerSlot = tgpu.samplerSlot(); // Create shader using slots const fragmentShader = tgpu.fragmentFn((texCoord: vec2f) => { const color = diffuseTextureSlot.sample(samplerSlot, texCoord); return { color }; }); ``` ## Texture Coordinates [Section titled “Texture Coordinates”](#texture-coordinates) ### UV Space [Section titled “UV Space”](#uv-space) Coordinates range from 0 to 1: ```plaintext (0,0) ────────────> U │ Top-left origin │ (WebGPU convention) v V (1,1) ``` Origin Conventions WebGPU uses top-left origin. Some image formats use bottom-left. Use `flipY: true` when loading if needed. Vertex data with UVs ```javascript const vertices = new Float32Array([ // Position (x,y,z) TexCoord (u,v) -1.0, -1.0, 0.0, 0.0, 1.0, // Bottom-left 1.0, -1.0, 0.0, 1.0, 1.0, // Bottom-right 1.0, 1.0, 0.0, 1.0, 0.0, // Top-right -1.0, 1.0, 0.0, 0.0, 0.0, // Top-left ]); ``` ### Texture Atlases [Section titled “Texture Atlases”](#texture-atlases) Combine multiple small textures to reduce bind group changes: Texture atlas approach ```javascript const atlas = device.createTexture({ size: [2048, 2048], format: "rgba8unorm", usage: GPUTextureUsage.TEXTURE_BINDING | GPUTextureUsage.COPY_DST, }); // Calculate UVs for specific sprite region function getSpriteUVs(spriteIndex, atlasLayout) { const col = spriteIndex % atlasLayout.columns; const row = Math.floor(spriteIndex / atlasLayout.columns); const u = col / atlasLayout.columns; const v = row / atlasLayout.rows; const spriteWidth = 1 / atlasLayout.columns; const spriteHeight = 1 / atlasLayout.rows; return { u, v, width: spriteWidth, height: spriteHeight }; } ``` ## Depth Textures [Section titled “Depth Textures”](#depth-textures) ### Depth Formats [Section titled “Depth Formats”](#depth-formats) | Format | Bits | Shader Access | Use Case | | ----------------------- | ---- | ------------- | ----------------------- | | `depth24plus` | ≥24 | No | Standard depth buffer | | `depth32float` | 32 | Yes | Shadow mapping, effects | | `depth24plus-stencil8` | 24+8 | No | Depth + stencil | | `depth32float-stencil8` | 32+8 | Depth only | Depth access + stencil | ### Creating Depth Textures [Section titled “Creating Depth Textures”](#creating-depth-textures) Shader-accessible depth texture ```javascript const depthTexture = device.createTexture({ size: [canvas.width, canvas.height], format: "depth32float", usage: GPUTextureUsage.RENDER_ATTACHMENT | GPUTextureUsage.TEXTURE_BINDING, }); ``` ### Shadow Mapping [Section titled “Shadow Mapping”](#shadow-mapping) ### Reading Depth Without Comparison [Section titled “Reading Depth Without Comparison”](#reading-depth-without-comparison) For post-processing effects that need raw depth values: Read raw depth values ```wgsl @group(0) @binding(0) var depthTex: texture_depth_2d; @fragment fn main(@builtin(position) fragCoord: vec4f) -> @location(0) vec4f { let depth = textureLoad(depthTex, vec2i(fragCoord.xy), 0); // depth is in [0, 1] range (0 = near, 1 = far) return vec4f(depth, depth, depth, 1.0); } ``` ## Resource Cleanup [Section titled “Resource Cleanup”](#resource-cleanup) Memory Management Large textures consume significant GPU memory. Destroy when no longer needed: ```javascript texture.destroy(); // Free GPU memory immediately ``` ## Resources [Section titled “Resources”](#resources)

# Compute Pipelines

## Overview [Section titled “Overview”](#overview) Compute pipelines enable general-purpose GPU computation (GPGPU) in WebGPU, executing arbitrary parallel algorithms without rendering operations. Unlike render pipelines that produce visual output, compute pipelines process data through storage buffers and textures. ## GPU Architecture [Section titled “GPU Architecture”](#gpu-architecture) ### Parallelism Model [Section titled “Parallelism Model”](#parallelism-model) | CPU | GPU | | ------------------------------------ | --------------------------------- | | 4-16 high-performance cores | 2,000-10,000+ simpler cores | | Optimized for sequential performance | Optimized for throughput | | Low latency per operation | High throughput across operations | GPUs execute the same instruction on different data simultaneously—the **SIMT (Single Instruction, Multiple Threads)** model. All threads in a group run the same instruction in lockstep. Thread Divergence When threads in a group take different branches, the GPU must execute both paths sequentially: ```wgsl if (data[id] > threshold) { result[id] = expensiveCalculation(data[id]); } else { result[id] = 0.0; } ``` If half the threads take each branch, execution time equals the sum of both paths, not the faster one. ### When GPUs Excel [Section titled “When GPUs Excel”](#when-gpus-excel) GPUs become advantageous when: * Workload is highly parallel (thousands+ independent operations) * Data transfers to GPU memory are efficient * Algorithm doesn’t require frequent CPU-GPU synchronization * Memory access patterns align with GPU architecture ## Workgroups and Invocations [Section titled “Workgroups and Invocations”](#workgroups-and-invocations) ### Workgroup Structure [Section titled “Workgroup Structure”](#workgroup-structure) A **workgroup** is a collection of shader invocations that execute together and can cooperate through shared memory. Workgroup size declaration ```wgsl @compute @workgroup_size(8, 8, 1) fn computeMain( @builtin(global_invocation_id) global_id: vec3<u32>, @builtin(local_invocation_id) local_id: vec3<u32>, @builtin(workgroup_id) workgroup_id: vec3<u32> ) { // 8×8×1 = 64 invocations per workgroup } ``` ### Built-in Identifiers [Section titled “Built-in Identifiers”](#built-in-identifiers) | Built-in | Type | Description | | ------------------------ | ----------- | ------------------------------------------ | | `local_invocation_id` | `vec3<u32>` | Position within workgroup (0 to size-1) | | `workgroup_id` | `vec3<u32>` | Which workgroup this invocation belongs to | | `global_invocation_id` | `vec3<u32>` | Unique position across all workgroups | | `local_invocation_index` | `u32` | Linearized index within workgroup | | `num_workgroups` | `vec3<u32>` | Total workgroups dispatched | The `global_invocation_id` is computed as: `workgroup_id × workgroup_size + local_invocation_id` ### Choosing Workgroup Size [Section titled “Choosing Workgroup Size”](#choosing-workgroup-size) Query device limits: Check workgroup limits ```javascript const limits = device.limits; console.log("Max workgroup size X:", limits.maxComputeWorkgroupSizeX); console.log("Max invocations per workgroup:", limits.maxComputeInvocationsPerWorkgroup); ``` ## Dispatching Work [Section titled “Dispatching Work”](#dispatching-work) ### dispatchWorkgroups() [Section titled “dispatchWorkgroups()”](#dispatchworkgroups) Dispatch compute work ```javascript const commandEncoder = device.createCommandEncoder(); const passEncoder = commandEncoder.beginComputePass(); passEncoder.setPipeline(computePipeline); passEncoder.setBindGroup(0, bindGroup); passEncoder.dispatchWorkgroups(workgroupsX, workgroupsY, workgroupsZ); passEncoder.end(); device.queue.submit([commandEncoder.finish()]); ``` ### Calculating Dispatch Dimensions [Section titled “Calculating Dispatch Dimensions”](#calculating-dispatch-dimensions) **1D Data** (10,000 elements, workgroup size 64): 1D dispatch calculation ```javascript const dataSize = 10000; const workgroupSize = 64; const workgroupsNeeded = Math.ceil(dataSize / workgroupSize); passEncoder.dispatchWorkgroups(workgroupsNeeded, 1, 1); // Dispatches 157 workgroups (157 × 64 = 10,048 invocations) ``` **2D Data** (1920×1080 image, 8×8 workgroups): 2D dispatch calculation ```javascript const workgroupsX = Math.ceil(1920 / 8); const workgroupsY = Math.ceil(1080 / 8); passEncoder.dispatchWorkgroups(workgroupsX, workgroupsY, 1); ``` Always Add Bounds Checks Dispatching more invocations than data elements causes out-of-bounds access: ```wgsl @compute @workgroup_size(64) fn process(@builtin(global_invocation_id) id: vec3<u32>) { if (id.x >= arrayLength(&data)) { return; // Guard against overflow } // Process data[id.x] } ``` ### Dispatch Strategies [Section titled “Dispatch Strategies”](#dispatch-strategies) **Multi-pass for cross-workgroup dependencies:** When results from one workgroup affect another, use separate dispatches: Multi-pass reduction ```javascript // Pass 1: Partial sums within workgroups pass1.dispatchWorkgroups(Math.ceil(dataSize / 256)); pass1.end(); // Pass 2: Final reduction of partial results const pass2 = encoder.beginComputePass(); pass2.setPipeline(reductionPipeline); pass2.setBindGroup(0, partialResultsBindGroup); pass2.dispatchWorkgroups(1); // Single workgroup for final sum pass2.end(); ``` **Indirect dispatch for GPU-driven workloads:** Let the GPU determine dispatch size: GPU-driven dispatch ```javascript // Buffer holds dispatch dimensions (3 × u32) const indirectBuffer = device.createBuffer({ size: 12, usage: GPUBufferUsage.INDIRECT | GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }); // Compute shader writes workgroup counts // Then dispatch using those counts pass.dispatchWorkgroupsIndirect(indirectBuffer, 0); ``` Write dispatch args in shader ```wgsl struct DispatchArgs { x: u32, y: u32, z: u32, } @group(0) @binding(0) var<storage, read_write> dispatch: DispatchArgs; @group(0) @binding(1) var<storage, read> activeCount: u32; @compute @workgroup_size(1) fn prepareDispatch() { dispatch.x = (activeCount + 63u) / 64u; // Workgroups needed dispatch.y = 1u; dispatch.z = 1u; } ``` ## Creating Compute Pipelines [Section titled “Creating Compute Pipelines”](#creating-compute-pipelines) ### GPUComputePipeline [Section titled “GPUComputePipeline”](#gpucomputepipeline) Create compute pipeline ```javascript const computePipeline = device.createComputePipeline({ label: "data-processing-pipeline", layout: "auto", compute: { module: shaderModule, entryPoint: "main", }, }); ``` Use async creation to avoid blocking: Async pipeline creation ```javascript const computePipeline = await device.createComputePipelineAsync({ layout: "auto", compute: { module: shaderModule, entryPoint: "main" }, }); ``` ### Storage Buffers [Section titled “Storage Buffers”](#storage-buffers) Storage buffers are the primary data mechanism for compute shaders: Create storage buffers ```javascript const inputBuffer = device.createBuffer({ size: dataArray.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, mappedAtCreation: true, }); new Float32Array(inputBuffer.getMappedRange()).set(dataArray); inputBuffer.unmap(); const outputBuffer = device.createBuffer({ size: dataArray.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC, }); ``` In WGSL: Storage buffer declarations ```wgsl @group(0) @binding(0) var<storage, read> input: array<f32>; @group(0) @binding(1) var<storage, read_write> output: array<f32>; ``` | Access Mode | WGSL | Use Case | | ----------- | -------------------------- | -------------------------- | | Read-only | `var<storage, read>` | Input data | | Read-write | `var<storage, read_write>` | Output or in-place updates | ### Complete Example [Section titled “Complete Example”](#complete-example) Full compute pipeline setup ```javascript const shaderCode = ` @group(0) @binding(0) var<storage, read> input: array<f32>; @group(0) @binding(1) var<storage, read_write> output: array<f32>; @compute @workgroup_size(64) fn main(@builtin(global_invocation_id) id: vec3<u32>) { let i = id.x; if (i < arrayLength(&input)) { output[i] = sqrt(input[i]); } } `; const shaderModule = device.createShaderModule({ code: shaderCode }); const pipeline = device.createComputePipeline({ layout: "auto", compute: { module: shaderModule, entryPoint: "main" }, }); // Create buffers and bind group const inputData = new Float32Array(1000).map((_, i) => i); const inputBuffer = device.createBuffer({ size: inputData.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST, }); device.queue.writeBuffer(inputBuffer, 0, inputData); const outputBuffer = device.createBuffer({ size: inputData.byteLength, usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC, }); const bindGroup = device.createBindGroup({ layout: pipeline.getBindGroupLayout(0), entries: [ { binding: 0, resource: { buffer: inputBuffer } }, { binding: 1, resource: { buffer: outputBuffer } }, ], }); // Execute const encoder = device.createCommandEncoder(); const pass = encoder.beginComputePass(); pass.setPipeline(pipeline); pass.setBindGroup(0, bindGroup); pass.dispatchWorkgroups(Math.ceil(inputData.length / 64)); pass.end(); device.queue.submit([encoder.finish()]); ``` ## TypeGPU Compute Pipelines [Section titled “TypeGPU Compute Pipelines”](#typegpu-compute-pipelines) TypeGPU provides type-safe compute pipeline abstractions: TypeGPU compute pipeline ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; const root = await tgpu.init(); const doubleValues = tgpu.computeFn([], () => { "use gpu"; const id = builtin.globalInvocationId.x; if (id < inputBuffer.value.length) { outputBuffer.value[id] = inputBuffer.value[id] * 2.0; } }).$workgroupSize(64); const pipeline = root["~unstable"].withCompute(doubleValues).createPipeline(); pipeline.with(bindGroup).dispatchWorkgroups(Math.ceil(1000 / 64)); ``` ## Memory Access Patterns [Section titled “Memory Access Patterns”](#memory-access-patterns) ### Coalesced Access [Section titled “Coalesced Access”](#coalesced-access) ### Shared Memory [Section titled “Shared Memory”](#shared-memory) Workgroup shared memory (`var<workgroup>`) is fast on-chip memory shared by all invocations in a workgroup: Using shared memory ```wgsl var<workgroup> sharedCache: array<f32, 64>; @compute @workgroup_size(64) fn process( @builtin(local_invocation_id) local_id: vec3<u32>, @builtin(global_invocation_id) global_id: vec3<u32> ) { // Load into shared memory sharedCache[local_id.x] = input[global_id.x]; // Wait for all threads to finish loading workgroupBarrier(); // Perform operations using cached data var sum = 0.0; for (var i = 0u; i < 64u; i++) { sum += sharedCache[i]; } output[global_id.x] = sum; } ``` ## Synchronization [Section titled “Synchronization”](#synchronization) ### workgroupBarrier() [Section titled “workgroupBarrier()”](#workgroupbarrier) Synchronizes all invocations in a workgroup—all threads must reach the barrier before any can proceed: Barrier synchronization ```wgsl var<workgroup> data: array<f32, 64>; @compute @workgroup_size(64) fn compute(@builtin(local_invocation_id) local_id: vec3<u32>) { // Phase 1: Each thread writes data[local_id.x] = computeValue(); // Wait for all writes to complete workgroupBarrier(); // Phase 2: Safely read other threads' data let sum = data[0] + data[local_id.x]; } ``` ### storageBarrier() [Section titled “storageBarrier()”](#storagebarrier) Ensures all storage buffer writes within a workgroup are visible before proceeding: Storage barrier ```wgsl @group(0) @binding(0) var<storage, read_write> buffer: array<f32>; @compute @workgroup_size(64) fn compute(@builtin(global_invocation_id) id: vec3<u32>) { // Write to storage buffer buffer[id.x] = computeValue(id.x); // Ensure writes are visible within workgroup storageBarrier(); // Now reads see updated values (within same workgroup) let neighbor = buffer[id.x + 1u]; } ``` Workgroup Scope Only Both `workgroupBarrier()` and `storageBarrier()` only synchronize invocations **within the same workgroup**. They cannot synchronize across different workgroups. For cross-workgroup synchronization: * Use multiple dispatch calls (separate passes) * Use atomics for coordination ### Barrier Comparison [Section titled “Barrier Comparison”](#barrier-comparison) | Barrier | Scope | Memory Affected | | -------------------- | --------- | ---------------- | | `workgroupBarrier()` | Workgroup | `var<workgroup>` | | `storageBarrier()` | Workgroup | `var<storage>` | | `textureBarrier()` | Workgroup | Storage textures | Race Conditions Without barriers, some threads may read before others have written: ```wgsl // BAD: Race condition! counter[0] = counter[0] + 1; // Multiple threads write simultaneously // GOOD: Use atomics atomicAdd(&counter[0], 1u); ``` ## Common Use Cases [Section titled “Common Use Cases”](#common-use-cases) ## Resources [Section titled “Resources”](#resources)

# Instancing and Indirect Drawing

## Overview [Section titled “Overview”](#overview) Instancing draws multiple copies of geometry with a single draw call. Indirect drawing reads draw parameters from GPU buffers, enabling GPU-driven rendering without CPU readback. ## Instancing [Section titled “Instancing”](#instancing) ### Basic Instanced Drawing [Section titled “Basic Instanced Drawing”](#basic-instanced-drawing) Draw 1000 instances ```javascript // Draw 36 vertices, 1000 times pass.draw(36, 1000); // Or with indices pass.drawIndexed(36, 1000); ``` ### Accessing Instance Data [Section titled “Accessing Instance Data”](#accessing-instance-data) Using instance\_index builtin ```wgsl struct VertexOutput { @builtin(position) position: vec4f, @location(0) color: vec4f, } @group(0) @binding(0) var<storage, read> transforms: array<mat4x4f>; @group(0) @binding(1) var<storage, read> colors: array<vec4f>; @vertex fn vertexMain( @location(0) position: vec3f, @builtin(instance_index) instanceIdx: u32 ) -> VertexOutput { var output: VertexOutput; output.position = transforms[instanceIdx] * vec4f(position, 1.0); output.color = colors[instanceIdx]; return output; } ``` ### Instance Step Mode [Section titled “Instance Step Mode”](#instance-step-mode) Alternative: store per-instance data in vertex buffers with `stepMode: "instance"`: Per-instance vertex attributes ```javascript const pipeline = device.createRenderPipeline({ vertex: { buffers: [ // Per-vertex data { arrayStride: 12, stepMode: "vertex", attributes: [...] }, // Per-instance data (advances once per instance) { arrayStride: 64, stepMode: "instance", attributes: [ { shaderLocation: 1, offset: 0, format: "float32x4" }, // row 0 { shaderLocation: 2, offset: 16, format: "float32x4" }, // row 1 { shaderLocation: 3, offset: 32, format: "float32x4" }, // row 2 { shaderLocation: 4, offset: 48, format: "float32x4" }, // row 3 ], }, ], }, // ... }); ``` ## Indirect Drawing [Section titled “Indirect Drawing”](#indirect-drawing) ### Indirect Buffer Layout [Section titled “Indirect Buffer Layout”](#indirect-buffer-layout) Draw parameters come from GPU buffers instead of JavaScript: | Method | Buffer Contents (u32) | Size | | --------------------- | ---------------------------------------------------------------- | -------- | | `drawIndirect` | vertexCount, instanceCount, firstVertex, firstInstance | 16 bytes | | `drawIndexedIndirect` | indexCount, instanceCount, firstIndex, baseVertex, firstInstance | 20 bytes | Create indirect buffer ```javascript const indirectBuffer = device.createBuffer({ size: 16, usage: GPUBufferUsage.INDIRECT | GPUBufferUsage.COPY_DST, }); // Write draw parameters const params = new Uint32Array([36, 1000, 0, 0]); // 36 verts, 1000 instances device.queue.writeBuffer(indirectBuffer, 0, params); ``` ### Indirect Draw Calls [Section titled “Indirect Draw Calls”](#indirect-draw-calls) Execute indirect draw ```javascript pass.drawIndirect(indirectBuffer, 0); // Or indexed pass.drawIndexedIndirect(indirectBuffer, 0); ``` indirect-first-instance Feature Non-zero `firstInstance` requires the `indirect-first-instance` feature: ```javascript const device = await adapter.requestDevice({ requiredFeatures: ["indirect-first-instance"], }); ``` Without this feature, non-zero firstInstance values are treated as no-ops. ### Multiple Indirect Draws [Section titled “Multiple Indirect Draws”](#multiple-indirect-draws) ## GPU-Driven Rendering [Section titled “GPU-Driven Rendering”](#gpu-driven-rendering) Combine compute shaders with indirect drawing for GPU culling: GPU culling compute shader ```wgsl struct DrawIndirect { vertexCount: u32, instanceCount: u32, firstVertex: u32, firstInstance: u32, } @group(0) @binding(0) var<storage, read> boundingSpheres: array<vec4f>; @group(0) @binding(1) var<storage, read_write> drawCommands: array<DrawIndirect>; @group(0) @binding(2) var<storage, read_write> visibleInstances: array<u32>; @group(0) @binding(3) var<storage, read_write> visibleCount: atomic<u32>; @compute @workgroup_size(64) fn cullInstances(@builtin(global_invocation_id) id: vec3u) { let idx = id.x; let sphere = boundingSpheres[idx]; if (isVisible(sphere)) { let slot = atomicAdd(&visibleCount, 1u); visibleInstances[slot] = idx; } } ``` ### Culling Pipeline [Section titled “Culling Pipeline”](#culling-pipeline) 1. **Reset**: Clear visible count to 0 2. **Cull**: Compute shader tests each instance, writes visible indices 3. **Update**: Copy visible count to indirect buffer’s instanceCount 4. **Draw**: Execute indirect draw with GPU-determined instance count GPU culling workflow ```javascript // Compute pass: cull instances const computePass = encoder.beginComputePass(); computePass.setPipeline(cullPipeline); computePass.setBindGroup(0, cullBindGroup); computePass.dispatchWorkgroups(Math.ceil(instanceCount / 64)); computePass.end(); // Copy visible count to indirect buffer encoder.copyBufferToBuffer( visibleCountBuffer, 0, indirectBuffer, 4, // offset to instanceCount field 4 ); // Render pass: draw visible instances const renderPass = encoder.beginRenderPass(descriptor); renderPass.setPipeline(renderPipeline); renderPass.drawIndirect(indirectBuffer, 0); renderPass.end(); ``` ## TypeGPU Instancing [Section titled “TypeGPU Instancing”](#typegpu-instancing) TypeGPU instanced rendering ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; const transforms = root.createBuffer(d.arrayOf(d.mat4x4f, 1000)) .$usage("storage"); const vertexFn = tgpu.vertexFn({ position: d.vec3f, }, d.vec4f).does`(input) -> vec4f { let idx = ${tgpu.builtin.instanceIndex}; return ${transforms}[idx] * vec4f(input.position, 1.0); }`; ``` ## Performance Comparison [Section titled “Performance Comparison”](#performance-comparison) | Technique | CPU Overhead | GPU Control | Use Case | | ---------------------- | ------------ | ----------- | ----------------------- | | Direct draw | Highest | None | Simple scenes | | Instancing | Medium | None | Many identical objects | | Indirect draw | Low | Partial | Dynamic instance counts | | GPU culling + indirect | Lowest | Full | Large scenes, culling |

# Render Bundles

## Overview [Section titled “Overview”](#overview) Render bundles pre-record sequences of render commands for efficient replay. They reduce CPU overhead by validating commands once during encoding rather than every frame. ## Creating Render Bundles [Section titled “Creating Render Bundles”](#creating-render-bundles) ### Bundle Encoder [Section titled “Bundle Encoder”](#bundle-encoder) Create bundle encoder ```javascript const bundleEncoder = device.createRenderBundleEncoder({ colorFormats: ["bgra8unorm"], depthStencilFormat: "depth24plus", sampleCount: 1, }); // Record commands (same API as render pass) bundleEncoder.setPipeline(pipeline); bundleEncoder.setBindGroup(0, bindGroup); bundleEncoder.setVertexBuffer(0, vertexBuffer); bundleEncoder.draw(36, 100); // Finalize into reusable bundle const bundle = bundleEncoder.finish(); ``` ### Executing Bundles [Section titled “Executing Bundles”](#executing-bundles) Execute in render pass ```javascript const pass = encoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: "clear", storeOp: "store", }], depthStencilAttachment: { /* ... */ }, }); pass.executeBundles([bundle]); pass.end(); ``` ## Configuration Requirements [Section titled “Configuration Requirements”](#configuration-requirements) Bundle encoder settings must match the render pass: | Property | Description | | -------------------- | ----------------------------------- | | `colorFormats` | Array of color attachment formats | | `depthStencilFormat` | Depth/stencil format (if used) | | `sampleCount` | MSAA sample count (1 or 4) | | `depthReadOnly` | If true, bundle won’t write depth | | `stencilReadOnly` | If true, bundle won’t write stencil | Format Mismatch Executing a bundle in an incompatible render pass causes a validation error. Always match formats exactly. ## Limitations [Section titled “Limitations”](#limitations) Bundles cannot: * Begin/end occlusion queries * Set scissor rect * Set viewport * Set blend constant * Set stencil reference These values are inherited from the render pass that executes the bundle. ## Use Cases [Section titled “Use Cases”](#use-cases) ### VR Rendering [Section titled “VR Rendering”](#vr-rendering) Record scene once, replay for each eye with different view matrix: VR stereo rendering ```javascript // Record scene commands once const sceneBundle = createSceneBundle(); // Left eye updateViewMatrix(leftEyeView); device.queue.writeBuffer(viewUniformBuffer, 0, leftEyeView); const leftPass = encoder.beginRenderPass(leftEyeDescriptor); leftPass.executeBundles([sceneBundle]); leftPass.end(); // Right eye updateViewMatrix(rightEyeView); device.queue.writeBuffer(viewUniformBuffer, 0, rightEyeView); const rightPass = encoder.beginRenderPass(rightEyeDescriptor); rightPass.executeBundles([sceneBundle]); rightPass.end(); ``` ### Static Scene Elements [Section titled “Static Scene Elements”](#static-scene-elements) Separate static and dynamic content: Static + dynamic rendering ```javascript // Pre-recorded static geometry (terrain, buildings) const staticBundle = createStaticBundle(); function render() { const pass = encoder.beginRenderPass(descriptor); // Execute pre-recorded static content pass.executeBundles([staticBundle]); // Draw dynamic content directly pass.setPipeline(characterPipeline); pass.setBindGroup(0, characterBindGroup); pass.draw(characterVertexCount); pass.end(); } ``` ### Multiple Bundles [Section titled “Multiple Bundles”](#multiple-bundles) Composing bundles ```javascript const terrainBundle = createTerrainBundle(); const buildingsBundle = createBuildingsBundle(); const vegetationBundle = createVegetationBundle(); // Execute all in single call pass.executeBundles([terrainBundle, buildingsBundle, vegetationBundle]); ``` ## Dynamic Content with Bundles [Section titled “Dynamic Content with Bundles”](#dynamic-content-with-bundles) ### Buffer Updates [Section titled “Buffer Updates”](#buffer-updates) Bundle commands are fixed, but buffer contents can change: Dynamic uniforms with static bundle ```javascript // Bundle uses same bind group every frame const bundle = createBundle(uniformBindGroup); function render(time) { // Update buffer content (not the bundle) uniformData[0] = time; device.queue.writeBuffer(uniformBuffer, 0, uniformData); // Execute unchanged bundle pass.executeBundles([bundle]); } ``` ### Indirect Drawing in Bundles [Section titled “Indirect Drawing in Bundles”](#indirect-drawing-in-bundles) Combine bundles with indirect draws for dynamic instance counts: Bundle with indirect draw ```javascript const bundleEncoder = device.createRenderBundleEncoder(config); bundleEncoder.setPipeline(pipeline); bundleEncoder.setBindGroup(0, bindGroup); // Instance count read from buffer at execution time bundleEncoder.drawIndirect(indirectBuffer, 0); const bundle = bundleEncoder.finish(); // Compute shader can update indirectBuffer each frame // Bundle stays unchanged, but drawn geometry varies ``` ## Performance [Section titled “Performance”](#performance) ### Benchmark Results [Section titled “Benchmark Results”](#benchmark-results) Testing on M1 Mac with 40,000 objects: | Method | Frame Time | | ----------------- | ---------- | | Direct draw calls | \~10ms | | Render bundle | \~2-5ms | **2-5x speedup** for CPU-bound scenarios. ### When to Use [Section titled “When to Use”](#when-to-use) | Scenario | Use Bundle? | | ----------------------------- | ----------- | | Static scenes | Yes | | VR (same scene, two views) | Yes | | Many draw calls (1000+) | Yes | | Frequently changing pipelines | No | | GPU-bound rendering | No benefit | | Dynamic scissor/viewport | No | ### Profiling [Section titled “Profiling”](#profiling) Measure bundle benefit ```javascript const start = performance.now(); for (let i = 0; i < 1000; i++) { // Simulate draw calls pass.setPipeline(pipeline); pass.setBindGroup(0, bindGroup); pass.draw(36); } const directTime = performance.now() - start; const bundleStart = performance.now(); pass.executeBundles([bundleWith1000Draws]); const bundleTime = performance.now() - bundleStart; console.log(`Direct: ${directTime}ms, Bundle: ${bundleTime}ms`); ``` ## TypeGPU Integration [Section titled “TypeGPU Integration”](#typegpu-integration) TypeGPU with bundles ```typescript import tgpu from "typegpu"; // Create pipeline through TypeGPU const pipeline = root["~unstable"] .withVertex(vertexFn, vertexLayout) .withFragment(fragmentFn, { format: "bgra8unorm" }) .withPrimitive({ topology: "triangle-list" }) .createPipeline(); // Get raw WebGPU pipeline for bundle const rawPipeline = pipeline.unwrap(); // Record bundle with raw API const bundleEncoder = device.createRenderBundleEncoder({ colorFormats: ["bgra8unorm"], }); bundleEncoder.setPipeline(rawPipeline); // ... record commands const bundle = bundleEncoder.finish(); ```

# Render Pipelines

## Overview [Section titled “Overview”](#overview) Render pipelines define how raw vertex data transforms into pixels on screen. They configure a sequence of programmable and fixed-function stages that process geometry through vertex transformation, rasterization, fragment shading, and output merging. ## Pipeline Stages [Section titled “Pipeline Stages”](#pipeline-stages) ### Vertex Stage [Section titled “Vertex Stage”](#vertex-stage) The first programmable stage—processes each vertex independently: * Transform positions using model-view-projection matrices * Calculate per-vertex lighting (Gouraud shading) * Generate texture coordinates * Pass attributes to the fragment shader Vertex shader output ```wgsl @builtin(position) position: vec4f // Required: clip space position ``` ### Primitive Assembly [Section titled “Primitive Assembly”](#primitive-assembly) Groups vertices into geometric primitives: | Topology | Description | | ---------------- | ---------------------------------------------- | | `triangle-list` | Every 3 vertices form a triangle (most common) | | `triangle-strip` | Shared edges between adjacent triangles | | `line-list` | Pairs of vertices form lines | | `line-strip` | Connected line segments | | `point-list` | Individual points | ### Rasterization [Section titled “Rasterization”](#rasterization) Converts primitives to fragments (potential pixels): 1. Clips primitives to view frustum 2. Performs perspective division 3. Maps to viewport coordinates 4. Determines pixel coverage 5. Interpolates vertex attributes ### Fragment Stage [Section titled “Fragment Stage”](#fragment-stage) Second programmable stage—determines pixel colors: * Samples textures and applies filtering * Calculates lighting (Phong, PBR) * Implements effects (bump mapping, reflections) * Performs alpha testing ### Output Merging [Section titled “Output Merging”](#output-merging) Combines fragment output with existing framebuffer: **Depth Testing**: | Compare | Description | | ------------ | ---------------------------- | | `less` | Standard 3D rendering | | `less-equal` | Same geometry multiple times | | `greater` | Reverse depth buffers | | `always` | Disable depth testing | **Blending**: | Mode | Formula | Use Case | | -------- | ----------------------- | ----------------- | | Opaque | Replace | Solid objects | | Alpha | `src × α + dst × (1-α)` | Transparency | | Additive | `src + dst` | Particles, lights | ## Creating Render Pipelines [Section titled “Creating Render Pipelines”](#creating-render-pipelines) ### Synchronous vs Async [Section titled “Synchronous vs Async”](#synchronous-vs-async) Synchronous creation ```javascript const pipeline = device.createRenderPipeline(descriptor); ``` Async creation (recommended) ```javascript const pipeline = await device.createRenderPipelineAsync(descriptor); ``` ### Complete Pipeline Descriptor [Section titled “Complete Pipeline Descriptor”](#complete-pipeline-descriptor) Full render pipeline ```javascript const pipeline = device.createRenderPipeline({ label: "Main Render Pipeline", layout: "auto", vertex: { module: shaderModule, entryPoint: "vertexMain", buffers: [{ arrayStride: 24, attributes: [ { shaderLocation: 0, offset: 0, format: "float32x3" }, { shaderLocation: 1, offset: 12, format: "float32x3" }, ], }], }, fragment: { module: shaderModule, entryPoint: "fragmentMain", targets: [{ format: navigator.gpu.getPreferredCanvasFormat(), blend: { color: { srcFactor: "src-alpha", dstFactor: "one-minus-src-alpha", operation: "add" }, alpha: { srcFactor: "one", dstFactor: "one-minus-src-alpha", operation: "add" }, }, }], }, primitive: { topology: "triangle-list", cullMode: "back", frontFace: "ccw", }, depthStencil: { format: "depth24plus", depthWriteEnabled: true, depthCompare: "less", }, }); ``` ## Shader Modules [Section titled “Shader Modules”](#shader-modules) Create shader module ```javascript const shaderModule = device.createShaderModule({ label: "Triangle Shaders", code: ` struct VertexOutput { @builtin(position) position: vec4f, @location(0) color: vec4f, }; @vertex fn vertexMain(@location(0) pos: vec2f, @location(1) color: vec4f) -> VertexOutput { var output: VertexOutput; output.position = vec4f(pos, 0.0, 1.0); output.color = color; return output; } @fragment fn fragmentMain(@location(0) color: vec4f) -> @location(0) vec4f { return color; } `, }); ``` Location Matching Fragment shader inputs must match vertex shader outputs by `@location` number and type: ```wgsl // Vertex output @location(0) color: vec3f, @location(1) texCoord: vec2f, // Fragment input - must match fn fragmentMain(@location(0) color: vec3f, @location(1) texCoord: vec2f) ``` ## Vertex State [Section titled “Vertex State”](#vertex-state) ### Buffer Layouts [Section titled “Buffer Layouts”](#buffer-layouts) Vertex buffer layout ```javascript buffers: [{ arrayStride: 24, // Bytes per vertex stepMode: "vertex", attributes: [ { shaderLocation: 0, offset: 0, format: "float32x3" }, // Position { shaderLocation: 1, offset: 12, format: "float32x3" }, // Normal ], }] ``` ### Attribute Formats [Section titled “Attribute Formats”](#attribute-formats) ### Step Modes [Section titled “Step Modes”](#step-modes) | Mode | Advances | Use Case | | ---------- | ------------ | ------------------- | | `vertex` | Per vertex | Standard attributes | | `instance` | Per instance | Instanced rendering | Instanced rendering layout ```javascript buffers: [ { arrayStride: 12, stepMode: "vertex", attributes: [/* per-vertex */] }, { arrayStride: 16, stepMode: "instance", attributes: [/* per-instance */] }, ] ``` ## Fragment State [Section titled “Fragment State”](#fragment-state) ### Render Targets [Section titled “Render Targets”](#render-targets) Multiple render targets ```javascript targets: [ { format: "rgba16float" }, // Position { format: "rgba16float" }, // Normal { format: "rgba8unorm" }, // Albedo ] ``` ### Blend Configurations [Section titled “Blend Configurations”](#blend-configurations) ## Primitive State [Section titled “Primitive State”](#primitive-state) Primitive configuration ```javascript primitive: { topology: "triangle-list", frontFace: "ccw", // Counter-clockwise = front cullMode: "back", // Cull back faces stripIndexFormat: undefined, } ``` ## Depth/Stencil State [Section titled “Depth/Stencil State”](#depthstencil-state) Depth testing configuration ```javascript depthStencil: { format: "depth24plus", depthWriteEnabled: true, depthCompare: "less", } ``` | Format | Description | | ---------------------- | ----------------------------- | | `depth16unorm` | 16-bit depth | | `depth24plus` | At least 24-bit (recommended) | | `depth32float` | Maximum precision | | `depth24plus-stencil8` | Depth + stencil | Create Depth Texture Depth testing requires a depth texture attached to the render pass: ```javascript const depthTexture = device.createTexture({ size: [canvas.width, canvas.height], format: "depth24plus", usage: GPUTextureUsage.RENDER_ATTACHMENT, }); ``` ## TypeGPU Render Pipelines [Section titled “TypeGPU Render Pipelines”](#typegpu-render-pipelines) ### Builder Pattern [Section titled “Builder Pattern”](#builder-pattern) TypeGPU pipeline builder ```typescript const pipeline = root .withVertex(vertexShader, { position, normal, texCoord }) .withFragment(fragmentShader, { color: { format: "bgra8unorm" } }) .withDepthStencil({ format: "depth24plus", depthCompare: "less", depthWriteEnabled: true }) .withPrimitive({ topology: "triangle-list", cullMode: "back" }) .createPipeline(); ``` ### Vertex Layout [Section titled “Vertex Layout”](#vertex-layout) TypeGPU vertex layout ```typescript const vertexLayout = tgpu.vertexLayout({ position: { format: "float32x3", offset: 0 }, normal: { format: "float32x3", offset: 12 }, texCoord: { format: "float32x2", offset: 24 }, }, { arrayStride: 32, stepMode: "vertex", }); ``` ### Drawing [Section titled “Drawing”](#drawing) TypeGPU draw call ```typescript pipeline .with(vertexLayout, vertexBuffer) .with(uniformBindGroup) .withColorAttachment({ color: { view: context.getCurrentTexture().createView(), loadOp: "clear", clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 }, storeOp: "store", }, }) .withDepthStencilAttachment({ view: depthTextureView, depthLoadOp: "clear", depthClearValue: 1.0, depthStoreOp: "store", }) .draw(vertexCount); ``` ## Complete Example [Section titled “Complete Example”](#complete-example) Full WebGPU render pipeline ```javascript const shaderModule = device.createShaderModule({ code: ` struct Uniforms { modelViewProjection: mat4x4f }; @group(0) @binding(0) var<uniform> uniforms: Uniforms; struct VertexInput { @location(0) position: vec3f, @location(1) color: vec3f }; struct VertexOutput { @builtin(position) position: vec4f, @location(0) color: vec3f }; @vertex fn vertexMain(input: VertexInput) -> VertexOutput { var output: VertexOutput; output.position = uniforms.modelViewProjection * vec4f(input.position, 1.0); output.color = input.color; return output; } @fragment fn fragmentMain(@location(0) color: vec3f) -> @location(0) vec4f { return vec4f(color, 1.0); } `, }); const pipeline = device.createRenderPipeline({ layout: "auto", vertex: { module: shaderModule, entryPoint: "vertexMain", buffers: [{ arrayStride: 24, attributes: [ { shaderLocation: 0, offset: 0, format: "float32x3" }, { shaderLocation: 1, offset: 12, format: "float32x3" }, ], }], }, fragment: { module: shaderModule, entryPoint: "fragmentMain", targets: [{ format: navigator.gpu.getPreferredCanvasFormat() }], }, primitive: { topology: "triangle-list", cullMode: "back" }, depthStencil: { format: "depth24plus", depthWriteEnabled: true, depthCompare: "less" }, }); // Render const commandEncoder = device.createCommandEncoder(); const renderPass = commandEncoder.beginRenderPass({ colorAttachments: [{ view: context.getCurrentTexture().createView(), loadOp: "clear", clearValue: { r: 0.0, g: 0.0, b: 0.0, a: 1.0 }, storeOp: "store", }], depthStencilAttachment: { view: depthTextureView, depthLoadOp: "clear", depthClearValue: 1.0, depthStoreOp: "store", }, }); renderPass.setPipeline(pipeline); renderPass.setVertexBuffer(0, vertexBuffer); renderPass.setBindGroup(0, bindGroup); renderPass.draw(3); renderPass.end(); device.queue.submit([commandEncoder.finish()]); ``` ## Performance Guidelines [Section titled “Performance Guidelines”](#performance-guidelines) Transparent Objects Transparent rendering requires correct blend state and drawing order: ```javascript depthStencil: { depthWriteEnabled: false, // Don't write depth depthCompare: "less", // Still test depth } ``` ## Resources [Section titled “Resources”](#resources)

# Slots and Derived Values

## Overview [Section titled “Overview”](#overview) TypeGPU provides **slots** for dynamic resource management and **derived values** for reactive computation patterns. These features enable runtime resource switching without shader recompilation. ## Creating Slots [Section titled “Creating Slots”](#creating-slots) Slot creation ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; // Scalar slots const timeSlot = tgpu.slot(d.f32); const scaleSlot = tgpu.slot(d.f32, 1.0); // With default // Vector/matrix slots const colorSlot = tgpu.slot(d.vec3f); const transformSlot = tgpu.slot(d.mat4x4f); // Texture/sampler slots const textureSlot = tgpu.slot(d.texture2d(d.f32)); const samplerSlot = tgpu.slot("filtering"); // Structured data const materialSlot = tgpu.slot(d.struct({ albedo: d.vec3f, metallic: d.f32, roughness: d.f32, })); ``` ## Using Slots in Shaders [Section titled “Using Slots in Shaders”](#using-slots-in-shaders) Access slots with `.value` (or `$` alias) and declare dependencies with `$uses`: Slots in TGSL ```typescript const timeSlot = tgpu.slot(d.f32, 0.0); const colorSlot = tgpu.slot(d.vec3f); const textureSlot = tgpu.slot(d.texture2d(d.f32)); const samplerSlot = tgpu.slot("filtering"); const fragmentShader = tgpu.fn([d.vec2f], d.vec4f) .$uses({ time: timeSlot, tint: colorSlot, tex: textureSlot, smp: samplerSlot }) ((uv) => { "use gpu"; const texColor = textureSample(textureSlot.value, samplerSlot.value, uv); const pulse = (Math.sin(timeSlot.value) + 1.0) * 0.5; return d.vec4f(texColor.rgb * colorSlot.value * pulse, texColor.a); }); ``` ## Runtime Resource Binding [Section titled “Runtime Resource Binding”](#runtime-resource-binding) Bind slots with `.with()` before pipeline execution: Binding slots ```typescript const renderPipeline = root["~unstable"] .withVertex(vertexFn, vertexLayout) .withFragment(fragmentFn, { color: format }) .createPipeline(); renderPipeline .with(bindGroup) .with(timeSlot, currentTime) .with(colorSlot, vec3(1.0, 0.5, 0.2)) .with(textureSlot, myTexture) .with(samplerSlot, mySampler) .withColorAttachment({ color: colorTarget }) .draw(vertexCount); ``` ### Switching Resources Between Draws [Section titled “Switching Resources Between Draws”](#switching-resources-between-draws) Dynamic material switching ```typescript // Red material renderPipeline .with(colorSlot, vec3(1.0, 0.0, 0.0)) .with(textureSlot, texture1) .withColorAttachment({ color: output }) .draw(vertexCount); // Blue material - no pipeline recreation renderPipeline .with(colorSlot, vec3(0.0, 0.0, 1.0)) .with(textureSlot, texture2) .withColorAttachment({ color: output }) .draw(vertexCount); ``` ## Derived Values [Section titled “Derived Values”](#derived-values) Unstable API Derived values are under `~unstable` namespace and may change in future versions. Derived values compute automatically from dependencies: Reactive computation ```typescript const widthSlot = tgpu.slot(d.f32, 1920); const heightSlot = tgpu.slot(d.f32, 1080); // Automatically recalculates when width/height change const aspectRatio = tgpu["~unstable"].derived(() => { return widthSlot.value / heightSlot.value; }); const invResolution = tgpu["~unstable"].derived(() => { return d.vec2f(1.0 / widthSlot.value, 1.0 / heightSlot.value); }); ``` ### Dependency Chains [Section titled “Dependency Chains”](#dependency-chains) Chained derived values ```typescript const timeSlot = tgpu.slot(d.f32); const speedSlot = tgpu.slot(d.f32, 1.0); // Depends on timeSlot and speedSlot const animatedOffset = tgpu["~unstable"].derived(() => { return Math.sin(timeSlot.value * speedSlot.value); }); // Depends on animatedOffset const animatedColor = tgpu["~unstable"].derived(() => { const t = animatedOffset.value; return d.vec3f(t * 0.5 + 0.5, 0.5, 1.0 - t * 0.5); }); ``` ## Use Cases [Section titled “Use Cases”](#use-cases) ## Performance Guidelines [Section titled “Performance Guidelines”](#performance-guidelines) ### Optimization Strategy [Section titled “Optimization Strategy”](#optimization-strategy) Combine static and dynamic bindings ```typescript // Static for unchanging resources const staticBindGroup = root.createBindGroup(layout, { camera: cameraBuffer, lights: lightBuffer, }); // Slots only for dynamic resources const textureSlot = tgpu.slot(d.texture2d(d.f32)); // Minimize slot changes by sorting objects objects.sort((a, b) => a.materialId - b.materialId); let currentMaterial = null; for (const obj of objects) { if (obj.material !== currentMaterial) { pipeline.with(materialSlot, obj.material); currentMaterial = obj.material; } pipeline.with(transformSlot, obj.transform).draw(obj.vertexCount); } ``` ## Common Pitfalls [Section titled “Common Pitfalls”](#common-pitfalls) Accessing Slots Without .value ```typescript // WRONG const shader = tgpu.fn([], d.f32)(() => { "use gpu"; return timeSlot; // Missing .value }); // CORRECT const shader = tgpu.fn([], d.f32)(() => { "use gpu"; return timeSlot.value; }); ``` Forgetting to Bind Slots ```typescript // WRONG - colorSlot not bound pipeline.withColorAttachment({ color: output }).draw(6); // CORRECT pipeline .with(colorSlot, vec3(1, 0, 0)) .withColorAttachment({ color: output }) .draw(6); ``` Type Mismatches TypeScript catches binding wrong types at compile time: ```typescript const vec3Slot = tgpu.slot(d.vec3f); pipeline.with(vec3Slot, vec4(1, 2, 3, 4)); // Type error pipeline.with(vec3Slot, vec3(1, 2, 3)); // Correct ``` ## Resources [Section titled “Resources”](#resources)

# TGSL Functions

## Overview [Section titled “Overview”](#overview) TGSL (TypeGPU Shading Language) enables writing GPU shader code in TypeScript rather than WGSL. Functions written in TGSL execute on both CPU and GPU, providing type safety, IDE support, and code reusability. ## The ‘use gpu’ Directive [Section titled “The ‘use gpu’ Directive”](#the-use-gpu-directive) Basic TGSL function ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; const addVectors = tgpu.fn([d.vec3f, d.vec3f], d.vec3f).does((a, b) => { "use gpu"; // First statement in function body return a.add(b); }); ``` ### Dual-Mode Execution [Section titled “Dual-Mode Execution”](#dual-mode-execution) Functions with `'use gpu'` run on both CPU (for testing) and GPU (as shaders): Test on CPU, run on GPU ```typescript const clampValue = tgpu.fn([d.f32, d.f32, d.f32], d.f32).does((value, min, max) => { "use gpu"; if (value < min) return min; if (value > max) return max; return value; }); // Test on CPU console.assert(clampValue(5, 0, 10) === 5); console.assert(clampValue(-5, 0, 10) === 0); // Use in GPU shader const shader = tgpu.fn([buffer]).does(() => { "use gpu"; buffer.value[idx] = clampValue(buffer.value[idx], 0.0, 1.0); }); ``` ### Supported JavaScript Features [Section titled “Supported JavaScript Features”](#supported-javascript-features) Build Plugin Required TGSL requires `unplugin-typegpu` to transpile to WGSL. Without the plugin, functions run only on CPU. ## Defining Functions with tgpu.fn() [Section titled “Defining Functions with tgpu.fn()”](#defining-functions-with-tgpufn) ### Basic Functions [Section titled “Basic Functions”](#basic-functions) Function definitions ```typescript // Scalar function const add = tgpu.fn([d.f32, d.f32], d.f32).does((a, b) => { "use gpu"; return a + b; }); // Vector function const distance = tgpu.fn([d.vec3f, d.vec3f], d.f32).does((a, b) => { "use gpu"; const delta = b.sub(a); return Math.sqrt(delta.dot(delta)); }); // Void return (omit return type) const updateParticle = tgpu.fn([particleBuffer, d.u32]).does((particles, index) => { "use gpu"; particles.value[index].position = particles.value[index].position.add(velocity); }); ``` ### Entry Points [Section titled “Entry Points”](#entry-points) ## Accessing GPU Resources [Section titled “Accessing GPU Resources”](#accessing-gpu-resources) ### The .value Property [Section titled “The .value Property”](#the-value-property) Objects with different CPU and GPU representations (buffers, slots) must be accessed via `.value` (or `$` alias): Resource access in TGSL ```typescript const particleBuffer = root.createBuffer(d.arrayOf(d.vec3f, 1000)).$usage("storage"); const deltaTime = root.createUniform(d.f32, 0.016); const updateParticles = tgpu.fn([particleBuffer, deltaTime]).does(() => { "use gpu"; const idx = builtin.globalInvocationId.x; // Use .value or $ to access GPU resources const position = particleBuffer.value[idx]; // or particleBuffer.$[idx] const dt = deltaTime.value; // or deltaTime.$ particleBuffer.value[idx] = position.add(d.vec3f(0, -9.8 * dt, 0)); }); ``` Why .value? TypeGPU buffer objects are JavaScript proxies with CPU APIs. The `.value` property signals “access the GPU representation.” ## Standard Library (typegpu/std) [Section titled “Standard Library (typegpu/std)”](#standard-library-typegpustd) Standard library functions ```typescript import { distance, length, normalize, dot, cross, clamp, mix, smoothstep } from "typegpu/std"; const lighting = tgpu.fn([d.vec3f, d.vec3f], d.vec3f).does((normal, lightDir) => { "use gpu"; const n = normalize(normal); const l = normalize(lightDir); const diffuse = clamp(dot(n, l), 0.0, 1.0); return d.vec3f(diffuse); }); ``` ## Vector and Matrix Operations [Section titled “Vector and Matrix Operations”](#vector-and-matrix-operations) ### Chained Methods (TypeGPU 0.7+) [Section titled “Chained Methods (TypeGPU 0.7+)”](#chained-methods-typegpu-07) Vector operations ```typescript const physics = tgpu.fn([d.vec3f, d.vec3f, d.f32], d.vec3f).does((pos, vel, dt) => { "use gpu"; const gravity = d.vec3f(0, -9.8, 0); const newVel = vel.add(gravity.mul(dt)); return pos.add(newVel.mul(dt)); }); ``` | Operation | Method | WGSL Equivalent | | ------------- | --------------- | --------------- | | Add | `a.add(b)` | `a + b` | | Subtract | `a.sub(b)` | `a - b` | | Multiply | `a.mul(b)` | `a * b` | | Divide | `a.div(b)` | `a / b` | | Length | `a.length()` | `length(a)` | | Normalize | `a.normalize()` | `normalize(a)` | | Dot product | `a.dot(b)` | `dot(a, b)` | | Cross product | `a.cross(b)` | `cross(a, b)` | ### Matrix Operations [Section titled “Matrix Operations”](#matrix-operations) Matrix multiplication ```typescript const transform = tgpu.fn([d.mat4x4f, d.vec3f], d.vec4f).does((mat, pos) => { "use gpu"; return mat.mul(d.vec4f(pos.x, pos.y, pos.z, 1.0)); }); ``` ## Mixing WGSL and TGSL [Section titled “Mixing WGSL and TGSL”](#mixing-wgsl-and-tgsl) ### Call WGSL from TGSL [Section titled “Call WGSL from TGSL”](#call-wgsl-from-tgsl) WGSL function called from TGSL ```typescript const complexOp = tgpu.fn([d.mat4x4f, d.mat4x4f], d.mat4x4f) .implement(/* wgsl */ ` fn complexOp(a: mat4x4f, b: mat4x4f) -> mat4x4f { return transpose(a * b); } `); const useWgsl = tgpu.fn([d.mat4x4f, d.mat4x4f], d.mat4x4f).does((a, b) => { "use gpu"; return complexOp(a, b); // Call WGSL function }); ``` ### Call TGSL from WGSL [Section titled “Call TGSL from WGSL”](#call-tgsl-from-wgsl) TGSL function used in WGSL ```typescript const clamp = tgpu.fn([d.f32, d.f32, d.f32], d.f32).does((val, min, max) => { "use gpu"; if (val < min) return min; if (val > max) return max; return val; }); const processValue = tgpu.fn([d.f32], d.f32) .implement(/* wgsl */ ` fn processValue(input: f32) -> f32 { return clamp(input * 2.0, 0.0, 10.0); } `) .$uses({ clamp }); ``` ## GPU Console.log [Section titled “GPU Console.log”](#gpu-consolelog) Debug with console.log ```typescript const debugShader = tgpu.computeFn([buffer]).does(() => { "use gpu"; const idx = builtin.globalInvocationId.x; if (idx < 5) { // Limit output console.log("Thread", idx, "value:", buffer.value[idx]); } buffer.value[idx] = buffer.value[idx] * 2; }).$workgroupSize(64); ``` Limitations * Buffer limit for logged values * Performance overhead—remove for production * Works with scalars and vectors ## Common Patterns [Section titled “Common Patterns”](#common-patterns) ### Function Composition [Section titled “Function Composition”](#function-composition) Reusable utilities ```typescript const saturate = tgpu.fn([d.f32], d.f32).does((x) => { "use gpu"; return clamp(x, 0, 1); }); const attenuate = tgpu.fn([d.f32, d.f32], d.f32).does((dist, radius) => { "use gpu"; const ratio = dist / radius; return saturate(1.0 - ratio * ratio); }); const pointLight = tgpu.fn([d.vec3f, d.vec3f, d.f32], d.f32).does((fragPos, lightPos, radius) => { "use gpu"; const dist = distance(fragPos, lightPos); return attenuate(dist, radius); }); ``` ### Shared CPU/GPU Logic [Section titled “Shared CPU/GPU Logic”](#shared-cpugpu-logic) Code reuse ```typescript const applyGravity = tgpu.fn([d.vec3f, d.f32], d.vec3f).does((velocity, dt) => { "use gpu"; return velocity.add(d.vec3f(0, -9.8, 0).mul(dt)); }); // CPU testing function simulateOnCPU(particles, dt) { for (const p of particles) { p.velocity = applyGravity(p.velocity, dt); } } // GPU shader const gpuUpdate = tgpu.computeFn([particleBuffer, deltaTime]).does(() => { "use gpu"; const idx = builtin.globalInvocationId.x; particleBuffer.value[idx].velocity = applyGravity( particleBuffer.value[idx].velocity, deltaTime.value ); }).$workgroupSize(64); ``` ## Common Pitfalls [Section titled “Common Pitfalls”](#common-pitfalls) Forgetting .value ```typescript // WRONG const value = buffer[0]; // CORRECT const value = buffer.value[0]; ``` Missing ‘use gpu’ ```typescript // WRONG - runs CPU only const shader = tgpu.fn([d.f32], d.f32).does((x) => { return x * 2; }); // CORRECT const shader = tgpu.fn([d.f32], d.f32).does((x) => { "use gpu"; return x * 2; }); ``` Value vs Reference Semantics In JavaScript (CPU), vectors are references. In WGSL (GPU), they’re values: ```typescript // CPU: b references same object as a const b = a; b[0] = 10; // Modifies a! // GPU: b is a copy const b = a; b[0] = 10; // a unchanged ``` ## Resources [Section titled “Resources”](#resources)

# Workgroup Variables and Shared Memory

## Overview [Section titled “Overview”](#overview) Workgroup variables provide fast on-chip shared memory accessible by all threads in a workgroup. This memory is 10-20× faster than global storage buffers, enabling efficient parallel algorithms. ## When to Use Workgroup Memory [Section titled “When to Use Workgroup Memory”](#when-to-use-workgroup-memory) | Use Case | Benefit | | -------------------- | ----------------------------------------- | | Data reuse | Load once, read many times | | Intermediate results | Share values between threads | | Parallel reduction | Sum/max/min across threads | | Tiled algorithms | Matrix multiplication, convolutions | | Cooperative caching | Cache global data for neighborhood access | ## WGSL Declaration [Section titled “WGSL Declaration”](#wgsl-declaration) Workgroup variable declarations ```wgsl var<workgroup> sharedData: array<f32, 256>; var<workgroup> tileCache: array<vec4f, 64>; var<workgroup> hitCount: atomic<u32>; ``` Compute Shaders Only Workgroup variables are only available in compute shaders—vertex and fragment shaders don’t have workgroups. ### Size Planning [Section titled “Size Planning”](#size-planning) Memory budget calculation ```wgsl var<workgroup> cache: array<f32, 256>; // 256 × 4 = 1,024 bytes var<workgroup> positions: array<vec4f, 128>; // 128 × 16 = 2,048 bytes var<workgroup> indices: array<u32, 512>; // 512 × 4 = 2,048 bytes // Total: 5,120 bytes (within 16KB limit) ``` Query device limits: Check available workgroup storage ```javascript console.log("Max storage:", device.limits.maxComputeWorkgroupStorageSize); ``` ## TypeGPU Declaration [Section titled “TypeGPU Declaration”](#typegpu-declaration) TypeGPU workgroup variables ```typescript import tgpu from "typegpu"; import * as d from "typegpu/data"; const sharedData = tgpu.workgroupVar(d.arrayOf(d.f32, 256)); const sharedVectors = tgpu.workgroupVar(d.arrayOf(d.vec4f, 128)); const ParticleData = d.struct({ position: d.vec3f, velocity: d.vec3f, }); const sharedParticles = tgpu.workgroupVar(d.arrayOf(ParticleData, 64)); ``` ## Synchronization [Section titled “Synchronization”](#synchronization) ### workgroupBarrier() [Section titled “workgroupBarrier()”](#workgroupbarrier) All threads must reach the barrier before any can proceed. Also ensures memory visibility. Barrier usage ```wgsl var<workgroup> sharedData: array<f32, 256>; @compute @workgroup_size(256) fn computeMain(@builtin(local_invocation_index) idx: u32) { // Phase 1: Each thread writes sharedData[idx] = f32(idx) * 2.0; workgroupBarrier(); // Wait for all writes // Phase 2: Safe to read other threads' data let neighbor = sharedData[(idx + 1u) % 256u]; } ``` Always Synchronize Without barriers, threads may read before others have written: ```wgsl // BUG: Race condition! sharedData[idx] = f32(idx); let value = sharedData[(idx + 1u) % 256u]; // May read garbage! ``` ### storageBarrier() [Section titled “storageBarrier()”](#storagebarrier) Synchronizes storage buffer and atomic operations (but NOT execution): Storage barrier ```wgsl atomicAdd(&globalCounter, 1u); storageBarrier(); // Ensure atomic completes output[gid.x] = f32(value); ``` ## Common Patterns [Section titled “Common Patterns”](#common-patterns) ## Performance Considerations [Section titled “Performance Considerations”](#performance-considerations) ### Bank Conflicts [Section titled “Bank Conflicts”](#bank-conflicts) Workgroup memory uses banks—simultaneous access to the same bank serializes: Avoid bank conflicts ```wgsl // BAD: All threads access same element let value = sharedData[0]; // Serialized! // GOOD: Each thread accesses different element let value = sharedData[idx]; // Parallel! ``` ### Optimal Access Patterns [Section titled “Optimal Access Patterns”](#optimal-access-patterns) | Pattern | Performance | | ---------------------- | ---------------- | | Sequential access | Fast | | Broadcast (same value) | Fast | | Random access | Medium | | Strided (every 32nd) | Slow (conflicts) | ### Workgroup Size [Section titled “Workgroup Size”](#workgroup-size) ## Platform Limits [Section titled “Platform Limits”](#platform-limits) | Platform | Typical Limit | | ----------------- | ------------- | | Desktop NVIDIA | 48KB | | Desktop AMD/Intel | 32KB | | Mobile | 16-32KB | | WebGPU Minimum | 16KB | ## Common Pitfalls [Section titled “Common Pitfalls”](#common-pitfalls) Missing Barrier ```wgsl sharedData[idx] = f32(idx); // Missing workgroupBarrier()! let value = sharedData[(idx + 1u) % 256u]; // Race condition! ``` Race Condition with Non-Atomic ```wgsl // WRONG: Use atomic<u32> for shared counters var<workgroup> counter: u32; counter += 1u; // Race condition! // CORRECT var<workgroup> counter: atomic<u32>; atomicAdd(&counter, 1u); ``` Exceeding Size Limit ```wgsl // May fail on some devices (32KB) var<workgroup> huge: array<vec4f, 2048>; // 2048 × 16 = 32KB ``` Reading Uninitialized Data Workgroup memory starts uninitialized. Always write before reading: ```wgsl // BUG: Only thread 0 writes, others read garbage if (idx == 0u) { sharedData[0] = 42.0; } workgroupBarrier(); let value = sharedData[idx]; // data[1..255] uninitialized! ``` ## Resources [Section titled “Resources”](#resources)